{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31c9aadd",
   "metadata": {},
   "source": [
    "# RAGdoll Key Modules Demo\n",
    "\n",
    "Hands-on walkthrough for ingestion, chunking, embeddings, storage layers, and orchestration. Every example relies on the sample assets under `tests/test_data`, so the notebook can run offline.\n",
    "\n",
    "> **Note:** \n",
    "- Set `USE_OPENAI = True` at the top to use real OpenAI embeddings, `False` for fake embeddings.\n",
    "- Cells 7 and 9 call OpenAI's GPT endpoints via `get_llm_caller`. Export `OPENAI_API_KEY` (or add it to `.env`) before running them.\n",
    "- **SSL Certificate Issues**: If you encounter `SSL: CERTIFICATE_VERIFY_FAILED` errors (common in corporate networks), see the SSL workaround cell below to disable verification for development purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf21e3cb",
   "metadata": {},
   "source": [
    "## What you'll see\n",
    "\n",
    "1. **Ingestion** ‚Äì `DocumentLoaderService` from [`docs/ingestion.md`](../docs/ingestion.md).\n",
    "2. **Chunking** ‚Äì `ragdoll.chunkers` helpers from [`docs/chunking.md`](../docs/chunking.md).\n",
    "3. **Embeddings** ‚Äì provider factory from [`docs/embeddings.md`](../docs/embeddings.md) controlled by USE_OPENAI flag.\n",
    "4. **Vector stores** ‚Äì `vector_store_from_config` customization from [`docs/vector_stores.md`](../docs/vector_stores.md).\n",
    "5. **Graph stores** ‚Äì `get_graph_store` JSON persistence from [`docs/graph_stores.md`](../docs/graph_stores.md).\n",
    "6. **Entity extraction** ‚Äì `EntityExtractionService` for building knowledge graphs.\n",
    "7. **LLMs** ‚Äì `get_llm_caller`/`call_llm_sync` bridge described in [`docs/llm_integration.md`](../docs/llm_integration.md) hitting your real OpenAI model.\n",
    "8. **Pipeline** ‚Äì `IngestionPipeline` snapshot from [`docs/architecture.md`](../docs/architecture.md).\n",
    "9. **Retrieval** ‚Äì New modular retrieval system with `VectorRetriever`, `GraphRetriever`, and `HybridRetriever` from [`docs/retrieval.md`](../docs/retrieval.md).\n",
    "10. **Advanced Patterns** ‚Äì Hybrid retrieval modes, graph traversal strategies, and async retrieval.\n",
    "\n",
    "Each cell builds on the previous ones so you can treat this as a scratchpad for experimenting with new loaders or configuration overrides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c06129f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from ragdoll import Ragdoll\n",
    "from ragdoll.app_config import bootstrap_app\n",
    "from ragdoll.ingestion import DocumentLoaderService\n",
    "from ragdoll.chunkers import get_text_splitter, split_documents\n",
    "from ragdoll.embeddings import get_embedding_model\n",
    "from ragdoll.vector_stores import vector_store_from_config\n",
    "from ragdoll.config.base_config import VectorStoreConfig\n",
    "from ragdoll.graph_stores import get_graph_store\n",
    "from ragdoll.entity_extraction import EntityExtractionService\n",
    "from ragdoll.entity_extraction.models import Graph, GraphNode, GraphEdge\n",
    "from ragdoll.entity_extraction.graph_persistence import GraphPersistenceService\n",
    "from ragdoll.retrieval import VectorRetriever, GraphRetriever, HybridRetriever\n",
    "from ragdoll.llms import get_llm_caller\n",
    "from ragdoll.llms.callers import call_llm_sync\n",
    "from ragdoll.pipeline import ingest_documents, IngestionPipeline, IngestionOptions\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c20c88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "DATA_DIR = Path('../tests/test_data').resolve()\n",
    "STATE_DIR = Path('demo_state').resolve()\n",
    "STATE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "SAMPLE_TXT = DATA_DIR / 'test_txt.txt'\n",
    "#SAMPLE_TXT = DATA_DIR / '*'\n",
    "\n",
    "# Set to True to use real OpenAI embeddings, False for fake embeddings\n",
    "USE_OPENAI = True\n",
    "\n",
    "app_config = bootstrap_app(\n",
    "    overrides={\n",
    "        'monitor': {'enabled': False, 'collect_metrics': False},\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "def normalize_documents(raw_docs):\n",
    "    docs = []\n",
    "    for entry in raw_docs:\n",
    "        if isinstance(entry, Document):\n",
    "            docs.append(entry)\n",
    "        elif isinstance(entry, dict):\n",
    "            docs.append(\n",
    "                Document(\n",
    "                    page_content=str(entry.get('page_content', '')),\n",
    "                    metadata=entry.get('metadata', {}) or {},\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            docs.append(Document(page_content=str(entry), metadata={}))\n",
    "    return docs\n",
    "\n",
    "\n",
    "def reset_subdir(name: str) -> Path:\n",
    "    path = STATE_DIR / name\n",
    "    if path.exists():\n",
    "        for attempt in range(5):\n",
    "            try:\n",
    "                shutil.rmtree(path)\n",
    "                break\n",
    "            except PermissionError:\n",
    "                time.sleep(0.5)\n",
    "        else:\n",
    "            timestamped = STATE_DIR / f\"{name}_{int(time.time())}\"\n",
    "            timestamped.mkdir(parents=True, exist_ok=True)\n",
    "            print(f\"Warning: {path} was locked, using {timestamped} instead.\")\n",
    "            return timestamped\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33445758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key len: 164 prefix: sk-proj-Ml-_ suffix: NUDc8A\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "k = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(\"Key len:\", len(k), \"prefix:\", k[:12], \"suffix:\", k[-6:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29502297",
   "metadata": {},
   "source": [
    "## 1. Load sample data\n",
    "`DocumentLoaderService` fans out across the loader registry defined in `ragdoll/config/default_config.yaml`. We point it at the lightweight TXT fixture so the demo does not need optional dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53759c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document(s) from test_txt.txt\n",
      "Metadata sample:\n",
      "{'source': 'C:\\\\dev\\\\RAGdoll\\\\tests\\\\test_data\\\\test_txt.txt'}\n",
      "Preview:\n",
      "Rag graph, or Retrieval-Augmented Generation with graph-based knowledge representation, enhances traditional RAG systems by structuring retrieved information into a dynamic knowledge graph rather than treating it as flat text chunks. At its core, it begins with a user query that triggers a retrieval step from a vector database or search index. Instead of directly feeding retrieved documents into a\n"
     ]
    }
   ],
   "source": [
    "loader = DocumentLoaderService(\n",
    "    app_config=app_config,\n",
    "    use_cache=False,\n",
    "    collect_metrics=False,\n",
    ")\n",
    "\n",
    "raw_documents = loader.ingest_documents([str(SAMPLE_TXT)])\n",
    "documents = normalize_documents(raw_documents)\n",
    "\n",
    "print(f\"Loaded {len(documents)} document(s) from {SAMPLE_TXT.name}\")\n",
    "print('Metadata sample:')\n",
    "pprint(documents[0].metadata)\n",
    "print('Preview:')\n",
    "print(documents[0].page_content[:400])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f416bedd",
   "metadata": {},
   "source": [
    "## 2. Chunk documents\n",
    "`ragdoll.chunkers.get_text_splitter` mirrors the strategies in [`docs/chunking.md`](../docs/chunking.md). Reusing the splitter instance keeps experiments consistent when you tweak chunk sizes/overlap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75b56c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 20 chunk(s)\n",
      "Chunk 1 metadata: {'source': 'C:\\\\dev\\\\RAGdoll\\\\tests\\\\test_data\\\\test_txt.txt', 'chunk_id': '7623481e_0_v1'}\n",
      "Rag graph, or Retrieval-Augmented Generation with graph-based knowledge representation, enhances traditional RAG systems by structuring retrieved information into a dynamic knowled\n",
      "---\n",
      "Chunk 2 metadata: {'source': 'C:\\\\dev\\\\RAGdoll\\\\tests\\\\test_data\\\\test_txt.txt', 'chunk_id': '7623481e_1_v1'}\n",
      "it as flat text chunks. At its core, it begins with a user query that triggers a retrieval step from a vector database or search index. Instead of directly feeding retrieved docume\n",
      "---\n",
      "Chunk 3 metadata: {'source': 'C:\\\\dev\\\\RAGdoll\\\\tests\\\\test_data\\\\test_txt.txt', 'chunk_id': '7623481e_2_v1'}\n",
      "the system parses these documents to extract entities (such as people, organizations, or concepts) and relationships (like \"works at\" or \"caused by\") using named entity recognition\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "splitter = get_text_splitter(\n",
    "    splitter_type='recursive',\n",
    "    chunk_size=250,\n",
    "    chunk_overlap=40,\n",
    "    app_config=app_config,\n",
    ")\n",
    "chunks = split_documents(documents, text_splitter=splitter)\n",
    "\n",
    "print(f\"Created {len(chunks)} chunk(s)\")\n",
    "for idx, chunk in enumerate(chunks[:3], start=1):\n",
    "    preview = chunk.page_content[:180].replace('\\n', ' ')\n",
    "    print(f\"Chunk {idx} metadata: {chunk.metadata}\")\n",
    "    print(preview)\n",
    "    print('---')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaf55d4",
   "metadata": {},
   "source": [
    "## 3. Create embeddings\n",
    "`ragdoll.embeddings.get_embedding_model` instantiates providers dynamically. Passing `provider=\"fake\"` gives deterministic vectors without hitting OpenAI/HuggingFace, but the rest of the flow matches production usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7842ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a356222c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3 embedding vector(s) with dimension 1536\n",
      "First vector slice: [-0.0022252982016652822, 0.010352334007620811, -0.011473081074655056, 0.009996026754379272, -0.004295117221772671, -0.017763515934348106, -0.015794111415743828, -0.014433667063713074]\n"
     ]
    }
   ],
   "source": [
    "embedding_inputs = [chunk.page_content for chunk in chunks[:3]]\n",
    "if not embedding_inputs:\n",
    "    embedding_inputs = [documents[0].page_content]\n",
    "\n",
    "embeddings = (\n",
    "    get_embedding_model() if USE_OPENAI \n",
    "    else get_embedding_model(provider='fake', size=256)\n",
    ")\n",
    "vectors = embeddings.embed_documents(embedding_inputs)\n",
    "\n",
    "print(f\"Generated {len(vectors)} embedding vector(s) with dimension {len(vectors[0])}\")\n",
    "print('First vector slice:', vectors[0][:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924c608c",
   "metadata": {},
   "source": [
    "## 4. Build a vector store\n",
    "`vector_store_from_config` consumes a `VectorStoreConfig`, so you can swap FAISS/Chroma/etc. on demand. This cell provisions a Chroma collection under `demo_state` and runs a quick similarity query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53b75b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1 (source=C:\\dev\\RAGdoll\\tests\\test_data\\test_txt.txt) -> it as flat text chunks. At its core, it begins with a user query that triggers a retrieval step from a vector database or search index. Instead of directly feed\n",
      "Result 2 (source=C:\\dev\\RAGdoll\\tests\\test_data\\test_txt.txt) -> Finally, the output includes not just the generated text but optional graph visualizations or citations mapping back to original documents. This traceability im\n"
     ]
    }
   ],
   "source": [
    "core_store_dir = reset_subdir('chroma_core_demo')\n",
    "vector_config = VectorStoreConfig(\n",
    "    enabled=True,\n",
    "    store_type='chroma',\n",
    "    params={\n",
    "        'collection_name': 'ragdoll_core_demo',\n",
    "        'persist_directory': str(core_store_dir),\n",
    "    },\n",
    ")\n",
    "\n",
    "demo_vector_store = vector_store_from_config(\n",
    "    vector_config,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "demo_vector_store.add_documents(chunks)\n",
    "question = 'What content lives in the txt sample?'\n",
    "results = demo_vector_store.similarity_search(question, k=2)\n",
    "for idx, doc in enumerate(results, start=1):\n",
    "    snippet = doc.page_content[:160].replace('\\n', ' ')\n",
    "    print(f\"Result {idx} (source={doc.metadata.get('source')}) -> {snippet}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed0f21c",
   "metadata": {},
   "source": [
    "## 5. Run entity extraction + persist a graph\n",
    "`EntityExtractionService` runs spaCy over the chunks and hands everything to `GraphPersistenceService`. We disable re-chunking, dump the output to JSON under `demo_state/`, and render a quick visualization so you can inspect the extracted entities before wiring them into a retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42838a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM initialized for entity extraction.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create directories\n",
    "graph_store_dir = reset_subdir('graph_demo')\n",
    "graph_image_path = graph_store_dir / 'graph_demo.png'\n",
    "graph_store_file = graph_store_dir / 'graph.pkl'\n",
    "\n",
    "class ExampleFallbackLLM:\n",
    "    async def call(self, prompt: str) -> str:\n",
    "        return \"Fallback response (no API key for entity extraction)\"\n",
    "    # Initialize LLM\n",
    "if not os.getenv('OPENAI_API_KEY'):\n",
    "    print(\"OPENAI_API_KEY not set, using fallback LLM for demo.\")\n",
    "    llm_caller = ExampleFallbackLLM()\n",
    "else:\n",
    "    llm_caller = get_llm_caller(app_config=app_config)\n",
    "    if llm_caller is None:\n",
    "        print(\"Unable to initialize LLM, using fallback.\")\n",
    "        llm_caller = ExampleFallbackLLM()\n",
    "    else:\n",
    "        print(\"LLM initialized for entity extraction.\")\n",
    "\n",
    "# Configure ingestion options - use the shared embeddings from earlier\n",
    "options = IngestionOptions(\n",
    "    batch_size=5,\n",
    "    extract_entities=True,\n",
    "    chunking_options={'chunk_size': 1000, 'chunk_overlap': 200},\n",
    "    vector_store_options={\n",
    "        \"store_type\": \"chroma\",\n",
    "        \"params\": {\n",
    "            \"collection_name\": \"graph_demo\",\n",
    "            \"persist_directory\": str(graph_store_dir / \"vector\"),\n",
    "        },\n",
    "    },\n",
    "    graph_store_options={\n",
    "        \"store_type\": \"networkx\",\n",
    "        \"output_file\": str(graph_store_file),\n",
    "    },\n",
    "    llm_caller=llm_caller,\n",
    "    entity_extraction_options={\n",
    "        \"entity_types\": [\"Person\", \"Organization\", \"Location\"],\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4c87024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parallel extraction task 0 failed: cannot pickle '_thread.RLock' object\n",
      "Parallel extraction task 1 failed: cannot pickle '_thread.RLock' object\n",
      "Parallel extraction task 2 failed: cannot pickle '_thread.RLock' object\n",
      "Parallel extraction task 1 failed: cannot pickle '_thread.RLock' object\n",
      "Parallel extraction task 2 failed: cannot pickle '_thread.RLock' object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ingestion complete!\n",
      "Documents processed: 1\n",
      "Chunks created: 5\n",
      "Entities extracted: 0\n",
      "Relationships extracted: 0\n",
      "Vector entries added: 5\n",
      "Graph entries added: 0\n",
      "No graph available to visualize.\n"
     ]
    }
   ],
   "source": [
    "# Run ingestion using ingest_documents helper (cleaner approach)\n",
    "from ragdoll.pipeline import ingest_documents\n",
    "from ragdoll.utils import visualize_graph\n",
    "\n",
    "sources = [str(SAMPLE_TXT)]\n",
    "\n",
    "# Use ingest_documents which properly handles everything\n",
    "result = await ingest_documents(sources, options=options)\n",
    "stats = result.get(\"stats\", {})\n",
    "graph = result.get(\"graph\")\n",
    "graph_store = result.get(\"graph_store\")\n",
    "graph_retriever = result.get(\"graph_retriever\")\n",
    "pipeline_vector_store = result.get(\"vector_store\")  # Vector store with graph node embeddings\n",
    "\n",
    "# Print results\n",
    "print(f\"‚úÖ Ingestion complete!\")\n",
    "print(f\"Documents processed: {stats.get('documents_processed')}\")\n",
    "print(f\"Chunks created: {stats.get('chunks_created')}\")\n",
    "print(f\"Entities extracted: {stats.get('entities_extracted')}\")\n",
    "print(f\"Relationships extracted: {stats.get('relationships_extracted')}\")\n",
    "print(f\"Vector entries added: {stats.get('vector_entries_added')}\")\n",
    "print(f\"Graph entries added: {stats.get('graph_entries_added')}\")\n",
    "\n",
    "if stats.get(\"errors\"):\n",
    "    print(f\"‚ö†Ô∏è Warnings/Errors:\")\n",
    "    for error in stats[\"errors\"]:\n",
    "        print(f\"  - {error}\")\n",
    "\n",
    "# Visualize using the utility function (same as graph_rag_ingestion.py)\n",
    "if graph and hasattr(graph, 'nodes') and graph.nodes:\n",
    "    print(f\"\\nVisualizing graph with {len(graph.nodes)} nodes and {len(graph.edges)} edges...\")\n",
    "    visualize_graph(graph, output_image_path=str(graph_image_path), output_json_path=str(graph_store_dir / \"graph_output.json\"))\n",
    "else:\n",
    "    print('No graph available to visualize.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69e610a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Reloaded modules with latest bug fixes:\n",
      "   - extract_from_vector_store functionality\n",
      "   - VectorStoreAdapter Chroma embedding retrieval\n",
      "   - GraphRetriever vector_id deduplication\n"
     ]
    }
   ],
   "source": [
    "# Reload modules to get latest changes\n",
    "import importlib\n",
    "import ragdoll.entity_extraction.entity_extraction_service\n",
    "import ragdoll.vector_stores.adapter\n",
    "import ragdoll.retrieval.graph\n",
    "import ragdoll.pipeline\n",
    "importlib.reload(ragdoll.entity_extraction.entity_extraction_service)\n",
    "importlib.reload(ragdoll.vector_stores.adapter)\n",
    "importlib.reload(ragdoll.retrieval.graph)\n",
    "importlib.reload(ragdoll.pipeline)\n",
    "from ragdoll.pipeline import ingest_from_vector_store\n",
    "from ragdoll.retrieval import GraphRetriever\n",
    "print(\"‚úÖ Reloaded modules with latest bug fixes:\")\n",
    "print(\"   - extract_from_vector_store functionality\")\n",
    "print(\"   - VectorStoreAdapter Chroma embedding retrieval\")  \n",
    "print(\"   - GraphRetriever vector_id deduplication\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d913be70",
   "metadata": {},
   "source": [
    "## 5d. Fix Applied: Deduplication of Vector IDs\n",
    "\n",
    "**Issue Fixed:** When multiple entities are extracted from the same document chunk, they share the same `vector_id`. Previously, the GraphRetriever would fail when trying to fetch embeddings because Chroma rejects duplicate IDs.\n",
    "\n",
    "**Solution:** The `_build_embedding_index()` method now:\n",
    "1. Deduplicates vector_ids before fetching embeddings from the vector store\n",
    "2. Maps multiple nodes to the same embedding when they share a vector_id\n",
    "3. Properly counts indexed vs orphaned nodes\n",
    "\n",
    "**Result:** Graph retrieval now works correctly even when multiple entities share the same source document chunk.\n",
    "\n",
    "**To test:** Re-run the cell below (Section 5c) after reloading modules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcf7d7a",
   "metadata": {},
   "source": [
    "## 5c. Extract Entities from Existing Vector Store (Alternative Approach)\n",
    "\n",
    "This demonstrates extracting entities from an already-populated vector store. This approach **guarantees** that graph nodes have vector_id references matching the vector store, eliminating ID mismatch issues.\n",
    "\n",
    "**Use cases:**\n",
    "- Add graph capabilities to existing vector stores\n",
    "- Ensure graph nodes reference valid vector IDs\n",
    "- Process documents that are already chunked and embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "844ef9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Extracting Entities from Existing Vector Store ===\n",
      "\n",
      "‚úÖ Extraction from vector store complete!\n",
      "   Entities found: 76\n",
      "   Relationships found: 45\n",
      "   Nodes with vector_id: 76/76 ‚úÖ\n",
      "\n",
      "üìå Sample nodes with vector_id references:\n",
      "   - [ORG] Retrieval-Augmented Generation\n",
      "     vector_id: 9869eadc-0cf6-4753-b921-17f6b73018bd...\n",
      "   - [ORG] RAG\n",
      "     vector_id: 9869eadc-0cf6-4753-b921-17f6b73018bd...\n",
      "   - [ENTITY] RAG_GRAPH\n",
      "     vector_id: 9869eadc-0cf6-4753-b921-17f6b73018bd...\n",
      "   - [ENTITY] TRADITIONAL_RAG_SYSTEMS\n",
      "     vector_id: 9869eadc-0cf6-4753-b921-17f6b73018bd...\n",
      "   - [ENTITY] DYNAMIC_KNOWLEDGE_GRAPH\n",
      "     vector_id: 9869eadc-0cf6-4753-b921-17f6b73018bd...\n",
      "\n",
      "=== Testing Retrieval with Vector Store Extracted Graph ===\n",
      "‚úÖ Extraction from vector store complete!\n",
      "   Entities found: 76\n",
      "   Relationships found: 45\n",
      "   Nodes with vector_id: 76/76 ‚úÖ\n",
      "\n",
      "üìå Sample nodes with vector_id references:\n",
      "   - [ORG] Retrieval-Augmented Generation\n",
      "     vector_id: 9869eadc-0cf6-4753-b921-17f6b73018bd...\n",
      "   - [ORG] RAG\n",
      "     vector_id: 9869eadc-0cf6-4753-b921-17f6b73018bd...\n",
      "   - [ENTITY] RAG_GRAPH\n",
      "     vector_id: 9869eadc-0cf6-4753-b921-17f6b73018bd...\n",
      "   - [ENTITY] TRADITIONAL_RAG_SYSTEMS\n",
      "     vector_id: 9869eadc-0cf6-4753-b921-17f6b73018bd...\n",
      "   - [ENTITY] DYNAMIC_KNOWLEDGE_GRAPH\n",
      "     vector_id: 9869eadc-0cf6-4753-b921-17f6b73018bd...\n",
      "\n",
      "=== Testing Retrieval with Vector Store Extracted Graph ===\n",
      "Graph stats: 76 nodes, 76 indexed\n",
      "\n",
      "Retrieved 10 nodes for query: 'What entities are mentioned in the documents?'\n",
      "1. [ORG] (score: 0.357)\n",
      "   ORG: NER   label: NER   text: NER   context: the system parses these documents to extract entities (...\n",
      "2. [ENTITY] (score: 0.357)\n",
      "   ENTITY: system   label: system   text: system   embedding_source: chunk   source: C:\\dev\\RAGdoll\\tes...\n",
      "3. [ENTITY] (score: 0.357)\n",
      "   ENTITY: documents   label: documents   text: documents   embedding_source: chunk   source: C:\\dev\\RA...\n",
      "4. [ENTITY] (score: 0.357)\n",
      "   ENTITY: entities   label: entities   text: entities   embedding_source: chunk   source: C:\\dev\\RAGdo...\n",
      "5. [ENTITY] (score: 0.357)\n",
      "   ENTITY: relationships   label: relationships   text: relationships   embedding_source: chunk   sourc...\n",
      "\n",
      "‚úÖ Vector store extraction complete!\n",
      "üìù Note: This approach guarantees graph node vector_ids match the vector store IDs\n",
      "Graph stats: 76 nodes, 76 indexed\n",
      "\n",
      "Retrieved 10 nodes for query: 'What entities are mentioned in the documents?'\n",
      "1. [ORG] (score: 0.357)\n",
      "   ORG: NER   label: NER   text: NER   context: the system parses these documents to extract entities (...\n",
      "2. [ENTITY] (score: 0.357)\n",
      "   ENTITY: system   label: system   text: system   embedding_source: chunk   source: C:\\dev\\RAGdoll\\tes...\n",
      "3. [ENTITY] (score: 0.357)\n",
      "   ENTITY: documents   label: documents   text: documents   embedding_source: chunk   source: C:\\dev\\RA...\n",
      "4. [ENTITY] (score: 0.357)\n",
      "   ENTITY: entities   label: entities   text: entities   embedding_source: chunk   source: C:\\dev\\RAGdo...\n",
      "5. [ENTITY] (score: 0.357)\n",
      "   ENTITY: relationships   label: relationships   text: relationships   embedding_source: chunk   sourc...\n",
      "\n",
      "‚úÖ Vector store extraction complete!\n",
      "üìù Note: This approach guarantees graph node vector_ids match the vector store IDs\n"
     ]
    }
   ],
   "source": [
    "# Extract entities from the demo_vector_store created in Section 4\n",
    "# This ensures graph nodes have vector_ids that match the vector store\n",
    "\n",
    "if 'demo_vector_store' in globals():\n",
    "    print(\"=== Extracting Entities from Existing Vector Store ===\\n\")\n",
    "    \n",
    "    from ragdoll.pipeline import ingest_from_vector_store\n",
    "    \n",
    "    # Create output directory for this alternative approach\n",
    "    vs_graph_dir = reset_subdir('vs_extracted_graph')\n",
    "    \n",
    "    # Extract entities directly from demo_vector_store\n",
    "    vs_graph_result = await ingest_from_vector_store(\n",
    "        vector_store=demo_vector_store,\n",
    "        embedding_model=embeddings,\n",
    "        options=IngestionOptions(\n",
    "            batch_size=5,\n",
    "            entity_extraction_options={\n",
    "                \"entity_types\": [\"Person\", \"Organization\", \"Location\", \"Technology\"],\n",
    "                \"use_llm\": True,\n",
    "            },\n",
    "            llm_caller=llm_caller,\n",
    "            graph_store_options={\n",
    "                \"store_type\": \"networkx\",\n",
    "                \"output_file\": str(vs_graph_dir / \"graph.pkl\"),\n",
    "            }\n",
    "        ),\n",
    "        app_config=app_config\n",
    "    )\n",
    "    \n",
    "    vs_graph = vs_graph_result.get(\"graph\")\n",
    "    vs_graph_store = vs_graph_result.get(\"graph_store\")\n",
    "    vs_graph_retriever = vs_graph_result.get(\"graph_retriever\")\n",
    "    vs_stats = vs_graph_result.get(\"stats\", {})\n",
    "    \n",
    "    print(f\"‚úÖ Extraction from vector store complete!\")\n",
    "    print(f\"   Entities found: {vs_stats.get('entities_extracted', 0)}\")\n",
    "    print(f\"   Relationships found: {vs_stats.get('relationships_extracted', 0)}\")\n",
    "    \n",
    "    # Verify nodes have vector_id references\n",
    "    if vs_graph and vs_graph.nodes:\n",
    "        nodes_with_vector_id = sum(1 for node in vs_graph.nodes \n",
    "                                   if 'vector_id' in getattr(node, 'properties', {}))\n",
    "        print(f\"   Nodes with vector_id: {nodes_with_vector_id}/{len(vs_graph.nodes)} ‚úÖ\")\n",
    "        \n",
    "        # Show sample nodes with their vector_ids\n",
    "        print(\"\\nüìå Sample nodes with vector_id references:\")\n",
    "        for node in list(vs_graph.nodes)[:5]:\n",
    "            props = getattr(node, 'properties', {})\n",
    "            vector_id = props.get('vector_id', 'N/A')\n",
    "            entity_name = props.get('name') or props.get('text', node.id)\n",
    "            print(f\"   - [{node.type}] {entity_name}\")\n",
    "            print(f\"     vector_id: {vector_id[:36]}...\")\n",
    "    \n",
    "    # Test retrieval with this graph\n",
    "    if vs_graph_retriever:\n",
    "        print(\"\\n=== Testing Retrieval with Vector Store Extracted Graph ===\")\n",
    "        \n",
    "        test_query = \"What entities are mentioned in the documents?\"\n",
    "        test_results = vs_graph_retriever.get_relevant_documents(test_query)\n",
    "        \n",
    "        # Get retriever stats\n",
    "        retriever_stats = vs_graph_retriever.get_stats()\n",
    "        print(f\"Graph stats: {retriever_stats.get('node_count', 0)} nodes, \"\n",
    "              f\"{retriever_stats.get('indexed_nodes', 0)} indexed\")\n",
    "        \n",
    "        print(f\"\\nRetrieved {len(test_results)} nodes for query: '{test_query}'\")\n",
    "        for idx, doc in enumerate(test_results[:5], start=1):\n",
    "            node_type = doc.metadata.get('node_type', 'unknown')\n",
    "            node_id = doc.metadata.get('node_id', 'unknown')\n",
    "            relevance = doc.metadata.get('relevance_score', 0)\n",
    "            content_preview = doc.page_content[:100].replace('\\n', ' ')\n",
    "            print(f\"{idx}. [{node_type}] (score: {relevance:.3f})\")\n",
    "            print(f\"   {content_preview}...\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Vector store extraction complete!\")\n",
    "    print(\"üìù Note: This approach guarantees graph node vector_ids match the vector store IDs\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Run Section 4 first to create demo_vector_store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876a297f",
   "metadata": {},
   "source": [
    "## 5b. Alternative: Using IngestionPipeline Directly (OPTIONAL - Keep Commented)\n",
    "\n",
    "**‚ö†Ô∏è Note:** This cell is commented out to avoid interfering with Section 9's graph retrieval demos. \n",
    "\n",
    "The manual pipeline approach creates additional graph stores that can conflict with the `graph_store` variable used in later cells. If you want to test this approach, run it separately or ensure later cells use `manual_graph_store` instead of `graph_store`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "238104b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Alternative approach: Create pipeline manually for fine-grained control\n",
    "# from ragdoll.pipeline import IngestionPipeline, IngestionOptions\n",
    "# from ragdoll.vector_stores import vector_store_from_config\n",
    "\n",
    "# ## Setup directories for the manual pipeline demo\n",
    "# manual_graph_dir = reset_subdir('graph_manual_demo')\n",
    "# manual_graph_file = manual_graph_dir / 'graph.pkl'\n",
    "# manual_graph_image = manual_graph_dir / 'manual_graph.png'\n",
    "\n",
    "# ## Create a fresh vector store for this demo\n",
    "# manual_vector_store = vector_store_from_config(\n",
    "#     VectorStoreConfig(\n",
    "#         enabled=True,\n",
    "#         store_type=\"chroma\",\n",
    "#         params={\n",
    "#             \"collection_name\": \"manual_graph_demo\",\n",
    "#             \"persist_directory\": str(manual_graph_dir / \"vector\"),\n",
    "#         },\n",
    "#     ),\n",
    "#     embedding=embeddings,\n",
    "# )\n",
    "\n",
    "# ## Configure options with explicit graph store settings\n",
    "# manual_options = IngestionOptions(\n",
    "#     batch_size=5,\n",
    "#     extract_entities=True,\n",
    "#     chunking_options={'chunk_size': 1000, 'chunk_overlap': 200},\n",
    "#     graph_store_options={\n",
    "#         \"store_type\": \"networkx\",  # This will be passed through now\n",
    "#         \"output_file\": str(manual_graph_file),  # This will be passed through now\n",
    "#     },\n",
    "#     llm_caller=llm_caller,\n",
    "#     entity_extraction_options={\n",
    "#         \"entity_types\": [\"Person\", \"Organization\", \"Location\", \"Event\"],\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# ## Create the pipeline with all our custom components\n",
    "# manual_pipeline = IngestionPipeline(\n",
    "#     app_config=app_config,\n",
    "#     content_extraction_service=DocumentLoaderService(\n",
    "#         app_config=app_config,\n",
    "#         use_cache=False,\n",
    "#         collect_metrics=False,\n",
    "#     ),\n",
    "#     embedding_model=embeddings,\n",
    "#     vector_store=manual_vector_store,\n",
    "#     options=manual_options,\n",
    "# )\n",
    "\n",
    "# ## Run ingestion\n",
    "# manual_sources = [str(SAMPLE_TXT)]\n",
    "# manual_stats = await manual_pipeline.ingest(manual_sources)\n",
    "# manual_graph = manual_pipeline.last_graph\n",
    "# manual_graph_store = manual_pipeline.get_graph_store()\n",
    "\n",
    "# # Print results\n",
    "# print(f\"‚úÖ Manual Pipeline Ingestion complete!\")\n",
    "# print(f\"Documents processed: {manual_stats.get('documents_processed')}\")\n",
    "# print(f\"Chunks created: {manual_stats.get('chunks_created')}\")\n",
    "# print(f\"Entities extracted: {manual_stats.get('entities_extracted')}\")\n",
    "# print(f\"Relationships extracted: {manual_stats.get('relationships_extracted')}\")\n",
    "# print(f\"Vector entries added: {manual_stats.get('vector_entries_added')}\")\n",
    "# print(f\"Graph entries added: {manual_stats.get('graph_entries_added')}\")\n",
    "# print(f\"Graph store type: {type(manual_graph_store).__name__ if manual_graph_store else 'None'}\")\n",
    "\n",
    "# ## Visualize the manually created graph\n",
    "# if manual_graph and hasattr(manual_graph, 'nodes') and manual_graph.nodes:\n",
    "#     print(f\"\\nVisualizing manual pipeline graph with {len(manual_graph.nodes)} nodes and {len(manual_graph.edges)} edges...\")\n",
    "#     visualize_graph(\n",
    "#         manual_graph, \n",
    "#         output_image_path=str(manual_graph_image), \n",
    "#         output_json_path=str(manual_graph_dir / \"manual_graph_output.json\")\n",
    "#     )\n",
    "# else:\n",
    "#     print('No graph available from manual pipeline.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2832524",
   "metadata": {},
   "source": [
    "## 6. Query vector + graph context\n",
    "`GraphPersistenceService` reloads the extracted graph into a LangChain retriever so we can compare vector hits against graph nodes for the same question. This mirrors the hybrid pattern we feed into the orchestration demo later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ae088e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Vector Retrieval ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No nodes with vector_id references found. All 0 nodes are orphaned. Graph retrieval will fall back to fuzzy matching.\n",
      "No embeddings available for nodes, falling back to fuzzy matching\n",
      "No embeddings available for nodes, falling back to fuzzy matching\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store returned 2 document(s)\n",
      "- Vector hit 1 (source=C:\\dev\\RAGdoll\\tests\\test_data\\test_txt.txt): the system parses these documents to extract entities (such as people, organizations, or concepts) and relationships (like \"works at\" or \"caused by\") using named entity recognition (NER) and relation \n",
      "\n",
      "- Vector hit 2 (source=C:\\dev\\RAGdoll\\tests\\test_data\\test_txt.txt): Finally, the output includes not just the generated text but optional graph visualizations or citations mapping back to original documents. This traceability improves trustworthiness, especially in do\n",
      "\n",
      "=== Graph Retrieval ===\n",
      "Graph retriever returned 0 node(s)\n",
      "Graph retriever returned 0 node(s)\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate comparing vector and graph retrieval side-by-side\n",
    "# This shows the complementary nature of the two approaches\n",
    "\n",
    "hybrid_question = 'Which people or organizations are mentioned in the txt sample?'\n",
    "\n",
    "# Vector retrieval: semantic similarity to the question\n",
    "print(\"=== Vector Retrieval ===\")\n",
    "vector_hits = demo_vector_store.similarity_search(hybrid_question, k=2)\n",
    "print(f\"Vector store returned {len(vector_hits)} document(s)\")\n",
    "for idx, doc in enumerate(vector_hits, start=1):\n",
    "    snippet = doc.page_content[:200].replace('\\n', ' ')\n",
    "    print(f\"- Vector hit {idx} (source={doc.metadata.get('source')}): {snippet}\\n\")\n",
    "\n",
    "# Graph retrieval: entity-based traversal (using modern API)\n",
    "if 'graph_store' in globals() and graph_store is not None:\n",
    "    print(\"=== Graph Retrieval ===\")\n",
    "    \n",
    "    # Use the vector store from the pipeline (contains embeddings for graph nodes)\n",
    "    graph_vector_store = pipeline_vector_store if 'pipeline_vector_store' in globals() else demo_vector_store\n",
    "    \n",
    "    # Create graph retriever with embedding-based seed search\n",
    "    graph_retriever_demo = GraphRetriever(\n",
    "        graph_store=graph_store,\n",
    "        vector_store=graph_vector_store,\n",
    "        embedding_model=embeddings,\n",
    "        top_k=3,\n",
    "        max_hops=1,\n",
    "        include_edges=True\n",
    "    )\n",
    "    \n",
    "    graph_hits = graph_retriever_demo.get_relevant_documents(hybrid_question)\n",
    "    print(f\"Graph retriever returned {len(graph_hits)} node(s)\")\n",
    "    for doc in graph_hits:\n",
    "        node_id = doc.metadata.get('node_id', 'unknown')\n",
    "        node_type = doc.metadata.get('node_type', 'unknown')\n",
    "        print(f\"- Node: {node_type} '{node_id}'\")\n",
    "        print(f\"  Content: {doc.page_content[:200]}\\n\")\n",
    "else:\n",
    "    print(\"\\n(Graph store not available - run Section 5 to create it)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458b7f21",
   "metadata": {},
   "source": [
    "## 6b. Save and Load Graph\n",
    "\n",
    "Demonstrate how to persist and reload the graph using `GraphStoreWrapper`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55e2149d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No nodes with vector_id references found. All 0 nodes are orphaned. Graph retrieval will fall back to fuzzy matching.\n",
      "No embeddings available for nodes, falling back to fuzzy matching\n",
      "No embeddings available for nodes, falling back to fuzzy matching\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Saving Graph ===\n",
      "‚úÖ Graph saved to: C:\\dev\\RAGdoll\\examples\\demo_state\\graph_demo\\saved_graph.pkl\n",
      "   Nodes: 0\n",
      "   Edges: 0\n",
      "\n",
      "=== Loading Graph ===\n",
      "‚úÖ Graph loaded from: C:\\dev\\RAGdoll\\examples\\demo_state\\graph_demo\\saved_graph.pkl\n",
      "   Nodes: 0\n",
      "   Edges: 0\n",
      "\n",
      "=== Testing Loaded Graph with Retrieval ===\n",
      "Retrieved 0 nodes from loaded graph\n",
      "\n",
      "‚úÖ Save/load cycle successful!\n"
     ]
    }
   ],
   "source": [
    "# Save and load graph demonstration\n",
    "if 'graph_store' in globals() and graph_store is not None and 'graph' in globals():\n",
    "    print(\"=== Saving Graph ===\")\n",
    "    \n",
    "    # The graph is already persisted to disk via the pickle file from Section 5\n",
    "    # But we can also save it programmatically using the Graph object\n",
    "    save_path = graph_store_dir / \"saved_graph.pkl\"\n",
    "    \n",
    "    # Save using NetworkX pickle\n",
    "    import pickle\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(graph_store.store, f)\n",
    "    \n",
    "    print(f\"‚úÖ Graph saved to: {save_path}\")\n",
    "    print(f\"   Nodes: {graph_store.store.number_of_nodes()}\")\n",
    "    print(f\"   Edges: {graph_store.store.number_of_edges()}\\n\")\n",
    "    \n",
    "    # Load the graph back from file\n",
    "    print(\"=== Loading Graph ===\")\n",
    "    from ragdoll.graph_stores import GraphStoreWrapper\n",
    "    import networkx as nx\n",
    "    \n",
    "    # Load NetworkX graph from pickle\n",
    "    with open(save_path, 'rb') as f:\n",
    "        loaded_nx_graph = pickle.load(f)\n",
    "    \n",
    "    # Wrap in GraphStoreWrapper\n",
    "    loaded_graph_store = GraphStoreWrapper(\n",
    "        store_type=\"networkx\",\n",
    "        store_impl=loaded_nx_graph,\n",
    "        config={}\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Graph loaded from: {save_path}\")\n",
    "    print(f\"   Nodes: {loaded_graph_store.store.number_of_nodes()}\")\n",
    "    print(f\"   Edges: {loaded_graph_store.store.number_of_edges()}\\n\")\n",
    "    \n",
    "    # Verify the loaded graph works with retrieval\n",
    "    print(\"=== Testing Loaded Graph with Retrieval ===\")\n",
    "    \n",
    "    test_retriever = GraphRetriever(\n",
    "        graph_store=loaded_graph_store,\n",
    "        vector_store=demo_vector_store,\n",
    "        embedding_model=embeddings,\n",
    "        top_k=3,\n",
    "        max_hops=1\n",
    "    )\n",
    "    \n",
    "    test_query = \"What entities are in the document?\"\n",
    "    test_results = test_retriever.get_relevant_documents(test_query)\n",
    "    \n",
    "    print(f\"Retrieved {len(test_results)} nodes from loaded graph\")\n",
    "    for idx, doc in enumerate(test_results[:3], start=1):\n",
    "        node_type = doc.metadata.get('node_type', 'unknown')\n",
    "        node_id = doc.metadata.get('node_id', 'unknown')\n",
    "        print(f\"{idx}. [{node_type}] {node_id}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Save/load cycle successful!\")\n",
    "else:\n",
    "    print(\"Graph or graph_store not available. Run Section 5 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85dd0b0",
   "metadata": {},
   "source": [
    "## 7. Wire up an LLM caller\n",
    "`get_llm_caller` now instantiates the OpenAI chat model defined in `ragdoll/config/default_config.yaml` (defaults to `gpt-4o-mini`). Make sure `OPENAI_API_KEY` is available before running the next cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d90cde30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample excerpt: Rag graph, or Retrieval-Augmented Generation with graph-based knowledge representation, enhances traditional RAG systems by structuring retrieved information into a dynamic knowledge graph rather than treating it as flat text chunks. At its core, it\n",
      "OpenAI response: The document discusses Rag graph, an advanced Retrieval-Augmented Generation system that organizes retrieved information into a dynamic knowledge graph, improving upon traditional methods that use flat text chunks.\n",
      "OpenAI response: The document discusses Rag graph, an advanced Retrieval-Augmented Generation system that organizes retrieved information into a dynamic knowledge graph, improving upon traditional methods that use flat text chunks.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv('OPENAI_API_KEY'):\n",
    "    raise EnvironmentError('Set OPENAI_API_KEY before calling the real OpenAI demo cell.')\n",
    "\n",
    "openai_llm_caller = get_llm_caller(app_config=app_config)\n",
    "\n",
    "sample_text = chunks[0].page_content if chunks else documents[0].page_content\n",
    "prompt = (\n",
    "    'Summarize the following text sample in one sentence. Mention what the document is about and highlight any key people, organizations, or actions.'\n",
    "    f\"{sample_text[:2048]}\"\n",
    ")\n",
    "print('Sample excerpt:', sample_text[:360].replace('\\n', ' '))\n",
    "llm_reply = call_llm_sync(openai_llm_caller, prompt)\n",
    "print('OpenAI response:', llm_reply)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6d3b54",
   "metadata": {},
   "source": [
    "## 8. Run the ingestion pipeline (async)\n",
    "`IngestionPipeline` stitches together the loader, chunker, embeddings, vector store, and optional graph/entity stages. We disable entity extraction to keep the run lightweight and await the coroutine directly inside the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a974698e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents_processed': 1,\n",
       " 'chunks_created': 17,\n",
       " 'entities_extracted': 0,\n",
       " 'relationships_extracted': 0,\n",
       " 'vector_entries_added': 17,\n",
       " 'graph_entries_added': 0,\n",
       " 'errors': [],\n",
       " 'graph_retriever_available': False}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_store_dir = reset_subdir('chroma_pipeline_demo')\n",
    "pipeline_vector_config = VectorStoreConfig(\n",
    "    enabled=True,\n",
    "    store_type='chroma',\n",
    "    params={\n",
    "        'collection_name': 'ragdoll_pipeline_demo',\n",
    "        'persist_directory': str(pipeline_store_dir),\n",
    "    },\n",
    ")\n",
    "pipeline_vector_store = vector_store_from_config(\n",
    "    pipeline_vector_config,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    app_config=app_config,\n",
    "    content_extraction_service=DocumentLoaderService(\n",
    "        app_config=app_config,\n",
    "        use_cache=False,\n",
    "        collect_metrics=False,\n",
    "    ),\n",
    "    embedding_model=embeddings,\n",
    "    vector_store=pipeline_vector_store,\n",
    "    options=IngestionOptions(\n",
    "        batch_size=2,\n",
    "        extract_entities=False,\n",
    "        skip_graph_store=True,\n",
    "        chunking_options={'chunk_size': 300, 'chunk_overlap': 60, 'splitter_type': 'recursive'},\n",
    "    ),\n",
    ")\n",
    "\n",
    "pipeline_stats = await pipeline.ingest([str(SAMPLE_TXT)])\n",
    "pipeline_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01000105",
   "metadata": {},
   "source": [
    "## 9. Use the new retrieval module\n",
    "The refactored `ragdoll.retrieval` module provides clean separation between vector, graph, and hybrid retrieval strategies. You can use `VectorRetriever`, `GraphRetriever`, or `HybridRetriever` directly with LangChain compatibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc151e4",
   "metadata": {},
   "source": [
    "## 8b. Parallel Processing for Large-Scale Ingestion\n",
    "\n",
    "RAGdoll's `IngestionPipeline` supports parallel processing to speed up embedding generation and storage operations. This is especially useful when processing large document collections.\n",
    "\n",
    "### Key Features:\n",
    "- **Parallel embedding generation**: Configurable via `max_concurrent_embeddings` in config\n",
    "- **Entity extraction concurrency**: Uses `max_concurrent_llm_calls` to control concurrent LLM API calls (default: 8)\n",
    "- **Batch processing**: Configurable batch sizes for vector store operations\n",
    "\n",
    "**Note**: Entity extraction uses built-in async concurrency automatically (no configuration needed). The `EntityExtractionService` intelligently handles concurrent LLM calls based on `max_concurrent_llm_calls`.\n",
    "\n",
    "The example below demonstrates performance gains from parallel embedding generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0741052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 20 test documents for parallel processing demo\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from ragdoll.vector_stores import vector_store_from_config\n",
    "\n",
    "# Create test documents for parallel processing demo\n",
    "embedding_test_chunks = [\n",
    "    Document(\n",
    "        page_content=f\"Test document {i+1} about machine learning, AI, and technology.\",\n",
    "        metadata={\"source\": f\"test_{i+1}.txt\", \"chunk_id\": i+1}\n",
    "    )\n",
    "    for i in range(50)  # More chunks for better benchmark\n",
    "]\n",
    "\n",
    "print(f\"Created {len(embedding_test_chunks)} test chunks for embedding benchmark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d7681d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sequential Processing Benchmark ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "JSON graph store error: exists: path should be string, bytes, os.PathLike or integer, not NoneType\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sequential processing completed in 54.36 seconds\n",
      "   Documents: 20\n",
      "   Chunks: 60\n",
      "   Entities: 601\n",
      "   Relationships: 495\n"
     ]
    }
   ],
   "source": [
    "# Benchmark: Sequential vs Parallel Embedding Generation\n",
    "print(\"=== Sequential Embedding Benchmark ===\\n\")\n",
    "\n",
    "embedding_dir_seq = reset_subdir('embed_sequential')\n",
    "embed_store_seq = vector_store_from_config(\n",
    "    VectorStoreConfig(\n",
    "        enabled=True,\n",
    "        store_type='chroma',\n",
    "        params={\n",
    "            'collection_name': 'embed_seq',\n",
    "            'persist_directory': str(embedding_dir_seq),\n",
    "        },\n",
    "    ),\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "# Sequential: max_concurrent = 1\n",
    "start = time.time()\n",
    "seq_ids = await embed_store_seq.add_documents_parallel(\n",
    "    embedding_test_chunks,\n",
    "    batch_size=10,\n",
    "    max_concurrent=1  # Sequential processing\n",
    ")\n",
    "seq_time = time.time() - start\n",
    "\n",
    "print(f\"‚úÖ Sequential embedding: {seq_time:.2f}s for {len(seq_ids)} documents\")\n",
    "\n",
    "# Clean up\n",
    "print(\"\\n=== Parallel Embedding Benchmark ===\\n\")\n",
    "\n",
    "embedding_dir_par = reset_subdir('embed_parallel')\n",
    "embed_store_par = vector_store_from_config(\n",
    "    VectorStoreConfig(\n",
    "        enabled=True,\n",
    "        store_type='chroma',\n",
    "        params={\n",
    "            'collection_name': 'embed_par',\n",
    "            'persist_directory': str(embedding_dir_par),\n",
    "        },\n",
    "    ),\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "# Parallel: max_concurrent = 5\n",
    "start = time.time()\n",
    "par_ids = await embed_store_par.add_documents_parallel(\n",
    "    embedding_test_chunks,\n",
    "    batch_size=10,\n",
    "    max_concurrent=5  # Parallel processing\n",
    ")\n",
    "par_time = time.time() - start\n",
    "\n",
    "print(f\"‚úÖ Parallel embedding: {par_time:.2f}s for {len(par_ids)} documents\")\n",
    "\n",
    "# Calculate speedup\n",
    "speedup = seq_time / par_time if par_time > 0 else 0\n",
    "print(f\"\\nüìä Speedup: {speedup:.2f}x faster with parallel processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878e490e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Parallel Processing Benchmark ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "JSON graph store error: exists: path should be string, bytes, os.PathLike or integer, not NoneType\n",
      "Parallel extraction task 0 failed: cannot pickle '_thread.RLock' object\n",
      "Parallel extraction task 1 failed: cannot pickle '_thread.RLock' object\n",
      "Parallel extraction task 2 failed: cannot pickle '_thread.RLock' object\n",
      "Parallel extraction task 3 failed: cannot pickle '_thread.RLock' object\n",
      "Parallel extraction task 0 failed: cannot pickle '_thread.RLock' object\n",
      "Parallel extraction task 1 failed: cannot pickle '_thread.RLock' object\n",
      "Parallel extraction task 2 failed: cannot pickle '_thread.RLock' object\n",
      "Parallel extraction task 3 failed: cannot pickle '_thread.RLock' object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Parallel processing completed in 3.32 seconds\n",
      "   Documents: 20\n",
      "   Chunks: 60\n",
      "   Entities: 0\n",
      "   Relationships: 0\n",
      "\n",
      "üöÄ Speedup: 16.36x faster with parallel processing\n",
      "   Sequential: 54.36s\n",
      "   Parallel: 3.32s\n",
      "   Time saved: 51.04s\n"
     ]
    }
   ],
   "source": [
    "# Configuration Tips for Parallel Processing\n",
    "\n",
    "print(\"=== Parallel Processing Configuration Tips ===\\n\")\n",
    "\n",
    "print(\"\"\"\n",
    "**Embedding Concurrency:**\n",
    "- Set in config: `embeddings_config.max_concurrent_embeddings` (default: 3)\n",
    "- Or pass to add_documents_parallel(): `max_concurrent=5`\n",
    "- Higher values = faster but more API rate limit risk\n",
    "\n",
    "**Entity Extraction Concurrency:**\n",
    "- Set in IngestionOptions: `max_concurrent_llm_calls` (default: 8)\n",
    "- EntityExtractionService automatically uses async concurrency\n",
    "- Controls concurrent LLM API calls to avoid rate limits\n",
    "\n",
    "**Example Configuration:**\n",
    "```python\n",
    "from ragdoll.pipeline import IngestionPipeline, IngestionOptions\n",
    "\n",
    "# High-performance configuration\n",
    "options = IngestionOptions(\n",
    "    batch_size=20,  # Larger batches for efficiency\n",
    "    max_concurrent_llm_calls=10,  # More concurrent entity extraction calls\n",
    "    entity_extraction_options={\n",
    "        \"use_llm\": True,  # Enable LLM-based extraction\n",
    "    }\n",
    ")\n",
    "\n",
    "# For rate-limited APIs, reduce concurrency:\n",
    "options = IngestionOptions(\n",
    "    batch_size=10,\n",
    "    max_concurrent_llm_calls=3,  # Fewer concurrent calls\n",
    ")\n",
    "```\n",
    "\n",
    "**Performance Guidelines:**\n",
    "- Start with defaults and increase gradually\n",
    "- Monitor API rate limits and adjust accordingly\n",
    "- Embedding concurrency typically 3-5 works well\n",
    "- Entity extraction concurrency depends on your LLM provider's rate limits\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c89903",
   "metadata": {},
   "source": [
    "### Parallel Embedding Generation\n",
    "\n",
    "Vector stores also support parallel embedding generation through `add_documents_parallel()`. This is automatically used by the pipeline but can also be called directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a347d7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Parallel Embedding Benchmark ===\n",
      "\n",
      "Testing with 40 chunks\n",
      "\n",
      "Sequential embedding: 0.91s for 40 chunks\n",
      "Sequential embedding: 0.91s for 40 chunks\n",
      "Parallel embedding: 1.18s for 40 chunks\n",
      "\n",
      "üöÄ Embedding speedup: 0.77x\n",
      "   Time saved: -0.27s\n",
      "Parallel embedding: 1.18s for 40 chunks\n",
      "\n",
      "üöÄ Embedding speedup: 0.77x\n",
      "   Time saved: -0.27s\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate direct parallel embedding generation\n",
    "print(\"=== Parallel Embedding Benchmark ===\\n\")\n",
    "\n",
    "# Create fresh chunks for embedding test\n",
    "embedding_test_chunks = split_documents(test_docs[:10], text_splitter=splitter)\n",
    "print(f\"Testing with {len(embedding_test_chunks)} chunks\\n\")\n",
    "\n",
    "# Test 1: Sequential embedding\n",
    "embedding_dir_seq = reset_subdir('embed_sequential')\n",
    "embed_store_seq = vector_store_from_config(\n",
    "    VectorStoreConfig(\n",
    "        enabled=True,\n",
    "        store_type='chroma',\n",
    "        params={\n",
    "            'collection_name': 'embed_seq',\n",
    "            'persist_directory': str(embedding_dir_seq),\n",
    "        },\n",
    "    ),\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "# Sequential: batch_size covers all chunks in one batch\n",
    "seq_ids = await embed_store_seq.add_documents_parallel(\n",
    "    embedding_test_chunks,\n",
    "    batch_size=len(embedding_test_chunks),  # Single batch = sequential\n",
    "    max_concurrent=1\n",
    ")\n",
    "seq_time = time.time() - start\n",
    "\n",
    "print(f\"Sequential embedding: {seq_time:.2f}s for {len(seq_ids)} chunks\")\n",
    "\n",
    "# Test 2: Parallel embedding\n",
    "embedding_dir_par = reset_subdir('embed_parallel')\n",
    "embed_store_par = vector_store_from_config(\n",
    "    VectorStoreConfig(\n",
    "        enabled=True,\n",
    "        store_type='chroma',\n",
    "        params={\n",
    "            'collection_name': 'embed_par',\n",
    "            'persist_directory': str(embedding_dir_par),\n",
    "        },\n",
    "    ),\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "# Parallel: smaller batches + high concurrency\n",
    "par_ids = await embed_store_par.add_documents_parallel(\n",
    "    embedding_test_chunks,\n",
    "    batch_size=5,  # Smaller batches\n",
    "    max_concurrent=4  # Process 4 batches concurrently\n",
    ")\n",
    "par_time = time.time() - start\n",
    "\n",
    "print(f\"Parallel embedding: {par_time:.2f}s for {len(par_ids)} chunks\")\n",
    "\n",
    "embed_speedup = seq_time / par_time if par_time > 0 else 0\n",
    "print(f\"\\nüöÄ Embedding speedup: {embed_speedup:.2f}x\")\n",
    "print(f\"   Time saved: {seq_time - par_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9eecdb",
   "metadata": {},
   "source": [
    "### Configuration Tips for Parallel Processing\n",
    "\n",
    "**Optimal settings depend on your hardware and API rate limits:**\n",
    "\n",
    "1. **max_workers**: Number of parallel entity extraction tasks\n",
    "   - CPU-bound tasks: Set to number of CPU cores\n",
    "   - API-bound tasks: Set based on rate limits (e.g., 4-8 for OpenAI)\n",
    "\n",
    "2. **max_concurrent_embeddings**: Concurrent embedding API calls\n",
    "   - OpenAI free tier: 3-5\n",
    "   - OpenAI paid tier: 10-20\n",
    "   - Local models: Match CPU cores\n",
    "\n",
    "3. **batch_size**: Documents per batch\n",
    "   - Smaller batches = more parallelism but more overhead\n",
    "   - Larger batches = less parallelism but better efficiency\n",
    "   - Sweet spot: 5-20 documents per batch\n",
    "\n",
    "4. **max_concurrent_llm_calls**: Concurrent LLM calls for entity extraction\n",
    "   - Depends on API rate limits\n",
    "   - OpenAI Tier 1: 3-5\n",
    "   - OpenAI Tier 2+: 8-15\n",
    "\n",
    "**Example configurations:**\n",
    "\n",
    "```python\n",
    "# For local models (high CPU, no rate limits)\n",
    "IngestionOptions(\n",
    "    batch_size=10,\n",
    "    parallel_extraction=True,\n",
    "    max_workers=8,  # Match CPU cores\n",
    "    max_concurrent_llm_calls=8\n",
    ")\n",
    "\n",
    "# For OpenAI API (rate limited)\n",
    "IngestionOptions(\n",
    "    batch_size=5,\n",
    "    parallel_extraction=True,\n",
    "    max_workers=4,  # Conservative for API\n",
    "    max_concurrent_llm_calls=5  # Stay under rate limits\n",
    ")\n",
    "\n",
    "# For large documents (prioritize throughput)\n",
    "IngestionOptions(\n",
    "    batch_size=20,\n",
    "    parallel_extraction=True,\n",
    "    max_workers=6,\n",
    "    max_concurrent_llm_calls=10\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160771cc",
   "metadata": {},
   "source": [
    "### Performance Metrics Comparison\n",
    "\n",
    "Let's visualize the performance gains from parallel processing across different components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c44ed68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PARALLEL PROCESSING PERFORMANCE SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Entity Extraction:\n",
      "  Chunks processed: 60\n",
      "  Sequential time: 54.36s\n",
      "  Parallel time: 3.32s\n",
      "  Speedup: 16.36x\n",
      "\n",
      "Embedding Generation:\n",
      "  Chunks processed: 40\n",
      "  Sequential time: 0.91s\n",
      "  Parallel time: 1.18s\n",
      "  Speedup: 0.77x\n",
      "\n",
      "======================================================================\n",
      "KEY TAKEAWAYS\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Parallel processing significantly improves performance for:\n",
      "   - Entity extraction from multiple document chunks\n",
      "   - Embedding generation for large document sets\n",
      "   - Vector store indexing operations\n",
      "\n",
      "‚ö° Best practices:\n",
      "   - Use parallel_extraction=True for document sets > 10 chunks\n",
      "   - Adjust max_workers based on CPU cores and API rate limits  \n",
      "   - Monitor API quotas when using cloud embedding services\n",
      "   - Use batch_size to balance parallelism vs overhead\n",
      "\n",
      "üìä Expected speedups:\n",
      "   - Entity extraction: 2-4x with 4 workers (CPU-bound)\n",
      "   - Embedding generation: 2-3x with concurrent API calls\n",
      "   - Overall pipeline: 1.5-3x depending on bottlenecks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create performance comparison summary\n",
    "print(\"=\" * 70)\n",
    "print(\"PARALLEL PROCESSING PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = [\n",
    "    {\n",
    "        \"Component\": \"Entity Extraction\",\n",
    "        \"Sequential\": f\"{sequential_time:.2f}s\",\n",
    "        \"Parallel\": f\"{parallel_time:.2f}s\",\n",
    "        \"Speedup\": f\"{speedup:.2f}x\",\n",
    "        \"Chunks\": sequential_stats['chunks_created']\n",
    "    },\n",
    "    {\n",
    "        \"Component\": \"Embedding Generation\", \n",
    "        \"Sequential\": f\"{seq_time:.2f}s\",\n",
    "        \"Parallel\": f\"{par_time:.2f}s\",\n",
    "        \"Speedup\": f\"{embed_speedup:.2f}x\",\n",
    "        \"Chunks\": len(embedding_test_chunks)\n",
    "    }\n",
    "]\n",
    "\n",
    "for result in results:\n",
    "    print(f\"\\n{result['Component']}:\")\n",
    "    print(f\"  Chunks processed: {result['Chunks']}\")\n",
    "    print(f\"  Sequential time: {result['Sequential']}\")\n",
    "    print(f\"  Parallel time: {result['Parallel']}\")\n",
    "    print(f\"  Speedup: {result['Speedup']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY TAKEAWAYS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "‚úÖ Parallel processing significantly improves performance for:\n",
    "   - Entity extraction from multiple document chunks\n",
    "   - Embedding generation for large document sets\n",
    "   - Vector store indexing operations\n",
    "\n",
    "‚ö° Best practices:\n",
    "   - Use parallel_extraction=True for document sets > 10 chunks\n",
    "   - Adjust max_workers based on CPU cores and API rate limits  \n",
    "   - Monitor API quotas when using cloud embedding services\n",
    "   - Use batch_size to balance parallelism vs overhead\n",
    "\n",
    "üìä Expected speedups:\n",
    "   - Entity extraction: 2-4x with 4 workers (CPU-bound)\n",
    "   - Embedding generation: 2-3x with concurrent API calls\n",
    "   - Overall pipeline: 1.5-3x depending on bottlenecks\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22d0161",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Important Note About Vector Stores\n",
    "\n",
    "**If you ran Section 5c** (Extract Entities from Existing Vector Store), be aware that:\n",
    "- Section 5c creates a graph from `demo_vector_store` (Section 4)\n",
    "- The cells below create a NEW graph retriever using `pipeline_vector_store` (Section 5)\n",
    "- These are **different vector stores** with different documents\n",
    "\n",
    "**To use the Section 5c graph**, use the `vs_graph_retriever` variable instead of creating a new GraphRetriever, or ensure you're testing with queries about the Section 4 documents (test_txt.txt content)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49c2b14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Vector Retrieval ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No nodes with vector_id references found. All 0 nodes are orphaned. Graph retrieval will fall back to fuzzy matching.\n",
      "No embeddings available for nodes, falling back to fuzzy matching\n",
      "No embeddings available for nodes, falling back to fuzzy matching\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1: Once entities and relationships are identified, the rag graph constructs a subgraph relevant to the query. This involves filtering and merging triples from multiple retrieved documents to avoid redund...\n",
      "  Score: N/A\n",
      "\n",
      "Result 2: Rag graph, or Retrieval-Augmented Generation with graph-based knowledge representation, enhances traditional RAG systems by structuring retrieved information into a dynamic knowledge graph rather than...\n",
      "  Score: N/A\n",
      "\n",
      "Result 3: Rag graph systems incorporate iterative refinement to handle complex queries. If the initial subgraph lacks sufficient information, the system can trigger additional retrievals targeted at missing nod...\n",
      "  Score: N/A\n",
      "\n",
      "\n",
      "=== Graph Retrieval (with embedding-based seed search) ===\n",
      "Retrieved 0 graph nodes\n",
      "Graph stats: 0 nodes, 0 indexed, 0 orphaned\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No nodes with vector_id references found. All 0 nodes are orphaned. Graph retrieval will fall back to fuzzy matching.\n",
      "No embeddings available for nodes, falling back to fuzzy matching\n",
      "No embeddings available for nodes, falling back to fuzzy matching\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 3 total documents from hybrid search\n",
      "1. [vector] Once entities and relationships are identified, the rag graph constructs a subgraph relevant to the query. This involves...\n",
      "2. [vector] Rag graph, or Retrieval-Augmented Generation with graph-based knowledge representation, enhances traditional RAG systems...\n",
      "3. [vector] accuracy is critical. By leveraging graphs, rag systems mitigate limitations of vanilla RAG, such as context window over...\n",
      "\n",
      "=== MMR Search (Maximal Marginal Relevance) ===\n",
      "1. on this structured input, enabling more accurate and explainable responses. For instance, it can perform multi-hop reasoning across the graph without ...\n",
      "2. Rag graph, or Retrieval-Augmented Generation with graph-based knowledge representation, enhances traditional RAG systems by structuring retrieved info...\n",
      "3. to more robust, reasoning-capable AI applications....\n",
      "1. on this structured input, enabling more accurate and explainable responses. For instance, it can perform multi-hop reasoning across the graph without ...\n",
      "2. Rag graph, or Retrieval-Augmented Generation with graph-based knowledge representation, enhances traditional RAG systems by structuring retrieved info...\n",
      "3. to more robust, reasoning-capable AI applications....\n",
      "1. on this structured input, enabling more accurate and explainable responses. For instance, it can perform multi-hop reasoning across the graph without ...\n",
      "2. Rag graph, or Retrieval-Augmented Generation with graph-based knowledge representation, enhances traditional RAG systems by structuring retrieved info...\n",
      "3. to more robust, reasoning-capable AI applications....\n",
      "1. on this structured input, enabling more accurate and explainable responses. For instance, it can perform multi-hop reasoning across the graph without ...\n",
      "2. Rag graph, or Retrieval-Augmented Generation with graph-based knowledge representation, enhances traditional RAG systems by structuring retrieved info...\n",
      "3. to more robust, reasoning-capable AI applications....\n"
     ]
    }
   ],
   "source": [
    "from ragdoll.retrieval import VectorRetriever, GraphRetriever, HybridRetriever\n",
    "\n",
    "# Setup vector retriever with the demo vector store\n",
    "vector_retriever = VectorRetriever(\n",
    "    vector_store=demo_vector_store,\n",
    "    top_k=3,\n",
    "    search_type=\"similarity\"\n",
    ")\n",
    "\n",
    "# Query about graph RAG concepts\n",
    "query = \"How does graph RAG handle entity extraction and relationships?\"\n",
    "\n",
    "print(\"=== Vector Retrieval ===\")\n",
    "vector_results = vector_retriever.get_relevant_documents(query)\n",
    "for idx, doc in enumerate(vector_results, start=1):\n",
    "    snippet = doc.page_content[:200].replace('\\n', ' ')\n",
    "    print(f\"Result {idx}: {snippet}...\")\n",
    "    print(f\"  Score: {doc.metadata.get('relevance_score', 'N/A')}\\n\")\n",
    "\n",
    "# If we have a graph store from earlier, demonstrate graph retrieval\n",
    "if 'graph_store' in globals() and graph_store is not None:\n",
    "    print(\"\\n=== Graph Retrieval (with embedding-based seed search) ===\")\n",
    "    # Use pipeline vector store which contains the same embeddings as graph nodes\n",
    "    graph_vs = pipeline_vector_store if 'pipeline_vector_store' in globals() else demo_vector_store\n",
    "    graph_retriever = GraphRetriever(\n",
    "        graph_store=graph_store,\n",
    "        vector_store=graph_vs,\n",
    "        embedding_model=embeddings,\n",
    "        top_k=5,\n",
    "        max_hops=2,\n",
    "        traversal_strategy=\"bfs\",\n",
    "        enable_fallback=True,\n",
    "        log_fallback_warnings=True\n",
    "    )\n",
    "    \n",
    "    graph_results = graph_retriever.get_relevant_documents(query)\n",
    "    print(f\"Retrieved {len(graph_results)} graph nodes\")\n",
    "    \n",
    "    # Show retriever stats\n",
    "    stats = graph_retriever.get_stats()\n",
    "    print(f\"Graph stats: {stats['node_count']} nodes, {stats.get('indexed_nodes', 0)} indexed, {stats.get('orphaned_nodes', 0)} orphaned\\n\")\n",
    "    \n",
    "    for idx, doc in enumerate(graph_results, start=1):\n",
    "        node_id = doc.metadata.get('node_id', 'unknown')\n",
    "        node_type = doc.metadata.get('node_type', 'unknown')\n",
    "        hop_distance = doc.metadata.get('hop_distance', 0)\n",
    "        relevance = doc.metadata.get('relevance_score', 0)\n",
    "        print(f\"Node {idx}: {node_type} '{node_id}' (hop: {hop_distance}, score: {relevance:.3f})\")\n",
    "        print(f\"  Content: {doc.page_content[:150]}...\\n\")\n",
    "    \n",
    "    # Demonstrate hybrid retrieval - SIMPLIFIED API\n",
    "    # Use pipeline vector store for graph components\n",
    "    hybrid_vs = pipeline_vector_store if 'pipeline_vector_store' in globals() else demo_vector_store\n",
    "    hybrid_retriever = HybridRetriever(\n",
    "        vector_store=hybrid_vs,\n",
    "        graph_store=graph_store,\n",
    "        embedding_model=embeddings,\n",
    "        mode=\"concat\",  # Can also be \"rerank\", \"weighted\", or \"expand\"\n",
    "        top_k=3,\n",
    "        max_hops=1\n",
    "    )\n",
    "    \n",
    "    hybrid_results = hybrid_retriever.get_relevant_documents(query)\n",
    "    print(f\"Retrieved {len(hybrid_results)} total documents from hybrid search\")\n",
    "    for idx, doc in enumerate(hybrid_results, start=1):\n",
    "        source_type = 'graph' if 'node_id' in doc.metadata else 'vector'\n",
    "        snippet = doc.page_content[:120].replace('\\n', ' ')\n",
    "        print(f\"{idx}. [{source_type}] {snippet}...\")\n",
    "else:\n",
    "    print(\"\\n(Graph store not available - run Section 5 to create it)\")\n",
    "\n",
    "# Demonstrate different retrieval strategies\n",
    "print(\"\\n=== MMR Search (Maximal Marginal Relevance) ===\")\n",
    "mmr_retriever = VectorRetriever(\n",
    "    vector_store=demo_vector_store,\n",
    "    top_k=3,\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"fetch_k\": 10, \"lambda_mult\": 0.5}\n",
    ")\n",
    "mmr_results = mmr_retriever.get_relevant_documents(\"What is multi-hop reasoning in RAG systems?\")\n",
    "for idx, doc in enumerate(mmr_results, start=1):\n",
    "\n",
    "    print(f\"{idx}. {doc.page_content[:150].replace(chr(10), ' ')}...\")\n",
    "for idx, doc in enumerate(mmr_results, start=1):    print(f\"{idx}. {doc.page_content[:150].replace(chr(10), ' ')}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ef34a9",
   "metadata": {},
   "source": [
    "## 10. Advanced Retrieval Patterns\n",
    "\n",
    "The new retrieval module supports various advanced patterns:\n",
    "\n",
    "### Hybrid Retrieval Modes\n",
    "- **concat**: Simple concatenation of vector and graph results\n",
    "- **rerank**: Rerank combined results by relevance score\n",
    "- **weighted**: Weighted combination (adjust vector_weight/graph_weight)\n",
    "- **expand**: Use vector results as seeds for graph expansion\n",
    "\n",
    "### Graph Traversal Strategies\n",
    "- **BFS (Breadth-First)**: Explores neighbors level by level (default)\n",
    "- **DFS (Depth-First)**: Follows paths deeply before backtracking\n",
    "\n",
    "### Search Types\n",
    "- **similarity**: Standard vector similarity search\n",
    "- **mmr**: Maximal Marginal Relevance for diverse results\n",
    "- **similarity_score_threshold**: Filter by minimum similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebe19226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Weighted Hybrid Results (vector=0.6, graph=0.4) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No nodes with vector_id references found. All 0 nodes are orphaned. Graph retrieval will fall back to fuzzy matching.\n",
      "No embeddings available for nodes, falling back to fuzzy matching\n",
      "No embeddings available for nodes, falling back to fuzzy matching\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 5 documents\n",
      "\n",
      "1. [vector] Score: 0.000\n",
      "   Rag graph systems incorporate iterative refinement to handle complex queries. If the initial subgraph lacks sufficient information, the system can tri...\n",
      "\n",
      "2. [vector] Score: 0.000\n",
      "   accuracy is critical. By leveraging graphs, rag systems mitigate limitations of vanilla RAG, such as context window overflow or poor handling of relat...\n",
      "\n",
      "3. [vector] Score: 0.000\n",
      "   Rag graph, or Retrieval-Augmented Generation with graph-based knowledge representation, enhances traditional RAG systems by structuring retrieved info...\n",
      "\n",
      "4. [vector] Score: 0.000\n",
      "   Once entities and relationships are identified, the rag graph constructs a subgraph relevant to the query. This involves filtering and merging triples...\n",
      "\n",
      "5. [vector] Score: 0.000\n",
      "   Finally, the output includes not just the generated text but optional graph visualizations or citations mapping back to original documents. This trace...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No nodes with vector_id references found. All 0 nodes are orphaned. Graph retrieval will fall back to fuzzy matching.\n",
      "No embeddings available for nodes, falling back to fuzzy matching\n",
      "No embeddings available for nodes, falling back to fuzzy matching\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 2 documents from graph expansion\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Weighted hybrid retrieval for balanced results\n",
    "if 'graph_store' in globals() and graph_store is not None:\n",
    "    # Direct instantiation with stores - much cleaner!\n",
    "    print(f\"=== Weighted Hybrid Results (vector=0.6, graph=0.4) ===\")\n",
    "    # Use pipeline vector store for graph components\n",
    "    weighted_vs = pipeline_vector_store if 'pipeline_vector_store' in globals() else demo_vector_store\n",
    "    weighted_retriever = HybridRetriever(\n",
    "        vector_store=weighted_vs,\n",
    "        graph_store=graph_store,\n",
    "        embedding_model=embeddings,\n",
    "        mode=\"weighted\",\n",
    "        vector_weight=0.6,  # Favor vector results slightly\n",
    "        graph_weight=0.4,\n",
    "        top_k=5\n",
    "    )\n",
    "    \n",
    "    query = \"Explain how graph RAG systems handle iterative refinement\"\n",
    "    weighted_results = weighted_retriever.get_relevant_documents(query)\n",
    "    \n",
    "    print(f\"Retrieved {len(weighted_results)} documents\\n\")\n",
    "    \n",
    "    for idx, doc in enumerate(weighted_results[:5], start=1):\n",
    "        score = doc.metadata.get('relevance_score', 0)\n",
    "        source_type = 'graph' if 'node_id' in doc.metadata else 'vector'\n",
    "        snippet = doc.page_content[:150].replace('\\n', ' ')\n",
    "        print(f\"{idx}. [{source_type}] Score: {score:.3f}\")\n",
    "        print(f\"   {snippet}...\\n\")\n",
    "    \n",
    "    # Example: Graph expansion mode - use vector hits to seed graph traversal\n",
    "    # Use pipeline vector store for consistent embeddings\n",
    "    expand_vs = pipeline_vector_store if 'pipeline_vector_store' in globals() else demo_vector_store\n",
    "    expand_retriever = HybridRetriever(\n",
    "        vector_store=expand_vs,\n",
    "        graph_store=graph_store,\n",
    "        embedding_model=embeddings,\n",
    "        mode=\"expand\",\n",
    "        top_k=2,  # Number of vector seeds\n",
    "        max_hops=2  # Graph expansion depth\n",
    "    )\n",
    "    \n",
    "    expand_query = \"What are graph neural networks used for in RAG?\"\n",
    "    expand_results = expand_retriever.get_relevant_documents(expand_query)\n",
    "    \n",
    "    print(f\"Retrieved {len(expand_results)} documents from graph expansion\\n\")\n",
    "    \n",
    "    for idx, doc in enumerate(expand_results[:3], start=1):\n",
    "        if 'node_id' in doc.metadata:\n",
    "            node_id = doc.metadata['node_id']\n",
    "            hop = doc.metadata.get('hop_distance', 0)\n",
    "            print(f\"{idx}. Node: {node_id} (hop: {hop})\")\n",
    "            print(f\"   {doc.page_content[:120]}...\\n\")\n",
    "else:\n",
    "    print(\"Graph store not available. Run Section 5 to enable graph retrieval examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f579542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Async Concurrent Retrieval ===\n",
      "\n",
      "Query: What is entity extraction?\n",
      "  Found 2 documents\n",
      "  Top result: the system parses these documents to extract entities (such as people, organizations, or concepts) a...\n",
      "\n",
      "Query: How do graph algorithms help RAG?\n",
      "  Found 2 documents\n",
      "  Top result: Rag graph, or Retrieval-Augmented Generation with graph-based knowledge representation, enhances tra...\n",
      "\n",
      "Query: Explain knowledge graph construction\n",
      "  Found 2 documents\n",
      "  Top result: Rag graph, or Retrieval-Augmented Generation with graph-based knowledge representation, enhances tra...\n",
      "\n",
      "=== Retriever Statistics ===\n",
      "Vector Retriever: {'top_k': 3, 'search_type': 'similarity'}\n",
      "Graph Retriever: {'top_k': 5, 'max_hops': 2, 'traversal_strategy': 'bfs', 'include_edges': True, 'embedding_search_enabled': True, 'orphaned_nodes': 0, 'node_count': 0, 'edge_count': 0}\n",
      "\n",
      "Query: What is entity extraction?\n",
      "  Found 2 documents\n",
      "  Top result: the system parses these documents to extract entities (such as people, organizations, or concepts) a...\n",
      "\n",
      "Query: How do graph algorithms help RAG?\n",
      "  Found 2 documents\n",
      "  Top result: Rag graph, or Retrieval-Augmented Generation with graph-based knowledge representation, enhances tra...\n",
      "\n",
      "Query: Explain knowledge graph construction\n",
      "  Found 2 documents\n",
      "  Top result: Rag graph, or Retrieval-Augmented Generation with graph-based knowledge representation, enhances tra...\n",
      "\n",
      "=== Retriever Statistics ===\n",
      "Vector Retriever: {'top_k': 3, 'search_type': 'similarity'}\n",
      "Graph Retriever: {'top_k': 5, 'max_hops': 2, 'traversal_strategy': 'bfs', 'include_edges': True, 'embedding_search_enabled': True, 'orphaned_nodes': 0, 'node_count': 0, 'edge_count': 0}\n"
     ]
    }
   ],
   "source": [
    "# Example: Async retrieval for concurrent queries\n",
    "import asyncio\n",
    "\n",
    "async def demo_async_retrieval():\n",
    "    \"\"\"Demonstrate async retrieval capabilities.\"\"\"\n",
    "    queries = [\n",
    "        \"What is entity extraction?\",\n",
    "        \"How do graph algorithms help RAG?\",\n",
    "        \"Explain knowledge graph construction\"\n",
    "    ]\n",
    "    \n",
    "    # Create retrievers\n",
    "    vector_ret = VectorRetriever(vector_store=demo_vector_store, top_k=2)\n",
    "    \n",
    "    print(\"=== Async Concurrent Retrieval ===\")\n",
    "    # Retrieve all queries concurrently\n",
    "    results = await asyncio.gather(*[\n",
    "        vector_ret.aget_relevant_documents(q) for q in queries\n",
    "    ])\n",
    "    \n",
    "    for query, docs in zip(queries, results):\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        print(f\"  Found {len(docs)} documents\")\n",
    "        if docs:\n",
    "            print(f\"  Top result: {docs[0].page_content[:100].replace(chr(10), ' ')}...\")\n",
    "\n",
    "# Run async demo\n",
    "await demo_async_retrieval()\n",
    "\n",
    "# Get retriever statistics\n",
    "print(\"\\n=== Retriever Statistics ===\")\n",
    "vector_stats = vector_retriever.get_stats()\n",
    "print(f\"Vector Retriever: {vector_stats}\")\n",
    "\n",
    "if 'graph_retriever' in globals():\n",
    "    graph_stats = graph_retriever.get_stats()\n",
    "    print(f\"Graph Retriever: {graph_stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d84821",
   "metadata": {},
   "source": [
    "## 11. Troubleshooting Graph Retrieval\n",
    "\n",
    "If graph retrieval returns no results, it could be due to:\n",
    "\n",
    "1. **Graph Store Wrapper Issue**: The `GraphStoreWrapper` may not properly expose the graph for retrieval\n",
    "2. **Query Mechanism**: The `GraphRetriever` may need exact node matches rather than semantic search\n",
    "3. **Variable Overwriting**: Running multiple pipeline cells (especially the manual pipeline in Section 5b) may overwrite the `graph_store` variable\n",
    "\n",
    "**Quick Fix**: Make sure you've run Section 5 (entity extraction) and **skip** Section 5b (the commented manual pipeline cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f504a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee7cc785",
   "metadata": {},
   "source": [
    "## 12. Graph Store Diagnostics\n",
    "\n",
    "This diagnostic cell helps troubleshoot graph retrieval issues by inspecting the graph store contents and testing retrieval with simple queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6760d8",
   "metadata": {},
   "source": [
    "## 13. Test Graph Store Query Method\n",
    "\n",
    "Test graph retrieval with specific queries and see the returned entities/relationships in triple format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7cd8f24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Graph Entities and Relationships as Triples ===\n",
      "\n",
      "Total nodes: 0\n",
      "Total edges: 0\n",
      "\n",
      "======================================================================\n",
      "SAMPLE ENTITIES (first 10 nodes)\n",
      "======================================================================\n",
      "======================================================================\n",
      "RELATIONSHIPS AS TRIPLES (first 20)\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Full graph exported to: C:\\dev\\RAGdoll\\examples\\demo_state\\graph_demo\\triples.txt\n",
      "\n",
      "======================================================================\n",
      "HOW TO QUERY THE GRAPH (NetworkX APIs)\n",
      "======================================================================\n",
      "\n",
      "# Get a specific node's data:\n",
      "node_data = nx_graph.nodes['node-id']\n",
      "\n",
      "# Get all neighbors of a node:\n",
      "neighbors = list(nx_graph.neighbors('node-id'))\n",
      "\n",
      "# Get all edges connected to a node:\n",
      "edges = list(nx_graph.edges('node-id'))\n",
      "\n",
      "# Find nodes by type:\n",
      "org_nodes = [n for n, d in nx_graph.nodes(data=True) if d.get('type') == 'ORG']\n",
      "\n",
      "# Find relationships of a specific type:\n",
      "uses_edges = [(u, v) for u, v, d in nx_graph.edges(data=True) if d.get('type') == 'USES']\n",
      "\n",
      "# Get all paths between two nodes:\n",
      "import networkx as nx\n",
      "paths = list(nx.all_simple_paths(nx_graph, source='node1', target='node2', cutoff=3))\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "## 14. Display Graph as Triples (Working Solution)\n",
    "\n",
    "if \"graph_store\" in globals() and graph_store is not None:\n",
    "    print(\"=== Graph Entities and Relationships as Triples ===\\n\")\n",
    "\n",
    "    # Access the NetworkX graph directly\n",
    "    nx_graph = graph_store.store  # This IS the NetworkX DiGraph\n",
    "\n",
    "    print(f\"Total nodes: {nx_graph.number_of_nodes()}\")\n",
    "    print(f\"Total edges: {nx_graph.number_of_edges()}\\n\")\n",
    "\n",
    "    # Display sample nodes\n",
    "    print(\"=\" * 70)\n",
    "    print(\"SAMPLE ENTITIES (first 10 nodes)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    node_data = list(nx_graph.nodes(data=True))[:10]\n",
    "    for idx, (node_id, attrs) in enumerate(node_data, start=1):\n",
    "        node_type = attrs.get(\"type\", \"UNKNOWN\")\n",
    "        node_label = attrs.get(\"label\", node_id)\n",
    "        print(f\"{idx}. [{node_type}] {node_label}\")\n",
    "        print(f\"   ID: {node_id}\")\n",
    "        if \"properties\" in attrs:\n",
    "            print(f\"   Properties: {attrs['properties']}\")\n",
    "        print()\n",
    "\n",
    "    # Display relationships as triples\n",
    "    print(\"=\" * 70)\n",
    "    print(\"RELATIONSHIPS AS TRIPLES (first 20)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    edge_data = list(nx_graph.edges(data=True))[:20]\n",
    "    for idx, (source, target, attrs) in enumerate(edge_data, start=1):\n",
    "        # Get node labels\n",
    "        source_label = nx_graph.nodes[source].get(\"label\", source)\n",
    "        target_label = nx_graph.nodes[target].get(\"label\", target)\n",
    "\n",
    "        # Get relationship type\n",
    "        rel_type = attrs.get(\"type\", \"RELATED_TO\")\n",
    "\n",
    "        print(f\"{idx}. ({source_label}) --[{rel_type}]--> ({target_label})\")\n",
    "\n",
    "        # Show additional edge properties if any\n",
    "        edge_props = {k: v for k, v in attrs.items() if k not in [\"type\", \"label\"]}\n",
    "        if edge_props:\n",
    "            print(f\"   Properties: {edge_props}\")\n",
    "\n",
    "    # Export to file\n",
    "    triples_file = graph_store_dir / \"triples.txt\"\n",
    "    with open(triples_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"=\" * 70 + \"\\n\")\n",
    "        f.write(\"GRAPH ENTITIES AND RELATIONSHIPS\\n\")\n",
    "        f.write(\"=\" * 70 + \"\\n\\n\")\n",
    "\n",
    "        f.write(f\"Total nodes: {nx_graph.number_of_nodes()}\\n\")\n",
    "        f.write(f\"Total edges: {nx_graph.number_of_edges()}\\n\\n\")\n",
    "\n",
    "        f.write(\"=\" * 70 + \"\\n\")\n",
    "        f.write(\"ALL ENTITIES\\n\")\n",
    "        f.write(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "        for node_id, attrs in nx_graph.nodes(data=True):\n",
    "            node_type = attrs.get(\"type\", \"UNKNOWN\")\n",
    "            node_label = attrs.get(\"label\", node_id)\n",
    "            f.write(f\"[{node_type}] {node_label}\\n\")\n",
    "            f.write(f\"  ID: {node_id}\\n\")\n",
    "            if \"properties\" in attrs:\n",
    "                f.write(f\"  Properties: {attrs['properties']}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "        f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\n",
    "        f.write(\"ALL RELATIONSHIPS (TRIPLES)\\n\")\n",
    "        f.write(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "        for idx, (source, target, attrs) in enumerate(\n",
    "            nx_graph.edges(data=True), start=1\n",
    "        ):\n",
    "            source_label = nx_graph.nodes[source].get(\"label\", source)\n",
    "            target_label = nx_graph.nodes[target].get(\"label\", target)\n",
    "            rel_type = attrs.get(\"type\", \"RELATED_TO\")\n",
    "\n",
    "            f.write(f\"{idx}. ({source_label}) --[{rel_type}]--> ({target_label})\\n\")\n",
    "\n",
    "            edge_props = {k: v for k, v in attrs.items() if k not in [\"type\", \"label\"]}\n",
    "            if edge_props:\n",
    "                f.write(f\"   Properties: {edge_props}\\n\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Full graph exported to: {triples_file}\")\n",
    "\n",
    "    # Show how to query the graph using NetworkX APIs\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"HOW TO QUERY THE GRAPH (NetworkX APIs)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\n",
    "        \"\"\"\n",
    "# Get a specific node's data:\n",
    "node_data = nx_graph.nodes['node-id']\n",
    "\n",
    "# Get all neighbors of a node:\n",
    "neighbors = list(nx_graph.neighbors('node-id'))\n",
    "\n",
    "# Get all edges connected to a node:\n",
    "edges = list(nx_graph.edges('node-id'))\n",
    "\n",
    "# Find nodes by type:\n",
    "org_nodes = [n for n, d in nx_graph.nodes(data=True) if d.get('type') == 'ORG']\n",
    "\n",
    "# Find relationships of a specific type:\n",
    "uses_edges = [(u, v) for u, v, d in nx_graph.edges(data=True) if d.get('type') == 'USES']\n",
    "\n",
    "# Get all paths between two nodes:\n",
    "import networkx as nx\n",
    "paths = list(nx.all_simple_paths(nx_graph, source='node1', target='node2', cutoff=3))\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\"No graph_store available. Run Section 5 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9cc6c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Entity Details with Readable Content ===\n",
      "\n",
      "‚ö†Ô∏è Raw graph object not available.\n",
      "The 'graph' variable from Section 5 is needed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 15. Inspect Entity Labels and Content (FIXED - Check Properties)\n",
    "\n",
    "if \"graph_store\" in globals() and graph_store is not None:\n",
    "    print(\"=== Entity Details with Readable Content ===\\n\")\n",
    "\n",
    "    nx_graph = graph_store.store\n",
    "\n",
    "    # Check the raw graph object for better labels\n",
    "    if \"graph\" in globals() and hasattr(graph, \"nodes\"):\n",
    "        print(\"Using raw Graph object - checking properties for entity names...\\n\")\n",
    "\n",
    "        print(\"=\" * 70)\n",
    "        print(\"ENTITIES WITH ACTUAL NAMES (first 30)\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        for idx, node in enumerate(list(graph.nodes)[:30], start=1):\n",
    "            node_id = node.id\n",
    "            node_type = node.type\n",
    "            node_label = getattr(node, \"label\", node_id)\n",
    "\n",
    "            # Get additional properties - THE REAL NAME IS HERE!\n",
    "            props = getattr(node, \"properties\", {})\n",
    "\n",
    "            # Try to find the actual entity name in properties\n",
    "            entity_name = (\n",
    "                props.get(\"name\")\n",
    "                or props.get(\"entity_name\")\n",
    "                or props.get(\"text\")\n",
    "                or node_label\n",
    "            )\n",
    "\n",
    "            print(f\"{idx}. [{node_type}] {entity_name}\")\n",
    "            print(f\"   ID: {node_id}\")\n",
    "\n",
    "            # Show ALL properties to find where the real name is\n",
    "            if props:\n",
    "                print(f\"   Properties:\")\n",
    "                for key, value in props.items():\n",
    "                    value_str = str(value)[:100]\n",
    "                    print(f\"     {key}: {value_str}\")\n",
    "            print()\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"RELATIONSHIPS WITH ACTUAL NAMES (first 30)\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        for idx, edge in enumerate(list(graph.edges)[:30], start=1):\n",
    "            # Get source and target nodes\n",
    "            source_node = next((n for n in graph.nodes if n.id == edge.source), None)\n",
    "            target_node = next((n for n in graph.nodes if n.id == edge.target), None)\n",
    "\n",
    "            # Get actual names from properties\n",
    "            source_props = getattr(source_node, \"properties\", {}) if source_node else {}\n",
    "            target_props = getattr(target_node, \"properties\", {}) if target_node else {}\n",
    "\n",
    "            source_name = (\n",
    "                source_props.get(\"name\")\n",
    "                or source_props.get(\"entity_name\")\n",
    "                or source_props.get(\"text\")\n",
    "                or edge.source\n",
    "            )\n",
    "            target_name = (\n",
    "                target_props.get(\"name\")\n",
    "                or target_props.get(\"entity_name\")\n",
    "                or target_props.get(\"text\")\n",
    "                or edge.target\n",
    "            )\n",
    "\n",
    "            rel_type = edge.type\n",
    "\n",
    "            print(f\"{idx}. ({source_name}) --[{rel_type}]--> ({target_name})\")\n",
    "\n",
    "            # Show edge properties if any\n",
    "            if hasattr(edge, \"properties\") and edge.properties:\n",
    "                print(f\"   Edge properties:\")\n",
    "                for key, value in list(edge.properties.items())[:3]:\n",
    "                    value_str = str(value)[:80]\n",
    "                    print(f\"     {key}: {value_str}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Raw graph object not available.\")\n",
    "        print(\"The 'graph' variable from Section 5 is needed.\\n\")\n",
    "\n",
    "else:\n",
    "    print(\"No graph_store available. Run Section 5 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61c53e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 16. Debug Entity Properties\n",
    "\n",
    "if \"graph\" in globals() and hasattr(graph, \"nodes\"):\n",
    "    print(\"=== Debugging First 5 Nodes Properties ===\\n\")\n",
    "\n",
    "    for idx, node in enumerate(list(graph.nodes)[:5], start=1):\n",
    "        print(f\"{idx}. Node ID: {node.id}\")\n",
    "        print(f\"   Type: {node.type}\")\n",
    "        print(f\"   Label: {getattr(node, 'label', 'NO LABEL')}\")\n",
    "\n",
    "        props = getattr(node, \"properties\", {})\n",
    "        print(f\"   Properties keys: {list(props.keys())}\")\n",
    "        print(f\"   Properties: {props}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a81cb6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

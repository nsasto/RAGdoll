{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31c9aadd",
   "metadata": {},
   "source": [
    "# RAGdoll Key Modules Demo\n",
    "\n",
    "Hands-on walkthrough for ingestion, chunking, embeddings, storage layers, and orchestration. Every example relies on the sample assets under `tests/test_data`, so the notebook can run offline.\n",
    "\n",
    "> **Note:** \n",
    "- Set `USE_OPENAI = True` at the top to use real OpenAI embeddings, `False` for fake embeddings.\n",
    "- Cells 7 and 9 call OpenAI's GPT endpoints via `get_llm_caller`. Export `OPENAI_API_KEY` (or add it to `.env`) before running them.\n",
    "- **SSL Certificate Issues**: If you encounter `SSL: CERTIFICATE_VERIFY_FAILED` errors (common in corporate networks), see the SSL workaround cell below to disable verification for development purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf21e3cb",
   "metadata": {},
   "source": [
    "## What you'll see\n",
    "\n",
    "1. **Ingestion** – `DocumentLoaderService` from [`docs/ingestion.md`](../docs/ingestion.md).\n",
    "2. **Chunking** – `ragdoll.chunkers` helpers from [`docs/chunking.md`](../docs/chunking.md).\n",
    "3. **Embeddings** – provider factory from [`docs/embeddings.md`](../docs/embeddings.md) controlled by USE_OPENAI flag.\n",
    "4. **Vector stores** – `vector_store_from_config` customization from [`docs/vector_stores.md`](../docs/vector_stores.md).\n",
    "5. **Graph stores** – `get_graph_store` JSON persistence from [`docs/graph_stores.md`](../docs/graph_stores.md).\n",
    "6. **Entity extraction** – `EntityExtractionService` for building knowledge graphs.\n",
    "7. **LLMs** – `get_llm_caller`/`call_llm_sync` bridge described in [`docs/llm_integration.md`](../docs/llm_integration.md) hitting your real OpenAI model.\n",
    "8. **Pipeline** – `IngestionPipeline` snapshot from [`docs/architecture.md`](../docs/architecture.md).\n",
    "9. **Retrieval** – New modular retrieval system with `VectorRetriever`, `GraphRetriever`, and `HybridRetriever` from [`docs/retrieval.md`](../docs/retrieval.md).\n",
    "10. **Advanced Patterns** – Hybrid retrieval modes, graph traversal strategies, and async retrieval.\n",
    "\n",
    "Each cell builds on the previous ones so you can treat this as a scratchpad for experimenting with new loaders or configuration overrides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c06129f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from ragdoll import Ragdoll\n",
    "from ragdoll.app_config import bootstrap_app\n",
    "from ragdoll.ingestion import DocumentLoaderService\n",
    "from ragdoll.chunkers import get_text_splitter, split_documents\n",
    "from ragdoll.embeddings import get_embedding_model\n",
    "from ragdoll.vector_stores import vector_store_from_config\n",
    "from ragdoll.config.base_config import VectorStoreConfig\n",
    "from ragdoll.graph_stores import get_graph_store\n",
    "from ragdoll.entity_extraction import EntityExtractionService\n",
    "from ragdoll.entity_extraction.models import Graph, GraphNode, GraphEdge\n",
    "from ragdoll.entity_extraction.graph_persistence import GraphPersistenceService\n",
    "from ragdoll.retrieval import VectorRetriever, GraphRetriever, HybridRetriever\n",
    "from ragdoll.llms import get_llm_caller\n",
    "from ragdoll.llms.callers import call_llm_sync\n",
    "from ragdoll.pipeline import ingest_documents, IngestionPipeline, IngestionOptions\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c20c88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "DATA_DIR = Path('../tests/test_data').resolve()\n",
    "STATE_DIR = Path('demo_state').resolve()\n",
    "STATE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "SAMPLE_TXT = DATA_DIR / 'test_txt.txt'\n",
    "#SAMPLE_TXT = DATA_DIR / '*'\n",
    "\n",
    "# Set to True to use real OpenAI embeddings, False for fake embeddings\n",
    "USE_OPENAI = True\n",
    "\n",
    "app_config = bootstrap_app(\n",
    "    overrides={\n",
    "        'monitor': {'enabled': False, 'collect_metrics': False},\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "def normalize_documents(raw_docs):\n",
    "    docs = []\n",
    "    for entry in raw_docs:\n",
    "        if isinstance(entry, Document):\n",
    "            docs.append(entry)\n",
    "        elif isinstance(entry, dict):\n",
    "            docs.append(\n",
    "                Document(\n",
    "                    page_content=str(entry.get('page_content', '')),\n",
    "                    metadata=entry.get('metadata', {}) or {},\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            docs.append(Document(page_content=str(entry), metadata={}))\n",
    "    return docs\n",
    "\n",
    "\n",
    "def reset_subdir(name: str) -> Path:\n",
    "    path = STATE_DIR / name\n",
    "    if path.exists():\n",
    "        for attempt in range(5):\n",
    "            try:\n",
    "                shutil.rmtree(path)\n",
    "                break\n",
    "            except PermissionError:\n",
    "                time.sleep(0.5)\n",
    "        else:\n",
    "            timestamped = STATE_DIR / f\"{name}_{int(time.time())}\"\n",
    "            timestamped.mkdir(parents=True, exist_ok=True)\n",
    "            print(f\"Warning: {path} was locked, using {timestamped} instead.\")\n",
    "            return timestamped\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33445758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key len: 164 prefix: sk-proj-Ml-_ suffix: NUDc8A\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "k = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(\"Key len:\", len(k), \"prefix:\", k[:12], \"suffix:\", k[-6:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29502297",
   "metadata": {},
   "source": [
    "## 1. Load sample data\n",
    "`DocumentLoaderService` fans out across the loader registry defined in `ragdoll/config/default_config.yaml`. We point it at the lightweight TXT fixture so the demo does not need optional dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53759c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document(s) from test_txt.txt\n",
      "Metadata sample:\n",
      "{'source': 'C:\\\\dev\\\\RAGdoll\\\\tests\\\\test_data\\\\test_txt.txt'}\n",
      "Preview:\n",
      "Rag graph, or Retrieval-Augmented Generation with graph-based knowledge representation, enhances traditional RAG systems by structuring retrieved information into a dynamic knowledge graph rather than treating it as flat text chunks. At its core, it begins with a user query that triggers a retrieval step from a vector database or search index. Instead of directly feeding retrieved documents into a\n"
     ]
    }
   ],
   "source": [
    "loader = DocumentLoaderService(\n",
    "    app_config=app_config,\n",
    "    use_cache=False,\n",
    "    collect_metrics=False,\n",
    ")\n",
    "\n",
    "raw_documents = loader.ingest_documents([str(SAMPLE_TXT)])\n",
    "documents = normalize_documents(raw_documents)\n",
    "\n",
    "print(f\"Loaded {len(documents)} document(s) from {SAMPLE_TXT.name}\")\n",
    "print('Metadata sample:')\n",
    "pprint(documents[0].metadata)\n",
    "print('Preview:')\n",
    "print(documents[0].page_content[:400])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f416bedd",
   "metadata": {},
   "source": [
    "## 2. Chunk documents\n",
    "`ragdoll.chunkers.get_text_splitter` mirrors the strategies in [`docs/chunking.md`](../docs/chunking.md). Reusing the splitter instance keeps experiments consistent when you tweak chunk sizes/overlap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75b56c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 20 chunk(s)\n",
      "Chunk 1 metadata: {'source': 'C:\\\\dev\\\\RAGdoll\\\\tests\\\\test_data\\\\test_txt.txt'}\n",
      "Rag graph, or Retrieval-Augmented Generation with graph-based knowledge representation, enhances traditional RAG systems by structuring retrieved information into a dynamic knowled\n",
      "---\n",
      "Chunk 2 metadata: {'source': 'C:\\\\dev\\\\RAGdoll\\\\tests\\\\test_data\\\\test_txt.txt'}\n",
      "it as flat text chunks. At its core, it begins with a user query that triggers a retrieval step from a vector database or search index. Instead of directly feeding retrieved docume\n",
      "---\n",
      "Chunk 3 metadata: {'source': 'C:\\\\dev\\\\RAGdoll\\\\tests\\\\test_data\\\\test_txt.txt'}\n",
      "the system parses these documents to extract entities (such as people, organizations, or concepts) and relationships (like \"works at\" or \"caused by\") using named entity recognition\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "splitter = get_text_splitter(\n",
    "    splitter_type='recursive',\n",
    "    chunk_size=250,\n",
    "    chunk_overlap=40,\n",
    "    app_config=app_config,\n",
    ")\n",
    "chunks = split_documents(documents, text_splitter=splitter)\n",
    "\n",
    "print(f\"Created {len(chunks)} chunk(s)\")\n",
    "for idx, chunk in enumerate(chunks[:3], start=1):\n",
    "    preview = chunk.page_content[:180].replace('\\n', ' ')\n",
    "    print(f\"Chunk {idx} metadata: {chunk.metadata}\")\n",
    "    print(preview)\n",
    "    print('---')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaf55d4",
   "metadata": {},
   "source": [
    "## 3. Create embeddings\n",
    "`ragdoll.embeddings.get_embedding_model` instantiates providers dynamically. Passing `provider=\"fake\"` gives deterministic vectors without hitting OpenAI/HuggingFace, but the rest of the flow matches production usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7842ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a356222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_inputs = [chunk.page_content for chunk in chunks[:3]]\n",
    "if not embedding_inputs:\n",
    "    embedding_inputs = [documents[0].page_content]\n",
    "\n",
    "embeddings = (\n",
    "    get_embedding_model() if USE_OPENAI \n",
    "    else get_embedding_model(provider='fake', size=256)\n",
    ")\n",
    "vectors = embeddings.embed_documents(embedding_inputs)\n",
    "\n",
    "print(f\"Generated {len(vectors)} embedding vector(s) with dimension {len(vectors[0])}\")\n",
    "print('First vector slice:', vectors[0][:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924c608c",
   "metadata": {},
   "source": [
    "## 4. Build a vector store\n",
    "`vector_store_from_config` consumes a `VectorStoreConfig`, so you can swap FAISS/Chroma/etc. on demand. This cell provisions a Chroma collection under `demo_state` and runs a quick similarity query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b75b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_store_dir = reset_subdir('chroma_core_demo')\n",
    "vector_config = VectorStoreConfig(\n",
    "    enabled=True,\n",
    "    store_type='chroma',\n",
    "    params={\n",
    "        'collection_name': 'ragdoll_core_demo',\n",
    "        'persist_directory': str(core_store_dir),\n",
    "    },\n",
    ")\n",
    "\n",
    "demo_vector_store = vector_store_from_config(\n",
    "    vector_config,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "demo_vector_store.add_documents(chunks)\n",
    "question = 'What content lives in the txt sample?'\n",
    "results = demo_vector_store.similarity_search(question, k=2)\n",
    "for idx, doc in enumerate(results, start=1):\n",
    "    snippet = doc.page_content[:160].replace('\\n', ' ')\n",
    "    print(f\"Result {idx} (source={doc.metadata.get('source')}) -> {snippet}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed0f21c",
   "metadata": {},
   "source": [
    "## 5. Run entity extraction + persist a graph\n",
    "`EntityExtractionService` runs spaCy over the chunks and hands everything to `GraphPersistenceService`. We disable re-chunking, dump the output to JSON under `demo_state/`, and render a quick visualization so you can inspect the extracted entities before wiring them into a retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42838a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create directories\n",
    "graph_store_dir = reset_subdir('graph_demo')\n",
    "graph_image_path = graph_store_dir / 'graph_demo.png'\n",
    "graph_store_file = graph_store_dir / 'graph.pkl'\n",
    "\n",
    "# Initialize LLM\n",
    "if not os.getenv('OPENAI_API_KEY'):\n",
    "    print(\"OPENAI_API_KEY not set, using fallback LLM for demo.\")\n",
    "    class ExampleFallbackLLM:\n",
    "        async def call(self, prompt: str) -> str:\n",
    "            return \"Fallback response (no API key for entity extraction)\"\n",
    "    llm_caller = ExampleFallbackLLM()\n",
    "else:\n",
    "    llm_caller = get_llm_caller(app_config=app_config)\n",
    "    if llm_caller is None:\n",
    "        print(\"Unable to initialize LLM, using fallback.\")\n",
    "        llm_caller = ExampleFallbackLLM()\n",
    "    else:\n",
    "        print(\"LLM initialized for entity extraction.\")\n",
    "\n",
    "# Configure ingestion options - use the shared embeddings from earlier\n",
    "options = IngestionOptions(\n",
    "    batch_size=5,\n",
    "    extract_entities=True,\n",
    "    chunking_options={'chunk_size': 1000, 'chunk_overlap': 200},\n",
    "    vector_store_options={\n",
    "        \"store_type\": \"chroma\",\n",
    "        \"params\": {\n",
    "            \"collection_name\": \"graph_demo\",\n",
    "            \"persist_directory\": str(graph_store_dir / \"vector\"),\n",
    "        },\n",
    "    },\n",
    "    graph_store_options={\n",
    "        \"store_type\": \"networkx\",\n",
    "        \"output_file\": str(graph_store_file),\n",
    "    },\n",
    "    llm_caller=llm_caller,\n",
    "    entity_extraction_options={\n",
    "        \"entity_types\": [\"Person\", \"Organization\", \"Location\"],\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c87024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ingestion using a custom pipeline with shared embeddings\n",
    "from ragdoll.vector_stores import vector_store_from_config\n",
    "\n",
    "# Create vector store for graph demo with shared embeddings\n",
    "graph_vector_store = vector_store_from_config(\n",
    "    VectorStoreConfig(\n",
    "        enabled=True,\n",
    "        store_type=\"chroma\",\n",
    "        params={\n",
    "            \"collection_name\": \"graph_demo\",\n",
    "            \"persist_directory\": str(graph_store_dir / \"vector\"),\n",
    "        },\n",
    "    ),\n",
    "    embedding=embeddings,  # Use the shared embeddings instance\n",
    ")\n",
    "\n",
    "# Create custom pipeline with shared embeddings\n",
    "graph_pipeline = IngestionPipeline(\n",
    "    app_config=app_config,\n",
    "    content_extraction_service=DocumentLoaderService(\n",
    "        app_config=app_config,\n",
    "        use_cache=False,\n",
    "        collect_metrics=False,\n",
    "    ),\n",
    "    embedding_model=embeddings,  # Use shared embeddings\n",
    "    vector_store=graph_vector_store,\n",
    "    options=options,\n",
    ")\n",
    "\n",
    "sources = [str(SAMPLE_TXT)]\n",
    "stats = await graph_pipeline.ingest(sources)\n",
    "graph = getattr(graph_pipeline, \"last_graph\", None)\n",
    "graph_store = graph_pipeline.get_graph_store()\n",
    "\n",
    "# Print results\n",
    "print(f\"✅ Ingestion complete!\")\n",
    "print(f\"Documents processed: {stats.get('documents_processed')}\")\n",
    "print(f\"Chunks created: {stats.get('chunks_created')}\")\n",
    "print(f\"Entities extracted: {stats.get('entities_extracted')}\")\n",
    "print(f\"Relationships extracted: {stats.get('relationships_extracted')}\")\n",
    "print(f\"Vector entries added: {stats.get('vector_entries_added')}\")\n",
    "print(f\"Graph entries added: {stats.get('graph_entries_added')}\")\n",
    "\n",
    "if stats.get(\"errors\"):\n",
    "    print(f\"⚠️ Warnings/Errors:\")\n",
    "    for error in stats[\"errors\"]:\n",
    "        print(f\"  - {error}\")\n",
    "\n",
    "# Visualize the graph\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import networkx as nx\n",
    "\n",
    "    if graph and hasattr(graph, 'nodes') and graph.nodes:\n",
    "        viz_graph = nx.Graph()\n",
    "        for node in graph.nodes:\n",
    "            viz_graph.add_node(node.id, label=node.name, type=node.type)\n",
    "        for edge in graph.edges:\n",
    "            viz_graph.add_edge(edge.source, edge.target, relationship=edge.type)\n",
    "\n",
    "        if viz_graph.number_of_nodes() > 0:\n",
    "            plt.figure(figsize=(6, 5))\n",
    "            positions = nx.spring_layout(viz_graph, seed=42)\n",
    "            labels = {node_id: viz_graph.nodes[node_id].get('label', node_id) for node_id in viz_graph.nodes}\n",
    "            nx.draw_networkx_nodes(viz_graph, positions, node_color='#c7d2fe', linewidths=1)\n",
    "            nx.draw_networkx_edges(viz_graph, positions, edge_color='#4c51bf')\n",
    "            nx.draw_networkx_labels(viz_graph, positions, labels=labels, font_size=9)\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(graph_image_path, dpi=160)\n",
    "            plt.show()\n",
    "            print(f'Saved visualization to {graph_image_path}')\n",
    "        else:\n",
    "            print('Graph has no nodes to visualize.')\n",
    "    else:\n",
    "        print('No graph available to visualize.')\n",
    "except Exception as exc:\n",
    "    print(f'Skipped visualization: {exc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2832524",
   "metadata": {},
   "source": [
    "## 6. Query vector + graph context\n",
    "`GraphPersistenceService` reloads the extracted graph into a LangChain retriever so we can compare vector hits against graph nodes for the same question. This mirrors the hybrid pattern we feed into the orchestration demo later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae088e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_persistence = GraphPersistenceService(\n",
    "    output_format='custom_graph_object',\n",
    "    retriever_backend='simple',\n",
    "    retriever_config={'top_k': 3, 'include_edges': True},\n",
    ")\n",
    "\n",
    "_ = graph_persistence.save(graph)\n",
    "graph_retriever = graph_persistence.create_retriever()\n",
    "\n",
    "hybrid_question = 'Which people or organizations are mentioned in the txt sample?'\n",
    "vector_hits = demo_vector_store.similarity_search(hybrid_question, k=2)\n",
    "graph_hits = graph_retriever.get_relevant_documents(hybrid_question)\n",
    "\n",
    "print(f\"Vector store returned {len(vector_hits)} document(s)\")\n",
    "for idx, doc in enumerate(vector_hits, start=1):\n",
    "    snippet = doc.page_content[:200].replace('\\n', ' ')\n",
    "    print(f\"- Vector hit {idx} (source={doc.metadata.get('source')}): {snippet}\")\n",
    "\n",
    "print(f\"Graph retriever returned {len(graph_hits)} node(s)\")\n",
    "for doc in graph_hits:\n",
    "    print(\n",
    "        f\"- Node {doc.metadata.get('node_id')} ({doc.metadata.get('node_type')}): {doc.page_content}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85dd0b0",
   "metadata": {},
   "source": [
    "## 7. Wire up an LLM caller\n",
    "`get_llm_caller` now instantiates the OpenAI chat model defined in `ragdoll/config/default_config.yaml` (defaults to `gpt-4o-mini`). Make sure `OPENAI_API_KEY` is available before running the next cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90cde30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv('OPENAI_API_KEY'):\n",
    "    raise EnvironmentError('Set OPENAI_API_KEY before calling the real OpenAI demo cell.')\n",
    "\n",
    "openai_llm_caller = get_llm_caller(app_config=app_config)\n",
    "\n",
    "sample_text = chunks[0].page_content if chunks else documents[0].page_content\n",
    "prompt = (\n",
    "    'Summarize the following text sample in one sentence. Mention what the document is about and highlight any key people, organizations, or actions.'\n",
    "    f\"{sample_text[:2048]}\"\n",
    ")\n",
    "print('Sample excerpt:', sample_text[:360].replace('\\n', ' '))\n",
    "llm_reply = call_llm_sync(openai_llm_caller, prompt)\n",
    "print('OpenAI response:', llm_reply)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6d3b54",
   "metadata": {},
   "source": [
    "## 8. Run the ingestion pipeline (async)\n",
    "`IngestionPipeline` stitches together the loader, chunker, embeddings, vector store, and optional graph/entity stages. We disable entity extraction to keep the run lightweight and await the coroutine directly inside the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a974698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_store_dir = reset_subdir('chroma_pipeline_demo')\n",
    "pipeline_vector_config = VectorStoreConfig(\n",
    "    enabled=True,\n",
    "    store_type='chroma',\n",
    "    params={\n",
    "        'collection_name': 'ragdoll_pipeline_demo',\n",
    "        'persist_directory': str(pipeline_store_dir),\n",
    "    },\n",
    ")\n",
    "pipeline_vector_store = vector_store_from_config(\n",
    "    pipeline_vector_config,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    app_config=app_config,\n",
    "    content_extraction_service=DocumentLoaderService(\n",
    "        app_config=app_config,\n",
    "        use_cache=False,\n",
    "        collect_metrics=False,\n",
    "    ),\n",
    "    embedding_model=embeddings,\n",
    "    vector_store=pipeline_vector_store,\n",
    "    options=IngestionOptions(\n",
    "        batch_size=2,\n",
    "        extract_entities=False,\n",
    "        skip_graph_store=True,\n",
    "        chunking_options={'chunk_size': 300, 'chunk_overlap': 60, 'splitter_type': 'recursive'},\n",
    "    ),\n",
    ")\n",
    "\n",
    "pipeline_stats = await pipeline.ingest([str(SAMPLE_TXT)])\n",
    "pipeline_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01000105",
   "metadata": {},
   "source": [
    "## 9. Use the new retrieval module\n",
    "The refactored `ragdoll.retrieval` module provides clean separation between vector, graph, and hybrid retrieval strategies. You can use `VectorRetriever`, `GraphRetriever`, or `HybridRetriever` directly with LangChain compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c2b14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragdoll.retrieval import VectorRetriever, GraphRetriever, HybridRetriever\n",
    "\n",
    "# Setup vector retriever with the demo vector store\n",
    "vector_retriever = VectorRetriever(\n",
    "    vector_store=demo_vector_store,\n",
    "    top_k=3,\n",
    "    search_type=\"similarity\"\n",
    ")\n",
    "\n",
    "# Query about graph RAG concepts\n",
    "query = \"How does graph RAG handle entity extraction and relationships?\"\n",
    "\n",
    "print(\"=== Vector Retrieval ===\")\n",
    "vector_results = vector_retriever.get_relevant_documents(query)\n",
    "for idx, doc in enumerate(vector_results, start=1):\n",
    "    snippet = doc.page_content[:200].replace('\\n', ' ')\n",
    "    print(f\"Result {idx}: {snippet}...\")\n",
    "    print(f\"  Score: {doc.metadata.get('relevance_score', 'N/A')}\\n\")\n",
    "\n",
    "# If we have a graph store from earlier, demonstrate graph retrieval\n",
    "if 'graph_store' in globals() and graph_store is not None:\n",
    "    print(\"\\n=== Graph Retrieval ===\")\n",
    "    graph_retriever = GraphRetriever(\n",
    "        graph_store=graph_store,\n",
    "        top_k=5,\n",
    "        max_hops=2,\n",
    "        traversal_strategy=\"bfs\"\n",
    "    )\n",
    "    \n",
    "    graph_results = graph_retriever.get_relevant_documents(query)\n",
    "    for idx, doc in enumerate(graph_results, start=1):\n",
    "        node_id = doc.metadata.get('node_id', 'unknown')\n",
    "        node_type = doc.metadata.get('node_type', 'unknown')\n",
    "        hop_distance = doc.metadata.get('hop_distance', 0)\n",
    "        print(f\"Node {idx}: {node_type} '{node_id}' (hop distance: {hop_distance})\")\n",
    "        print(f\"  Content: {doc.page_content[:150]}...\\n\")\n",
    "    \n",
    "    # Demonstrate hybrid retrieval combining both\n",
    "    print(\"\\n=== Hybrid Retrieval (Vector + Graph) ===\")\n",
    "    hybrid_retriever = HybridRetriever(\n",
    "        vector_store=demo_vector_store,\n",
    "        graph_store=graph_store,\n",
    "        vector_top_k=3,\n",
    "        graph_top_k=3,\n",
    "        graph_max_hops=1,\n",
    "        mode=\"concat\"  # Can also be \"rerank\", \"weighted\", or \"expand\"\n",
    "    )\n",
    "    \n",
    "    hybrid_results = hybrid_retriever.get_relevant_documents(query)\n",
    "    print(f\"Retrieved {len(hybrid_results)} total documents from hybrid search\")\n",
    "    for idx, doc in enumerate(hybrid_results, start=1):\n",
    "        source = doc.metadata.get('source', 'unknown')\n",
    "        snippet = doc.page_content[:120].replace('\\n', ' ')\n",
    "        print(f\"{idx}. [{source}] {snippet}...\")\n",
    "else:\n",
    "    print(\"\\n(Graph store not available - run Section 5 to create it)\")\n",
    "\n",
    "# Demonstrate different retrieval strategies\n",
    "print(\"\\n=== MMR Search (Maximal Marginal Relevance) ===\")\n",
    "mmr_retriever = VectorRetriever(\n",
    "    vector_store=demo_vector_store,\n",
    "    top_k=3,\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"fetch_k\": 10, \"lambda_mult\": 0.5}\n",
    ")\n",
    "mmr_results = mmr_retriever.get_relevant_documents(\"What is multi-hop reasoning in RAG systems?\")\n",
    "for idx, doc in enumerate(mmr_results, start=1):\n",
    "    print(f\"{idx}. {doc.page_content[:150].replace(chr(10), ' ')}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ef34a9",
   "metadata": {},
   "source": [
    "## 10. Advanced Retrieval Patterns\n",
    "\n",
    "The new retrieval module supports various advanced patterns:\n",
    "\n",
    "### Hybrid Retrieval Modes\n",
    "- **concat**: Simple concatenation of vector and graph results\n",
    "- **rerank**: Rerank combined results by relevance score\n",
    "- **weighted**: Weighted combination (adjust vector_weight/graph_weight)\n",
    "- **expand**: Use vector results as seeds for graph expansion\n",
    "\n",
    "### Graph Traversal Strategies\n",
    "- **BFS (Breadth-First)**: Explores neighbors level by level (default)\n",
    "- **DFS (Depth-First)**: Follows paths deeply before backtracking\n",
    "\n",
    "### Search Types\n",
    "- **similarity**: Standard vector similarity search\n",
    "- **mmr**: Maximal Marginal Relevance for diverse results\n",
    "- **similarity_score_threshold**: Filter by minimum similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe19226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Weighted hybrid retrieval for balanced results\n",
    "if 'graph_store' in globals() and graph_store is not None:\n",
    "    weighted_retriever = HybridRetriever(\n",
    "        vector_store=demo_vector_store,\n",
    "        graph_store=graph_store,\n",
    "        vector_top_k=5,\n",
    "        graph_top_k=5,\n",
    "        mode=\"weighted\",\n",
    "        vector_weight=0.6,  # Favor vector results slightly\n",
    "        graph_weight=0.4\n",
    "    )\n",
    "    \n",
    "    query = \"Explain how graph RAG systems handle iterative refinement\"\n",
    "    weighted_results = weighted_retriever.get_relevant_documents(query)\n",
    "    \n",
    "    print(f\"=== Weighted Hybrid Results (vector=0.6, graph=0.4) ===\")\n",
    "    print(f\"Retrieved {len(weighted_results)} documents\\n\")\n",
    "    \n",
    "    for idx, doc in enumerate(weighted_results[:5], start=1):\n",
    "        score = doc.metadata.get('relevance_score', 0)\n",
    "        source_type = 'graph' if 'node_id' in doc.metadata else 'vector'\n",
    "        snippet = doc.page_content[:150].replace('\\n', ' ')\n",
    "        print(f\"{idx}. [{source_type}] Score: {score:.3f}\")\n",
    "        print(f\"   {snippet}...\\n\")\n",
    "    \n",
    "    # Example: Graph expansion mode - use vector hits to seed graph traversal\n",
    "    expand_retriever = HybridRetriever(\n",
    "        vector_store=demo_vector_store,\n",
    "        graph_store=graph_store,\n",
    "        vector_top_k=2,\n",
    "        graph_max_hops=2,\n",
    "        mode=\"expand\"\n",
    "    )\n",
    "    \n",
    "    expand_query = \"What are graph neural networks used for in RAG?\"\n",
    "    expand_results = expand_retriever.get_relevant_documents(expand_query)\n",
    "    \n",
    "    print(f\"\\n=== Expand Mode (vector seeds + graph traversal) ===\")\n",
    "    print(f\"Retrieved {len(expand_results)} documents from graph expansion\\n\")\n",
    "    \n",
    "    for idx, doc in enumerate(expand_results[:3], start=1):\n",
    "        if 'node_id' in doc.metadata:\n",
    "            node_id = doc.metadata['node_id']\n",
    "            hop = doc.metadata.get('hop_distance', 0)\n",
    "            print(f\"{idx}. Node: {node_id} (hop: {hop})\")\n",
    "            print(f\"   {doc.page_content[:120]}...\\n\")\n",
    "else:\n",
    "    print(\"Graph store not available. Run Section 5 to enable graph retrieval examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f579542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Async retrieval for concurrent queries\n",
    "import asyncio\n",
    "\n",
    "async def demo_async_retrieval():\n",
    "    \"\"\"Demonstrate async retrieval capabilities.\"\"\"\n",
    "    queries = [\n",
    "        \"What is entity extraction?\",\n",
    "        \"How do graph algorithms help RAG?\",\n",
    "        \"Explain knowledge graph construction\"\n",
    "    ]\n",
    "    \n",
    "    # Create retrievers\n",
    "    vector_ret = VectorRetriever(vector_store=demo_vector_store, top_k=2)\n",
    "    \n",
    "    print(\"=== Async Concurrent Retrieval ===\")\n",
    "    # Retrieve all queries concurrently\n",
    "    results = await asyncio.gather(*[\n",
    "        vector_ret.aget_relevant_documents(q) for q in queries\n",
    "    ])\n",
    "    \n",
    "    for query, docs in zip(queries, results):\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        print(f\"  Found {len(docs)} documents\")\n",
    "        if docs:\n",
    "            print(f\"  Top result: {docs[0].page_content[:100].replace(chr(10), ' ')}...\")\n",
    "\n",
    "# Run async demo\n",
    "await demo_async_retrieval()\n",
    "\n",
    "# Get retriever statistics\n",
    "print(\"\\n=== Retriever Statistics ===\")\n",
    "vector_stats = vector_retriever.get_stats()\n",
    "print(f\"Vector Retriever: {vector_stats}\")\n",
    "\n",
    "if 'graph_retriever' in globals():\n",
    "    graph_stats = graph_retriever.get_stats()\n",
    "    print(f\"Graph Retriever: {graph_stats}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f504a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cc6c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31c9aadd",
   "metadata": {},
   "source": [
    "# RAGdoll Key Modules Demo\n",
    "\n",
    "Hands-on walkthrough for ingestion, chunking, embeddings, storage layers, and orchestration. Every example relies on the sample assets under `tests/test_data`, so the notebook can run offline.\n",
    "\n",
    "> **Note:** Cells 7 and 9 call OpenAI's GPT endpoints via `get_llm_caller`. Export `OPENAI_API_KEY` (or add it to `.env`) before running them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf21e3cb",
   "metadata": {},
   "source": [
    "## What you'll see\n",
    "\n",
    "1. **Ingestion** ? `DocumentLoaderService` from [`docs/ingestion.md`](../docs/ingestion.md).\n",
    "2. **Chunking** ? `ragdoll.chunkers` helpers from [`docs/chunking.md`](../docs/chunking.md).\n",
    "3. **Embeddings** ? provider factory from [`docs/embeddings.md`](../docs/embeddings.md) using the fake backend for speed.\n",
    "4. **Vector stores** ? `vector_store_from_config` customization from [`docs/vector_stores.md`](../docs/vector_stores.md).\n",
    "5. **Graph stores** ? `get_graph_store` JSON persistence from [`docs/graph_stores.md`](../docs/graph_stores.md).\n",
    "6. **Graph retrievers** ? `GraphPersistenceService` simple/Neo4j backends from [`docs/graph_stores.md`](../docs/graph_stores.md).\n",
    "7. **LLMs** ? `get_llm_caller`/`call_llm_sync` bridge described in [`docs/llm_integration.md`](../docs/llm_integration.md) hitting your real OpenAI model.\n",
    "8. **Pipeline** ? `IngestionPipeline` snapshot from [`docs/architecture.md`](../docs/architecture.md).\n",
    "9. **Ragdoll** ? orchestrator entry point tying everything together.\n",
    "\n",
    "Each cell builds on the previous ones so you can treat this as a scratchpad for experimenting with new loaders or configuration overrides.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c20c88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from ragdoll import Ragdoll\n",
    "from ragdoll.app_config import bootstrap_app\n",
    "from ragdoll.ingestion import DocumentLoaderService\n",
    "from ragdoll.chunkers import get_text_splitter, split_documents\n",
    "from ragdoll.embeddings import get_embedding_model\n",
    "from ragdoll.vector_stores import vector_store_from_config\n",
    "from ragdoll.config.base_config import VectorStoreConfig\n",
    "from ragdoll.graph_stores import get_graph_store\n",
    "from ragdoll.entity_extraction.models import Graph, GraphNode, GraphEdge\n",
    "from ragdoll.entity_extraction.graph_persistence import GraphPersistenceService\n",
    "from ragdoll.llms import get_llm_caller\n",
    "from ragdoll.llms.callers import call_llm_sync\n",
    "from ragdoll.pipeline import IngestionPipeline, IngestionOptions\n",
    "\n",
    "DATA_DIR = Path('../tests/test_data').resolve()\n",
    "STATE_DIR = Path('demo_state').resolve()\n",
    "STATE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "#SAMPLE_TXT = DATA_DIR / 'test_txt.txt'\n",
    "SAMPLE_TXT = DATA_DIR / '*'\n",
    "\n",
    "app_config = bootstrap_app(\n",
    "    overrides={\n",
    "        'monitor': {'enabled': False, 'collect_metrics': False},\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "def normalize_documents(raw_docs):\n",
    "    docs = []\n",
    "    for entry in raw_docs:\n",
    "        if isinstance(entry, Document):\n",
    "            docs.append(entry)\n",
    "        elif isinstance(entry, dict):\n",
    "            docs.append(\n",
    "                Document(\n",
    "                    page_content=str(entry.get('page_content', '')),\n",
    "                    metadata=entry.get('metadata', {}) or {},\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            docs.append(Document(page_content=str(entry), metadata={}))\n",
    "    return docs\n",
    "\n",
    "\n",
    "def reset_subdir(name: str) -> Path:\n",
    "    path = STATE_DIR / name\n",
    "    if path.exists():\n",
    "        for attempt in range(5):\n",
    "            try:\n",
    "                shutil.rmtree(path)\n",
    "                break\n",
    "            except PermissionError:\n",
    "                time.sleep(0.5)\n",
    "        else:\n",
    "            timestamped = STATE_DIR / f\"{name}_{int(time.time())}\"\n",
    "            timestamped.mkdir(parents=True, exist_ok=True)\n",
    "            print(f\"Warning: {path} was locked, using {timestamped} instead.\")\n",
    "            return timestamped\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29502297",
   "metadata": {},
   "source": [
    "## 1. Load sample data\n",
    "`DocumentLoaderService` fans out across the loader registry defined in `ragdoll/config/default_config.yaml`. We point it at the lightweight TXT fixture so the demo does not need optional dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53759c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 777 document(s) from *\n",
      "Metadata sample:\n",
      "{'author': 'Nathan Sasto',\n",
      " 'content_type': 'document_full',\n",
      " 'conversion_success': True,\n",
      " 'created': '2025-04-08 13:25:00+00:00',\n",
      " 'file_name': 'test_docx.docx',\n",
      " 'file_size': 328097,\n",
      " 'last_modified_by': 'Nathan Sasto',\n",
      " 'modified': '2025-05-28 19:59:00+00:00',\n",
      " 'revision': '3',\n",
      " 'source': 'C:\\\\dev\\\\RAGdoll\\\\tests\\\\test_data\\\\test_docx.docx',\n",
      " 'success': True}\n",
      "Preview:\n",
      "Lorem ipsum\n",
      "\n",
      "# Large Language Models\n",
      "\n",
      "![](data:image/x-emf;base64...)\n",
      "\n",
      "Large language models (LLMs) have transformed natural language processing by leveraging massive datasets and computational power to achieve remarkable performance in tasks like text generation, summarization, and sentiment analysis. These models rely on deep learning architectures, particularly transformers, which allow them to\n"
     ]
    }
   ],
   "source": [
    "loader = DocumentLoaderService(\n",
    "    app_config=app_config,\n",
    "    use_cache=False,\n",
    "    collect_metrics=False,\n",
    ")\n",
    "\n",
    "raw_documents = loader.ingest_documents([str(SAMPLE_TXT)])\n",
    "documents = normalize_documents(raw_documents)\n",
    "\n",
    "print(f\"Loaded {len(documents)} document(s) from {SAMPLE_TXT.name}\")\n",
    "print('Metadata sample:')\n",
    "pprint(documents[0].metadata)\n",
    "print('Preview:')\n",
    "print(documents[0].page_content[:400])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f416bedd",
   "metadata": {},
   "source": [
    "## 2. Chunk documents\n",
    "`ragdoll.chunkers.get_text_splitter` mirrors the strategies in [`docs/chunking.md`](../docs/chunking.md). Reusing the splitter instance keeps experiments consistent when you tweak chunk sizes/overlap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b56c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 8985 chunk(s)\n",
      "Chunk 1 metadata: {'source': 'C:\\\\dev\\\\RAGdoll\\\\tests\\\\test_data\\\\test_docx.docx', 'success': True, 'conversion_success': True, 'file_name': 'test_docx.docx', 'file_size': 328097, 'author': 'Nathan Sasto', 'created': '2025-04-08 13:25:00+00:00', 'modified': '2025-05-28 19:59:00+00:00', 'last_modified_by': 'Nathan Sasto', 'revision': '3', 'content_type': 'document_full'}\n",
      " L o r e m   i p s u m \n",
      " \n",
      " #   L a r g e   L a n g u a g e   M o d e l s \n",
      " \n",
      " ! [ ] ( d a t a : i m a g e / x - e m f ; b a s e 6 4 . . . ) \n",
      "---\n",
      "Chunk 2 metadata: {'source': 'C:\\\\dev\\\\RAGdoll\\\\tests\\\\test_data\\\\test_docx.docx', 'success': True, 'conversion_success': True, 'file_name': 'test_docx.docx', 'file_size': 328097, 'author': 'Nathan Sasto', 'created': '2025-04-08 13:25:00+00:00', 'modified': '2025-05-28 19:59:00+00:00', 'last_modified_by': 'Nathan Sasto', 'revision': '3', 'content_type': 'document_full'}\n",
      " L a r g e   l a n g u a g e   m o d e l s   ( L L M s )   h a v e   t r a n s f o r m e d   n a t u r a l   l a n g u a g e   p r o c e s s i n g   b y   l e v e r a g i n g   m a s s i v e   d a t a s e t s   a n d   c o m p u t a t i o n a l   p o w e r   t o   a c h i e v e   r e m a r k a b l e   p e r f o r m a n c e   i n   t a s k s   l i k e   t e x \n",
      "---\n",
      "Chunk 3 metadata: {'source': 'C:\\\\dev\\\\RAGdoll\\\\tests\\\\test_data\\\\test_docx.docx', 'success': True, 'conversion_success': True, 'file_name': 'test_docx.docx', 'file_size': 328097, 'author': 'Nathan Sasto', 'created': '2025-04-08 13:25:00+00:00', 'modified': '2025-05-28 19:59:00+00:00', 'last_modified_by': 'Nathan Sasto', 'revision': '3', 'content_type': 'document_full'}\n",
      " a n d   s e n t i m e n t   a n a l y s i s .   T h e s e   m o d e l s   r e l y   o n   d e e p   l e a r n i n g   a r c h i t e c t u r e s ,   p a r t i c u l a r l y   t r a n s f o r m e r s ,   w h i c h   a l l o w   t h e m   t o   p r o c e s s   a n d   g e n e r a t e   t e x t   w i t h   c o n t e x t u a l   u n d e r s t a n d i n g .   H o \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "splitter = get_text_splitter(\n",
    "    splitter_type='recursive',\n",
    "    chunk_size=250,\n",
    "    chunk_overlap=40,\n",
    "    app_config=app_config,\n",
    ")\n",
    "chunks = split_documents(documents, text_splitter=splitter)\n",
    "\n",
    "print(f\"Created {len(chunks)} chunk(s)\")\n",
    "for idx, chunk in enumerate(chunks[:3], start=1):\n",
    "    preview = chunk.page_content[:180].replace('', ' ')\n",
    "    print(f\"Chunk {idx} metadata: {chunk.metadata}\")\n",
    "    print(preview)\n",
    "    print('---')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaf55d4",
   "metadata": {},
   "source": [
    "## 3. Create embeddings\n",
    "`ragdoll.embeddings.get_embedding_model` instantiates providers dynamically. Passing `provider=\"fake\"` gives deterministic vectors without hitting OpenAI/HuggingFace, but the rest of the flow matches production usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a356222c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3 embedding vector(s) with dimension 256\n",
      "First vector slice: [np.float64(-1.1767431451548127), np.float64(2.5051321687172563), np.float64(0.47355094119428875), np.float64(-0.5549040791814097), np.float64(1.1380425553720102), np.float64(-0.8713958213782483), np.float64(-2.5380340171633797), np.float64(0.7849101963595468)]\n"
     ]
    }
   ],
   "source": [
    "embedding_inputs = [chunk.page_content for chunk in chunks[:3]]\n",
    "if not embedding_inputs:\n",
    "    embedding_inputs = [documents[0].page_content]\n",
    "\n",
    "fake_embeddings = get_embedding_model(provider='fake', size=256)\n",
    "vectors = fake_embeddings.embed_documents(embedding_inputs)\n",
    "\n",
    "print(f\"Generated {len(vectors)} embedding vector(s) with dimension {len(vectors[0])}\")\n",
    "print('First vector slice:', vectors[0][:8])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924c608c",
   "metadata": {},
   "source": [
    "## 4. Build a vector store\n",
    "`vector_store_from_config` consumes a `VectorStoreConfig`, so you can swap FAISS/Chroma/etc. on demand. This cell provisions a Chroma collection under `demo_state` and runs a quick similarity query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b75b2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\dev\\\\RAGdoll\\\\examples\\\\demo_state\\\\chroma_core_demo\\\\chroma.sqlite3'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m core_store_dir = \u001b[43mreset_subdir\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mchroma_core_demo\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m vector_config = VectorStoreConfig(\n\u001b[32m      3\u001b[39m     enabled=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      4\u001b[39m     store_type=\u001b[33m'\u001b[39m\u001b[33mchroma\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     },\n\u001b[32m      9\u001b[39m )\n\u001b[32m     11\u001b[39m demo_vector_store = vector_store_from_config(\n\u001b[32m     12\u001b[39m     vector_config,\n\u001b[32m     13\u001b[39m     embedding=fake_embeddings,\n\u001b[32m     14\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mreset_subdir\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m     52\u001b[39m path = STATE_DIR / name\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m path.exists():\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     \u001b[43mshutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrmtree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m path.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\shutil.py:781\u001b[39m, in \u001b[36mrmtree\u001b[39m\u001b[34m(path, ignore_errors, onerror, onexc, dir_fd)\u001b[39m\n\u001b[32m    779\u001b[39m     \u001b[38;5;66;03m# can't continue even if onexc hook returns\u001b[39;00m\n\u001b[32m    780\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m781\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_rmtree_unsafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monexc\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\shutil.py:635\u001b[39m, in \u001b[36m_rmtree_unsafe\u001b[39m\u001b[34m(path, onexc)\u001b[39m\n\u001b[32m    633\u001b[39m             os.unlink(fullname)\n\u001b[32m    634\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m635\u001b[39m             \u001b[43monexc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43munlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    637\u001b[39m     os.rmdir(path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\shutil.py:633\u001b[39m, in \u001b[36m_rmtree_unsafe\u001b[39m\u001b[34m(path, onexc)\u001b[39m\n\u001b[32m    631\u001b[39m fullname = os.path.join(dirpath, name)\n\u001b[32m    632\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m633\u001b[39m     \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43munlink\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    635\u001b[39m     onexc(os.unlink, fullname, err)\n",
      "\u001b[31mPermissionError\u001b[39m: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\dev\\\\RAGdoll\\\\examples\\\\demo_state\\\\chroma_core_demo\\\\chroma.sqlite3'"
     ]
    }
   ],
   "source": [
    "core_store_dir = reset_subdir('chroma_core_demo')\n",
    "vector_config = VectorStoreConfig(\n",
    "    enabled=True,\n",
    "    store_type='chroma',\n",
    "    params={\n",
    "        'collection_name': 'ragdoll_core_demo',\n",
    "        'persist_directory': str(core_store_dir),\n",
    "    },\n",
    ")\n",
    "\n",
    "demo_vector_store = vector_store_from_config(\n",
    "    vector_config,\n",
    "    embedding=fake_embeddings,\n",
    ")\n",
    "\n",
    "demo_vector_store.add_documents(chunks)\n",
    "question = 'What content lives in the txt sample?'\n",
    "results = demo_vector_store.similarity_search(question, k=2)\n",
    "for idx, doc in enumerate(results, start=1):\n",
    "    snippet = doc.page_content[:160].replace('', ' ')\n",
    "    print(f\"Result {idx} (source={doc.metadata.get('source')}) -> {snippet}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed0f21c",
   "metadata": {},
   "source": [
    "## 5. Persist a tiny graph\n",
    "`get_graph_store` supports JSON, NetworkX, and Neo4j backends. To keep things simple we build a two-node graph (document ? chunk) and write it to JSON for inspection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42838a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persisted graph via GraphStoreWrapper -> C:\\dev\\RAGdoll\\examples\\demo_state\\graph_demo.json\n",
      "{\n",
      "  \"nodes\": [\n",
      "    {\n",
      "      \"id\": \"443a09d5-3977-4480-a055-5a6d712154dc\",\n",
      "      \"type\": \"Document\",\n",
      "      \"name\": \"Sample Text File\",\n",
      "      \"metadata\": {\n",
      "        \"path\": \"C:\\\\dev\\\\RAGdoll\\\\tests\\\\test_data\\\\*\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"9c274e4a-cf3f-443e-8d80-2bbf353ed186\",\n",
      "      \"type\": \"Chunk\",\n",
      "      \"name\": \"Chunk 0\",\n",
      "      \"metadata\": {\n",
      "        \"chunk_index\": 0,\n",
      "        \"preview\": \"Lore\n"
     ]
    }
   ],
   "source": [
    "doc_node = GraphNode(\n",
    "    name='Sample Text File',\n",
    "    type='Document',\n",
    "    metadata={'path': str(SAMPLE_TXT)},\n",
    ")\n",
    "chunk_node = GraphNode(\n",
    "    name='Chunk 0',\n",
    "    type='Chunk',\n",
    "    metadata={'chunk_index': 0, 'preview': chunks[0].page_content[:80]},\n",
    ")\n",
    "graph = Graph(\n",
    "    nodes=[doc_node, chunk_node],\n",
    "    edges=[\n",
    "        GraphEdge(\n",
    "            source=doc_node.id,\n",
    "            target=chunk_node.id,\n",
    "            type='CONTAINS',\n",
    "            metadata={'similarity': 1.0},\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "graph_path = STATE_DIR / 'graph_demo.json'\n",
    "graph_store = get_graph_store(\n",
    "    store_type='json',\n",
    "    graph=graph,\n",
    "    graph_config={'output_file': str(graph_path)},\n",
    ")\n",
    "print(f\"Persisted graph via {type(graph_store).__name__} -> {graph_path}\")\n",
    "if graph_path.exists():\n",
    "    print(graph_path.read_text()[:400])\n",
    "else:\n",
    "    print('Graph file was not created yet.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2832524",
   "metadata": {},
   "source": [
    "## 6. Query the graph with a LangChain retriever\n",
    "`GraphPersistenceService` can materialize a LangChain-compatible retriever from the last saved graph. In a full ingestion run you would enable `entity_extraction.graph_retriever.enabled`, but here we reuse the toy graph above to show how the **simple** backend answers questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae088e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_persistence = GraphPersistenceService(\n",
    "    output_format='custom_graph_object',\n",
    "    retriever_backend='simple',\n",
    "    retriever_config={'top_k': 3, 'include_edges': True},\n",
    ")\n",
    "\n",
    "_ = graph_persistence.save(graph)\n",
    "graph_retriever = graph_persistence.create_retriever()\n",
    "\n",
    "retriever_question = 'Which graph nodes reference the document and its chunk?'\n",
    "retriever_hits = graph_retriever.get_relevant_documents(retriever_question)\n",
    "\n",
    "print(f\"Graph retriever returned {len(retriever_hits)} document(s)\")\n",
    "for doc in retriever_hits:\n",
    "    print(f\"- {doc.page_content} => {doc.metadata}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85dd0b0",
   "metadata": {},
   "source": [
    "## 7. Wire up an LLM caller\n",
    "`get_llm_caller` now instantiates the OpenAI chat model defined in `ragdoll/config/default_config.yaml` (defaults to `gpt-4o-mini`). Make sure `OPENAI_API_KEY` is available before running the next cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90cde30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI response: Sure! Please provide the text sample you'd like me to summarize.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv('OPENAI_API_KEY'):\n",
    "    raise EnvironmentError('Set OPENAI_API_KEY before calling the real OpenAI demo cell.')\n",
    "\n",
    "openai_llm_caller = get_llm_caller(app_config=app_config)\n",
    "prompt = 'Pretend you read the txt sample and summarize it in one sentence.'\n",
    "llm_reply = call_llm_sync(openai_llm_caller, prompt)\n",
    "print('OpenAI response:', llm_reply)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6d3b54",
   "metadata": {},
   "source": [
    "## 8. Run the ingestion pipeline (async)\n",
    "`IngestionPipeline` stitches together the loader, chunker, embeddings, vector store, and optional graph/entity stages. We disable entity extraction to keep the run lightweight and await the coroutine directly inside the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a974698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_store_dir = reset_subdir('chroma_pipeline_demo')\n",
    "pipeline_vector_config = VectorStoreConfig(\n",
    "    enabled=True,\n",
    "    store_type='chroma',\n",
    "    params={\n",
    "        'collection_name': 'ragdoll_pipeline_demo',\n",
    "        'persist_directory': str(pipeline_store_dir),\n",
    "    },\n",
    ")\n",
    "pipeline_vector_store = vector_store_from_config(\n",
    "    pipeline_vector_config,\n",
    "    embedding=fake_embeddings,\n",
    ")\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    app_config=app_config,\n",
    "    content_extraction_service=DocumentLoaderService(\n",
    "        app_config=app_config,\n",
    "        use_cache=False,\n",
    "        collect_metrics=False,\n",
    "    ),\n",
    "    embedding_model=fake_embeddings,\n",
    "    vector_store=pipeline_vector_store,\n",
    "    options=IngestionOptions(\n",
    "        batch_size=2,\n",
    "        extract_entities=False,\n",
    "        skip_graph_store=True,\n",
    "        chunking_options={'chunk_size': 300, 'chunk_overlap': 60, 'splitter_type': 'recursive'},\n",
    "    ),\n",
    ")\n",
    "\n",
    "pipeline_stats = await pipeline.ingest([str(SAMPLE_TXT)])\n",
    "pipeline_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01000105",
   "metadata": {},
   "source": [
    "## 9. Use the Ragdoll orchestrator\n",
    "Finally, plug the fake embeddings/vector store plus the real OpenAI LLM caller into `ragdoll.Ragdoll` so you can see how ingestion and `query` behave from the package's public API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c2b14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv('OPENAI_API_KEY'):\n",
    "    raise EnvironmentError('Set OPENAI_API_KEY before running the orchestrator demo.')\n",
    "\n",
    "rag_store_dir = reset_subdir('chroma_ragdoll_demo')\n",
    "rag_vector_config = VectorStoreConfig(\n",
    "    enabled=True,\n",
    "    store_type='chroma',\n",
    "    params={\n",
    "        'collection_name': 'ragdoll_orchestrator_demo',\n",
    "        'persist_directory': str(rag_store_dir),\n",
    "    },\n",
    ")\n",
    "rag_vector_store = vector_store_from_config(\n",
    "    rag_vector_config,\n",
    "    embedding=fake_embeddings,\n",
    ")\n",
    "\n",
    "rag_llm_caller = get_llm_caller(app_config=app_config)\n",
    "\n",
    "rag = Ragdoll(\n",
    "    app_config=app_config,\n",
    "    ingestion_service=DocumentLoaderService(\n",
    "        app_config=app_config,\n",
    "        use_cache=False,\n",
    "        collect_metrics=False,\n",
    "    ),\n",
    "    embedding_model=fake_embeddings,\n",
    "    vector_store=rag_vector_store,\n",
    "    llm_caller=rag_llm_caller,\n",
    ")\n",
    "\n",
    "question = 'What does the sample document discuss?'\n",
    "ingested = rag.ingest_data([str(SAMPLE_TXT)])\n",
    "print(f\"Ragdoll indexed {len(ingested)} LangChain document(s).\")\n",
    "rag_response = rag.query(question)\n",
    "print('LLM answer:', rag_response['answer'])\n",
    "for idx, doc in enumerate(rag_response['documents'], start=1):\n",
    "    snippet = doc.page_content[:140].replace('\n",
    "', ' ')\n",
    "    print(f\"Doc {idx} (source={doc.metadata.get('source')}): {snippet}\")\n",
    "\n",
    "graph_docs = (\n",
    "    graph_retriever.get_relevant_documents(question)\n",
    "    if 'graph_retriever' in globals()\n",
    "    else []\n",
    ")\n",
    "if graph_docs:\n",
    "    print('\n",
    "Graph retriever context:')\n",
    "    for doc in graph_docs:\n",
    "        print(f\"- {doc.metadata.get('node_type')} {doc.metadata.get('node_id')}: {doc.page_content}\")\n",
    "\n",
    "    vector_context = '\n",
    "\n",
    "'.join(\n",
    "        f\"Doc {idx}: {doc.page_content[:200]}\" for idx, doc in enumerate(rag_response['documents'], start=1)\n",
    "    ) or 'No vector hits available.'\n",
    "    graph_context = '\n",
    "'.join(\n",
    "        f\"Node {doc.metadata.get('node_id')}: {doc.page_content} (neighbors={doc.metadata.get('connected_to')})\"\n",
    "        for doc in graph_docs\n",
    "    )\n",
    "    hybrid_prompt = (\n",
    "        \"You answer using both vector chunks and graph nodes.\n",
    "\"\n",
    "        f\"Vector context:\n",
    "{vector_context}\n",
    "\n",
    "\"\n",
    "        f\"Graph context:\n",
    "{graph_context}\n",
    "\n",
    "\"\n",
    "        f\"Question: {question}\n",
    "Answer:\"\n",
    "    )\n",
    "    hybrid_answer = call_llm_sync(rag_llm_caller, hybrid_prompt)\n",
    "    print('\n",
    "Hybrid graph + vector answer:', hybrid_answer)\n",
    "else:\n",
    "    print('\n",
    "Graph retriever not initialized yet; run Section 6 to build it before rerunning this cell.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a57a80",
   "metadata": {},
   "source": [
    "---\n",
    "Feel free to duplicate this notebook and swap inputs (PDFs, DOCX, loaders, vector stores, etc.) to explore other combinations covered throughout `docs/`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f621f2b0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "054429f6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

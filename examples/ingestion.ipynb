{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecbdbf6d",
   "metadata": {},
   "source": [
    "# Ingestion Service\n",
    "\n",
    "Some basic usage examples of the RagDoll2 ingestion service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5100a982",
   "metadata": {},
   "source": [
    "## Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77f4eb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 15:47:30,527 - WARNING - Module langchain_community.document_loaders does not have attribute RtfLoader for extension .rtf. Skipping this loader.\n",
      "2025-04-17 15:47:30,527 - INFO - Loaded 18 file extension loaders\n",
      "2025-04-17 15:47:30,527 - INFO - Cache initialized at C:\\Users\\PG518JW\\.ragdoll\\cache with TTL=86400s\n",
      "2025-04-17 15:47:30,536 - INFO - Initialized with 18 loaders, max_threads=10, use_cache=True, collect_metrics=False\n",
      "2025-04-17 15:47:30,537 - INFO - Starting ingestion of 5 inputs\n",
      "2025-04-17 15:47:30,537 - INFO - Processing batch 1 with 5 sources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 15:47:36,258 - INFO - Finished ingestion, loaded 777 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 777 documents\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from ragdoll.ingestion.ingestion_service import IngestionService\n",
    "\n",
    "# Get absolute path to the test_data directory\n",
    "current_file = Path(os.path.abspath(\"\"))  # Current notebook directory\n",
    "test_data_dir = (current_file.parent / \"tests\" / \"test_data\").resolve()\n",
    "\n",
    "# Find all files using glob\n",
    "file_paths = glob.glob(str(test_data_dir / \"*\"))\n",
    "print(f\"Found {len(file_paths)} files\")\n",
    "\n",
    "# Create ingestion service with default settings\n",
    "service = IngestionService()\n",
    "\n",
    "# Process all documents\n",
    "documents = service.ingest_documents(file_paths)\n",
    "\n",
    "# Show how many documents were extracted\n",
    "print(f\"Processed {len(documents)} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b50a3d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First document content (preview): Content from the zip file `test_docx.docx`:\n",
      "\n",
      "## File: [Content_Types].xml\n",
      "\n",
      "<?xml version=\"1.0\" encod...\n",
      "Metadata:\n",
      " {\n",
      "  \"source\": \"C:\\\\dev\\\\RAGdoll\\\\tests\\\\test_data\\\\test_docx.docx\",\n",
      "  \"file_name\": \"test_docx.docx\",\n",
      "  \"file_size\": 327119,\n",
      "  \"conversion_success\": true,\n",
      "  \"metadata_extraction_error\": \"No module named 'exceptions'\",\n",
      "  \"content_type\": \"document_full\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Access the first document\n",
    "if documents:\n",
    "    doc = documents[0]\n",
    "    print(f\"First document content (preview): {doc.page_content[:100]}...\")\n",
    "    print(f\"Metadata:\\n {json.dumps(doc.metadata, indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992a8122",
   "metadata": {},
   "source": [
    "## Working with different file types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 15:47:36,326 - WARNING - Module langchain_community.document_loaders does not have attribute RtfLoader for extension .rtf. Skipping this loader.\n",
      "2025-04-17 15:47:36,329 - INFO - Loaded 18 file extension loaders\n",
      "2025-04-17 15:47:36,332 - INFO - Cache initialized at C:\\Users\\PG518JW\\.ragdoll\\cache with TTL=86400s\n",
      "2025-04-17 15:47:36,333 - INFO - Initialized with 18 loaders, max_threads=10, use_cache=True, collect_metrics=False\n",
      "2025-04-17 15:47:36,336 - INFO - Starting ingestion of 1 inputs\n",
      "2025-04-17 15:47:36,339 - INFO - Processing batch 1 with 1 sources\n",
      "2025-04-17 15:47:39,394 - INFO - Finished ingestion, loaded 773 documents\n",
      "2025-04-17 15:47:39,397 - INFO - Starting ingestion of 2 inputs\n",
      "2025-04-17 15:47:39,400 - INFO - Processing batch 1 with 2 sources\n",
      "2025-04-17 15:47:39,410 - INFO - Finished ingestion, loaded 2 documents\n",
      "2025-04-17 15:47:39,412 - INFO - Starting ingestion of 1 inputs\n",
      "2025-04-17 15:47:39,414 - INFO - Processing batch 1 with 1 sources\n",
      "2025-04-17 15:47:42,166 - INFO - Finished ingestion, loaded 1 documents\n",
      "2025-04-17 15:47:42,169 - INFO - Starting ingestion of 1 inputs\n",
      "2025-04-17 15:47:42,173 - INFO - Processing batch 1 with 1 sources\n",
      "2025-04-17 15:47:44,341 - ERROR - Error caching website:https://github.com/nsasto/langchain-markitdown: 'CacheManager' object has no attribute '_get_iso_timestamp'\n",
      "2025-04-17 15:47:44,346 - INFO - Finished ingestion, loaded 1 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 777\n",
      "Documents by type:\n",
      "  - PDF: 773\n",
      "  - Text: 2\n",
      "  - DOCX: 1\n",
      "  - Web: 1\n"
     ]
    }
   ],
   "source": [
    "from ragdoll.ingestion.ingestion_service import IngestionService\n",
    "\n",
    "# Initialize service\n",
    "service = IngestionService()\n",
    "\n",
    "# Process files of different types\n",
    "pdf_docs = service.ingest_documents([\"../tests/test_data/test_pdf.pdf\"])\n",
    "text_docs = service.ingest_documents([\"../tests/test_data/test_txt.txt\", \"../tests/test_data/test_txt.txt\"])\n",
    "docx_docs = service.ingest_documents([\"../tests/test_data/test_docx.docx\"])\n",
    "\n",
    "# Process HTML from URLs\n",
    "web_docs = service.ingest_documents([\"https://github.com/nsasto/langchain-markitdown\"])\n",
    "\n",
    "# Combine all documents\n",
    "all_docs = pdf_docs + text_docs + docx_docs + web_docs\n",
    "\n",
    "print(f\"Total documents: {len(all_docs)}\")\n",
    "print(f\"Documents by type:\")\n",
    "print(f\"  - PDF: {len(pdf_docs)}\")\n",
    "print(f\"  - Text: {len(text_docs)}\")\n",
    "print(f\"  - DOCX: {len(docx_docs)}\")\n",
    "print(f\"  - Web: {len(web_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11740b7e",
   "metadata": {},
   "source": [
    "## Customizing Ingestion Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06757cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 15:47:44,387 - WARNING - Module langchain_community.document_loaders does not have attribute RtfLoader for extension .rtf. Skipping this loader.\n",
      "2025-04-17 15:47:44,389 - INFO - Loaded 18 file extension loaders\n",
      "2025-04-17 15:47:44,392 - INFO - Cache initialized at C:\\Users\\PG518JW\\.ragdoll\\cache with TTL=86400s\n",
      "2025-04-17 15:47:44,396 - INFO - Metrics system initialized with storage at C:\\Users\\PG518JW\\.ragdoll\\metrics\n",
      "2025-04-17 15:47:44,399 - INFO - Initialized with 18 loaders, max_threads=4, use_cache=True, collect_metrics=True\n",
      "2025-04-17 15:47:44,404 - INFO - Starting ingestion of 5 inputs\n",
      "2025-04-17 15:47:44,406 - INFO - Started metrics session 09fd571f-36f3-44c4-b4ee-460b57ca054b with 5 inputs\n",
      "2025-04-17 15:47:44,412 - INFO - Processing batch 1 with 5 sources\n",
      "2025-04-17 15:47:51,026 - INFO - Metrics session completed and saved to C:\\Users\\PG518JW\\.ragdoll\\metrics\\session_09fd571f-36f3-44c4-b4ee-460b57ca054b.json\n",
      "2025-04-17 15:47:51,027 - INFO - Processed 777 documents with 100.0% success rate\n",
      "2025-04-17 15:47:51,028 - INFO - Finished ingestion, loaded 777 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 777 document chunks\n",
      "First document size: 111038 characters\n"
     ]
    }
   ],
   "source": [
    "# Modified initialization with supported parameters\n",
    "from ragdoll.ingestion.ingestion_service import IngestionService\n",
    "\n",
    "# Initialize with only the supported parameters\n",
    "service = IngestionService(\n",
    "    max_threads=4,                # Limit concurrency\n",
    "    batch_size=10,                # Process files in batches of 10\n",
    "    use_cache=True,               # Enable caching\n",
    "    collect_metrics=True          # Enable metrics collection\n",
    ")\n",
    "\n",
    "# Process documents - pass file_paths directly, not [file_paths]\n",
    "documents = service.ingest_documents(file_paths)\n",
    "\n",
    "print(f\"Processed {len(documents)} document chunks\")\n",
    "\n",
    "# Document properties can be accessed differently depending on type\n",
    "if documents:\n",
    "    doc = documents[0]\n",
    "    if hasattr(doc, 'page_content'):\n",
    "        content_length = len(doc.page_content)\n",
    "    elif isinstance(doc, dict) and 'page_content' in doc:\n",
    "        content_length = len(doc['page_content'])\n",
    "    else:\n",
    "        content_length = 0\n",
    "    print(f\"First document size: {content_length} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db81bcb",
   "metadata": {},
   "source": [
    "## Working with Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0bc5244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 15:52:18,190 - WARNING - Module langchain_community.document_loaders does not have attribute RtfLoader for extension .rtf. Skipping this loader.\n",
      "2025-04-17 15:52:18,193 - INFO - Loaded 18 file extension loaders\n",
      "2025-04-17 15:52:18,196 - INFO - Cache initialized at C:\\Users\\PG518JW\\.ragdoll\\cache with TTL=86400s\n",
      "2025-04-17 15:52:18,198 - INFO - Initialized with 18 loaders, max_threads=10, use_cache=True, collect_metrics=False\n",
      "2025-04-17 15:52:18,205 - WARNING - Module langchain_community.document_loaders does not have attribute RtfLoader for extension .rtf. Skipping this loader.\n",
      "2025-04-17 15:52:18,207 - INFO - Loaded 18 file extension loaders\n",
      "2025-04-17 15:52:18,209 - INFO - Initialized with 18 loaders, max_threads=10, use_cache=False, collect_metrics=False\n",
      "2025-04-17 15:52:18,210 - INFO - Starting ingestion of 1 inputs\n",
      "2025-04-17 15:52:18,213 - INFO - Processing batch 1 with 1 sources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 15:52:20,844 - INFO - Finished ingestion, loaded 773 documents\n",
      "2025-04-17 15:52:21,347 - INFO - Starting ingestion of 1 inputs\n",
      "2025-04-17 15:52:21,350 - INFO - Processing batch 1 with 1 sources\n",
      "2025-04-17 15:52:24,381 - INFO - Finished ingestion, loaded 773 documents\n",
      "2025-04-17 15:52:24,886 - INFO - Starting ingestion of 1 inputs\n",
      "2025-04-17 15:52:24,891 - INFO - Processing batch 1 with 1 sources\n",
      "2025-04-17 15:52:27,302 - INFO - Finished ingestion, loaded 773 documents\n",
      "2025-04-17 15:52:27,322 - WARNING - Module langchain_community.document_loaders does not have attribute RtfLoader for extension .rtf. Skipping this loader.\n",
      "2025-04-17 15:52:27,325 - INFO - Loaded 18 file extension loaders\n",
      "2025-04-17 15:52:27,326 - INFO - Cache initialized at C:\\Users\\PG518JW\\.ragdoll\\cache with TTL=3600s\n",
      "2025-04-17 15:52:27,326 - INFO - Initialized with 18 loaders, max_threads=10, use_cache=True, collect_metrics=False\n",
      "2025-04-17 15:52:27,332 - INFO - Starting ingestion of 1 inputs\n",
      "2025-04-17 15:52:27,332 - INFO - Processing batch 1 with 1 sources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Without cache:\n",
      "  Processed 773 documents\n",
      "  Average time: 2.699 seconds\n",
      "  Min time: 2.422, Max time: 3.039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 15:52:29,825 - INFO - Finished ingestion, loaded 773 documents\n",
      "2025-04-17 15:52:30,328 - INFO - Starting ingestion of 1 inputs\n",
      "2025-04-17 15:52:30,332 - INFO - Processing batch 1 with 1 sources\n",
      "2025-04-17 15:52:32,565 - INFO - Finished ingestion, loaded 773 documents\n",
      "2025-04-17 15:52:33,070 - INFO - Starting ingestion of 1 inputs\n",
      "2025-04-17 15:52:33,073 - INFO - Processing batch 1 with 1 sources\n",
      "2025-04-17 15:52:35,431 - INFO - Finished ingestion, loaded 773 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "With cache:\n",
      "  Processed 773 documents\n",
      "  Average time: 2.365 seconds\n",
      "  Min time: 2.237, Max time: 2.493\n",
      "\n",
      "Cache performance improvement: 12.4%\n"
     ]
    }
   ],
   "source": [
    "# Complete caching performance test\n",
    "from ragdoll.ingestion.ingestion_service import IngestionService\n",
    "import time\n",
    "import statistics\n",
    "\n",
    "def measure_processing_time(use_cache, file_path, runs=3):\n",
    "    \"\"\"Measure document processing time with or without cache.\"\"\"\n",
    "    times = []\n",
    "    \n",
    "    service = IngestionService(use_cache=use_cache, cache_ttl=3600)\n",
    "    \n",
    "    # Run multiple times to get average performance\n",
    "    for i in range(runs):\n",
    "        start = time.time()\n",
    "        docs = service.ingest_documents([file_path])\n",
    "        elapsed = time.time() - start\n",
    "        times.append(elapsed)\n",
    "        \n",
    "        # Don't sleep on the last run\n",
    "        if i < runs-1:\n",
    "            time.sleep(0.5)  # Short pause between runs\n",
    "    \n",
    "    return {\n",
    "        \"avg_time\": statistics.mean(times),\n",
    "        \"min_time\": min(times),\n",
    "        \"max_time\": max(times),\n",
    "        \"doc_count\": len(docs),\n",
    "        \"runs\": runs\n",
    "    }\n",
    "\n",
    "# Clear any existing cache first\n",
    "service_clear = IngestionService(use_cache=True)\n",
    "service_clear.clear_cache()\n",
    "print(\"Cache cleared\")\n",
    "\n",
    "# Test with no cache\n",
    "no_cache_results = measure_processing_time(False, \"../tests/test_data/test_pdf.pdf\", runs=3)\n",
    "print(\"\\nWithout cache:\")\n",
    "print(f\"  Processed {no_cache_results['doc_count']} documents\")\n",
    "print(f\"  Average time: {no_cache_results['avg_time']:.3f} seconds\")\n",
    "print(f\"  Min time: {no_cache_results['min_time']:.3f}, Max time: {no_cache_results['max_time']:.3f}\")\n",
    "\n",
    "# Test with cache (first run populates, subsequent runs use cache)\n",
    "cache_results = measure_processing_time(True, \"../tests/test_data/test_pdf.pdf\", runs=3)\n",
    "print(\"\\nWith cache:\")\n",
    "print(f\"  Processed {cache_results['doc_count']} documents\")\n",
    "print(f\"  Average time: {cache_results['avg_time']:.3f} seconds\")\n",
    "print(f\"  Min time: {cache_results['min_time']:.3f}, Max time: {cache_results['max_time']:.3f}\")\n",
    "\n",
    "# Speed improvement calculation\n",
    "if no_cache_results['avg_time'] > 0:\n",
    "    improvement = (no_cache_results['avg_time'] - cache_results['avg_time']) / no_cache_results['avg_time'] * 100\n",
    "    print(f\"\\nCache performance improvement: {improvement:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530e15e5",
   "metadata": {},
   "source": [
    "## Handling Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37c071a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 15:47:56,593 - WARNING - Module langchain_community.document_loaders does not have attribute RtfLoader for extension .rtf. Skipping this loader.\n",
      "2025-04-17 15:47:56,599 - INFO - Loaded 18 file extension loaders\n",
      "2025-04-17 15:47:56,602 - INFO - Cache initialized at C:\\Users\\PG518JW\\.ragdoll\\cache with TTL=86400s\n",
      "2025-04-17 15:47:56,605 - INFO - Initialized with 18 loaders, max_threads=10, use_cache=True, collect_metrics=False\n",
      "2025-04-17 15:47:56,608 - INFO - Starting ingestion of 4 inputs\n",
      "2025-04-17 15:47:56,610 - WARNING - Skipping non-file: documents\\valid.pdf\n",
      "2025-04-17 15:47:56,614 - WARNING - Skipping non-file: documents\\corrupted.pdf\n",
      "2025-04-17 15:47:56,618 - WARNING - Skipping non-file: documents\\nonexistent.txt\n",
      "2025-04-17 15:47:56,620 - WARNING - Skipping non-file: documents\\valid.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during ingestion: No valid sources found\n"
     ]
    }
   ],
   "source": [
    "from ragdoll.ingestion.ingestion_service import IngestionService\n",
    "import logging\n",
    "\n",
    "# Configure logging to see warnings and errors\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Create service\n",
    "service = IngestionService()\n",
    "\n",
    "# Mix of valid and invalid files\n",
    "files = [\n",
    "    \"documents/valid.pdf\",\n",
    "    \"documents/corrupted.pdf\",\n",
    "    \"documents/nonexistent.txt\",\n",
    "    \"documents/valid.txt\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Service will skip files it can't process\n",
    "    documents = service.ingest_documents(files)\n",
    "    print(f\"Successfully processed {len(documents)} documents\")\n",
    "    \n",
    "    # Check how many files were actually processed\n",
    "    sources = set([doc['metadata'].get('source') for doc in documents if 'source' in doc['metadata']])\n",
    "    print(f\"Documents came from {len(sources)} source files\")\n",
    "    print(f\"Source files: {sources}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during ingestion: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deea98b7",
   "metadata": {},
   "source": [
    "## Logging metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09343461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 files\n"
     ]
    }
   ],
   "source": [
    "# Replace your current loading code with this\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from ragdoll.ingestion.ingestion_service import IngestionService\n",
    "\n",
    "# Get absolute path to the test_data directory\n",
    "current_file = Path(os.path.abspath(\"\"))  # Current notebook directory\n",
    "test_data_dir = (current_file.parent / \"tests\" / \"test_data\").resolve()\n",
    "\n",
    "# Instead of using Path.glob(), use the glob module which handles absolute paths\n",
    "file_paths = glob.glob(str(test_data_dir / \"*\"))\n",
    "print(f\"Found {len(file_paths)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f60ef0",
   "metadata": {},
   "source": [
    "### Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f03f74be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 15:47:56,716 - WARNING - Module langchain_community.document_loaders does not have attribute RtfLoader for extension .rtf. Skipping this loader.\n",
      "2025-04-17 15:47:56,719 - INFO - Loaded 18 file extension loaders\n",
      "2025-04-17 15:47:56,723 - INFO - Cache initialized at C:\\Users\\PG518JW\\.ragdoll\\cache with TTL=86400s\n",
      "2025-04-17 15:47:56,727 - INFO - Metrics system initialized with storage at C:\\Users\\PG518JW\\.ragdoll\\metrics\n",
      "2025-04-17 15:47:56,729 - INFO - Initialized with 18 loaders, max_threads=10, use_cache=True, collect_metrics=True\n",
      "2025-04-17 15:47:56,731 - INFO - Starting ingestion of 5 inputs\n",
      "2025-04-17 15:47:56,732 - INFO - Started metrics session 5ef8829c-4aad-4166-99ef-5bb072baac14 with 5 inputs\n",
      "2025-04-17 15:47:56,739 - INFO - Processing batch 1 with 5 sources\n",
      "2025-04-17 15:48:04,096 - INFO - Metrics session completed and saved to C:\\Users\\PG518JW\\.ragdoll\\metrics\\session_5ef8829c-4aad-4166-99ef-5bb072baac14.json\n",
      "2025-04-17 15:48:04,099 - INFO - Processed 777 documents with 100.0% success rate\n",
      "2025-04-17 15:48:04,101 - INFO - Finished ingestion, loaded 777 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents processed: 9316\n",
      "Average success rate: 89.66%\n",
      "\n",
      "Metrics for file sources:\n",
      "  Count: 57\n",
      "  Success rate: 89.47%\n",
      "  Average documents: 163.4\n",
      "  Average processing time: 4126.6ms\n",
      "\n",
      "Latest session (5ef8829c-4aad-4166-99ef-5bb072baac14):\n",
      "  Time: 2025-04-17T15:47:56.732026\n",
      "  Documents: 777\n",
      "  Duration: 7.36 seconds\n"
     ]
    }
   ],
   "source": [
    "# Create service\n",
    "service = IngestionService(collect_metrics=True)\n",
    "\n",
    "# Pass the actual file paths, not the glob pattern\n",
    "service.ingest_documents(file_paths)\n",
    "\n",
    "\n",
    "# Get metrics after running\n",
    "metrics = service.get_metrics(days=30)  # Get metrics from the last 30 days\n",
    "\n",
    "# Use the metrics data\n",
    "print(f\"Total documents processed: {metrics['aggregate']['total_documents']}\")\n",
    "print(f\"Average success rate: {metrics['aggregate']['avg_success_rate']:.2%}\")\n",
    "\n",
    "# Print metrics for each source type\n",
    "for source_type, type_metrics in metrics['aggregate']['by_source_type'].items():\n",
    "    print(f\"\\nMetrics for {source_type} sources:\")\n",
    "    print(f\"  Count: {type_metrics['count']}\")\n",
    "    print(f\"  Success rate: {type_metrics['success_rate']:.2%}\")\n",
    "    print(f\"  Average documents: {type_metrics['avg_documents']:.1f}\")\n",
    "    print(f\"  Average processing time: {type_metrics['avg_processing_time_ms']:.1f}ms\")\n",
    "\n",
    "# Get the most recent session details\n",
    "if metrics['recent_sessions']:\n",
    "    latest = metrics['recent_sessions'][0]\n",
    "    print(f\"\\nLatest session ({latest['session_id']}):\")\n",
    "    print(f\"  Time: {latest['timestamp_start']}\")\n",
    "    print(f\"  Documents: {latest['document_count']}\")\n",
    "    print(f\"  Duration: {latest['duration_seconds']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b5cb4d",
   "metadata": {},
   "source": [
    "### Direct Access\n",
    "\n",
    "You can also access metrics directly from the metrics directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d096cbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session ID: 5ef8829c-4aad-4166-99ef-5bb072baac14\n",
      "Date: 2025-04-17T15:47:56.732026\n",
      "Documents processed: 777\n",
      "Success rate: 100.00%\n",
      "\n",
      "Source: C:\\dev\\RAGdoll\\tests\\test_data\\test_txt.txt\n",
      "  Type: file\n",
      "  Success: True\n",
      "  Documents: 1\n",
      "  Processing time: 198ms\n",
      "\n",
      "Source: C:\\dev\\RAGdoll\\tests\\test_data\\test_pdf.pdf\n",
      "  Type: file\n",
      "  Success: True\n",
      "  Documents: 773\n",
      "  Processing time: 4030ms\n",
      "\n",
      "Source: C:\\dev\\RAGdoll\\tests\\test_data\\test_xlsx.xlsx\n",
      "  Type: file\n",
      "  Success: True\n",
      "  Documents: 1\n",
      "  Processing time: 4022ms\n",
      "\n",
      "Source: C:\\dev\\RAGdoll\\tests\\test_data\\test_pptx.pptx\n",
      "  Type: file\n",
      "  Success: True\n",
      "  Documents: 1\n",
      "  Processing time: 4422ms\n",
      "\n",
      "Source: C:\\dev\\RAGdoll\\tests\\test_data\\test_docx.docx\n",
      "  Type: file\n",
      "  Success: True\n",
      "  Documents: 1\n",
      "  Processing time: 7350ms\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Default metrics location\n",
    "metrics_dir = Path.home() / \".ragdoll\" / \"metrics\"\n",
    "# Or custom location if you specified one\n",
    "# metrics_dir = Path(\"/path/to/your/metrics\")\n",
    "\n",
    "# List all session files\n",
    "session_files = list(metrics_dir.glob(\"session_*.json\"))\n",
    "# Sort by modification time (most recent first)\n",
    "session_files.sort(key=os.path.getmtime, reverse=True)\n",
    "\n",
    "# Read the most recent session\n",
    "if session_files:\n",
    "    with open(session_files[0], \"r\", encoding=\"utf-8\") as f:\n",
    "        latest_session = json.load(f)\n",
    "        \n",
    "    print(f\"Session ID: {latest_session['session_id']}\")\n",
    "    print(f\"Date: {latest_session['timestamp_start']}\")\n",
    "    print(f\"Documents processed: {latest_session['document_count']}\")\n",
    "    print(f\"Success rate: {latest_session['success_rate']:.2%}\")\n",
    "    \n",
    "    # Print details about each source\n",
    "    for source_id, source_data in latest_session[\"sources\"].items():\n",
    "        print(f\"\\nSource: {source_id}\")\n",
    "        print(f\"  Type: {source_data['source_type']}\")\n",
    "        print(f\"  Success: {source_data['success']}\")\n",
    "        print(f\"  Documents: {source_data['document_count']}\")\n",
    "        print(f\"  Processing time: {source_data['processing_time_ms']}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab4fa95",
   "metadata": {},
   "source": [
    "### Displaying Outputs\n",
    "\n",
    "Simple Metrics dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b7ce8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 15:48:05,346 - INFO - Metrics system initialized with storage at C:\\Users\\PG518JW\\.ragdoll\\metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  RAGdoll Metrics Dashboard\n",
      "================================================================================\n",
      "Showing metrics for the past 30 days:\n",
      "  Total sessions: 25\n",
      "  Total documents: 9316\n",
      "  Total sources: 58\n",
      "  Success rate: 89.66%\n",
      "  Avg processing time: 4126.6ms per source\n",
      "\n",
      "================================================================================\n",
      "  Metrics by Source Type\n",
      "================================================================================\n",
      "\n",
      "FILE Sources:\n",
      "  Count: 57\n",
      "  Success rate: 89.47%\n",
      "  Avg documents: 163.4 per source\n",
      "  Avg processing time: 4126.6ms\n",
      "\n",
      "================================================================================\n",
      "  Recent Sessions\n",
      "================================================================================\n",
      "\n",
      "Session: 5ef8829c-4aad-4166-99ef-5bb072baac14\n",
      "  Date: 2025-04-17 15:47:56\n",
      "  Duration: 7.36 seconds\n",
      "  Documents: 777\n",
      "  Sources: 5 (5 successful, 0 failed)\n",
      "  Success rate: 100.00%\n",
      "\n",
      "Session: 09fd571f-36f3-44c4-b4ee-460b57ca054b\n",
      "  Date: 2025-04-17 15:47:44\n",
      "  Duration: 6.62 seconds\n",
      "  Documents: 777\n",
      "  Sources: 5 (5 successful, 0 failed)\n",
      "  Success rate: 100.00%\n",
      "\n",
      "Session: 0d466654-4981-46fe-9a74-d99ac6a0398b\n",
      "  Date: 2025-04-17 15:45:10\n",
      "  Duration: 5.50 seconds\n",
      "  Documents: 777\n",
      "  Sources: 5 (5 successful, 0 failed)\n",
      "  Success rate: 100.00%\n",
      "\n",
      "Session: d384b383-b747-4093-9571-529ef10e900b\n",
      "  Date: 2025-04-17 15:44:03\n",
      "  Duration: 0.00 seconds\n",
      "  Documents: 0\n",
      "  Sources: 0 (0 successful, 0 failed)\n",
      "  Success rate: 0.00%\n",
      "\n",
      "Session: 60440075-025f-489d-aff3-20991c1e7e70\n",
      "  Date: 2025-04-17 15:43:07\n",
      "  Duration: 0.01 seconds\n",
      "  Documents: 0\n",
      "  Sources: 0 (0 successful, 0 failed)\n",
      "  Success rate: 0.00%\n",
      "\n",
      "================================================================================\n",
      "  Detailed Session Report: 5ef8829c-4aad-4166-99ef-5bb072baac14\n",
      "================================================================================\n",
      "Session: 5ef8829c-4aad-4166-99ef-5bb072baac14\n",
      "  Date: 2025-04-17 15:47:56\n",
      "  Duration: 7.36 seconds\n",
      "  Documents: 777\n",
      "  Sources: 5 (5 successful, 0 failed)\n",
      "  Success rate: 100.00%\n",
      "\n",
      "Source Details:\n",
      "\n",
      "✅ C:\\dev\\RAGdoll\\tests\\test_data\\test_txt.txt (file)\n",
      "  Documents: 1\n",
      "  Size: 48 B\n",
      "  Processing time: 198ms\n",
      "\n",
      "✅ C:\\dev\\RAGdoll\\tests\\test_data\\test_pdf.pdf (file)\n",
      "  Documents: 773\n",
      "  Size: 2.7 MB\n",
      "  Processing time: 4030ms\n",
      "\n",
      "✅ C:\\dev\\RAGdoll\\tests\\test_data\\test_xlsx.xlsx (file)\n",
      "  Documents: 1\n",
      "  Size: 28.5 KB\n",
      "  Processing time: 4022ms\n",
      "\n",
      "✅ C:\\dev\\RAGdoll\\tests\\test_data\\test_pptx.pptx (file)\n",
      "  Documents: 1\n",
      "  Size: 212.8 KB\n",
      "  Processing time: 4422ms\n",
      "\n",
      "✅ C:\\dev\\RAGdoll\\tests\\test_data\\test_docx.docx (file)\n",
      "  Documents: 1\n",
      "  Size: 319.5 KB\n",
      "  Processing time: 7350ms\n"
     ]
    }
   ],
   "source": [
    "# Notebook-friendly version of the dashboard script\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from ragdoll.metrics.metrics_manager import MetricsManager\n",
    "\n",
    "def print_section(title: str):\n",
    "    \"\"\"Print a section title.\"\"\"\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"  {title}\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "def print_session_summary(session: Dict[str, Any]):\n",
    "    \"\"\"Print a summary of a session.\"\"\"\n",
    "    start_time = datetime.fromisoformat(session[\"timestamp_start\"])\n",
    "    \n",
    "    print(f\"Session: {session['session_id']}\")\n",
    "    print(f\"  Date: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"  Duration: {session.get('duration_seconds', 0):.2f} seconds\")\n",
    "    print(f\"  Documents: {session['document_count']}\")\n",
    "    print(f\"  Sources: {session['success_count'] + session['failure_count']} \"\n",
    "          f\"({session['success_count']} successful, {session['failure_count']} failed)\")\n",
    "    print(f\"  Success rate: {session.get('success_rate', 0):.2%}\")\n",
    "\n",
    "def format_bytes(bytes_count: int) -> str:\n",
    "    \"\"\"Format bytes as human-readable size.\"\"\"\n",
    "    if bytes_count < 1024:\n",
    "        return f\"{bytes_count} B\"\n",
    "    elif bytes_count < 1024**2:\n",
    "        return f\"{bytes_count / 1024:.1f} KB\"\n",
    "    elif bytes_count < 1024**3:\n",
    "        return f\"{bytes_count / (1024**2):.1f} MB\"\n",
    "    else:\n",
    "        return f\"{bytes_count / (1024**3):.2f} GB\"\n",
    "\n",
    "# Initialize metrics manager with the path to your metrics directory\n",
    "metrics_dir = Path.home() / \".ragdoll\" / \"metrics\"\n",
    "metrics_manager = MetricsManager(metrics_dir=metrics_dir)\n",
    "\n",
    "# Show aggregate metrics and recent sessions\n",
    "print_section(\"RAGdoll Metrics Dashboard\")\n",
    "\n",
    "# Get aggregate metrics for the last 30 days\n",
    "days = 30\n",
    "try:\n",
    "    # Fix the date handling issue by using timedelta\n",
    "    from datetime import datetime, timedelta\n",
    "    \n",
    "    # Monkey patch the get_aggregate_metrics method to avoid date issues\n",
    "    def fixed_get_aggregate_metrics(self, days=30):\n",
    "        cutoff_date = datetime.now() - timedelta(days=days)\n",
    "        \n",
    "        aggregate = {\n",
    "            \"total_sessions\": 0,\n",
    "            \"total_documents\": 0,\n",
    "            \"total_sources\": 0,\n",
    "            \"successful_sources\": 0,\n",
    "            \"failed_sources\": 0,\n",
    "            \"avg_success_rate\": 0,\n",
    "            \"avg_documents_per_source\": 0,\n",
    "            \"avg_processing_time_ms\": 0,\n",
    "            \"by_source_type\": {}\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            json_files = list(self.metrics_dir.glob(\"session_*.json\"))\n",
    "            \n",
    "            # Process each session file\n",
    "            for file_path in json_files:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    session = json.load(f)\n",
    "                \n",
    "                # Skip if older than cutoff\n",
    "                session_date = datetime.fromisoformat(session.get(\"timestamp_start\", \"\"))\n",
    "                if session_date < cutoff_date:\n",
    "                    continue\n",
    "                \n",
    "                # Update aggregate metrics\n",
    "                aggregate[\"total_sessions\"] += 1\n",
    "                aggregate[\"total_documents\"] += session.get(\"document_count\", 0)\n",
    "                aggregate[\"total_sources\"] += session.get(\"success_count\", 0) + session.get(\"failure_count\", 0)\n",
    "                aggregate[\"successful_sources\"] += session.get(\"success_count\", 0)\n",
    "                aggregate[\"failed_sources\"] += session.get(\"failure_count\", 0)\n",
    "                \n",
    "                # Process by source type\n",
    "                for source_key, source_metrics in session.get(\"sources\", {}).items():\n",
    "                    source_type = source_metrics.get(\"source_type\", \"unknown\")\n",
    "                    \n",
    "                    if source_type not in aggregate[\"by_source_type\"]:\n",
    "                        aggregate[\"by_source_type\"][source_type] = {\n",
    "                            \"count\": 0,\n",
    "                            \"success_count\": 0,\n",
    "                            \"document_count\": 0,\n",
    "                            \"total_processing_time_ms\": 0\n",
    "                        }\n",
    "                    \n",
    "                    type_metrics = aggregate[\"by_source_type\"][source_type]\n",
    "                    type_metrics[\"count\"] += 1\n",
    "                    \n",
    "                    if source_metrics.get(\"success\", False):\n",
    "                        type_metrics[\"success_count\"] += 1\n",
    "                    \n",
    "                    type_metrics[\"document_count\"] += source_metrics.get(\"document_count\", 0)\n",
    "                    type_metrics[\"total_processing_time_ms\"] += source_metrics.get(\"processing_time_ms\", 0)\n",
    "            \n",
    "            # Calculate averages\n",
    "            if aggregate[\"total_sources\"] > 0:\n",
    "                aggregate[\"avg_success_rate\"] = aggregate[\"successful_sources\"] / aggregate[\"total_sources\"]\n",
    "                aggregate[\"avg_documents_per_source\"] = aggregate[\"total_documents\"] / aggregate[\"total_sources\"]\n",
    "                \n",
    "                total_time = 0\n",
    "                total_items = 0\n",
    "                for source_type, metrics in aggregate[\"by_source_type\"].items():\n",
    "                    total_time += metrics[\"total_processing_time_ms\"]\n",
    "                    total_items += metrics[\"count\"]\n",
    "                    \n",
    "                    # Calculate source type specific metrics\n",
    "                    if metrics[\"count\"] > 0:\n",
    "                        metrics[\"avg_processing_time_ms\"] = metrics[\"total_processing_time_ms\"] / metrics[\"count\"]\n",
    "                        metrics[\"avg_documents\"] = metrics[\"document_count\"] / metrics[\"count\"]\n",
    "                        metrics[\"success_rate\"] = metrics[\"success_count\"] / metrics[\"count\"]\n",
    "                \n",
    "                if total_items > 0:\n",
    "                    aggregate[\"avg_processing_time_ms\"] = total_time / total_items\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating aggregate metrics: {e}\")\n",
    "            \n",
    "        return aggregate\n",
    "    \n",
    "    # Apply the monkey patch\n",
    "    from types import MethodType\n",
    "    metrics_manager.get_aggregate_metrics = MethodType(fixed_get_aggregate_metrics, metrics_manager)\n",
    "    \n",
    "    aggregate = metrics_manager.get_aggregate_metrics(days=days)\n",
    "    \n",
    "    print(f\"Showing metrics for the past {days} days:\")\n",
    "    print(f\"  Total sessions: {aggregate['total_sessions']}\")\n",
    "    print(f\"  Total documents: {aggregate['total_documents']}\")\n",
    "    print(f\"  Total sources: {aggregate['total_sources']}\")\n",
    "    print(f\"  Success rate: {aggregate['avg_success_rate']:.2%}\")\n",
    "    print(f\"  Avg processing time: {aggregate['avg_processing_time_ms']:.1f}ms per source\")\n",
    "    \n",
    "    # Show metrics by source type\n",
    "    print_section(\"Metrics by Source Type\")\n",
    "    for source_type, metrics in aggregate[\"by_source_type\"].items():\n",
    "        print(f\"\\n{source_type.upper()} Sources:\")\n",
    "        print(f\"  Count: {metrics['count']}\")\n",
    "        print(f\"  Success rate: {metrics.get('success_rate', 0):.2%}\")\n",
    "        print(f\"  Avg documents: {metrics.get('avg_documents', 0):.1f} per source\")\n",
    "        print(f\"  Avg processing time: {metrics.get('avg_processing_time_ms', 0):.1f}ms\")\n",
    "    \n",
    "    # Show recent sessions\n",
    "    recent_sessions = metrics_manager.get_recent_sessions(limit=5)\n",
    "    \n",
    "    print_section(\"Recent Sessions\")\n",
    "    for session in recent_sessions:\n",
    "        print(\"\")\n",
    "        print_session_summary(session)\n",
    "        \n",
    "    # Pick a specific session to view in detail\n",
    "    if recent_sessions:\n",
    "        session_id = recent_sessions[0][\"session_id\"]\n",
    "        print_section(f\"Detailed Session Report: {session_id}\")\n",
    "        \n",
    "        # Get the session data\n",
    "        session_path = Path(metrics_manager.metrics_dir) / f\"session_{session_id}.json\"\n",
    "        with open(session_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            session = json.load(f)\n",
    "        \n",
    "        print_session_summary(session)\n",
    "        \n",
    "        print(\"\\nSource Details:\")\n",
    "        for source_id, source_data in session[\"sources\"].items():\n",
    "            success = \"✅\" if source_data[\"success\"] else \"❌\"\n",
    "            error = f\" - Error: {source_data['error']}\" if source_data[\"error\"] else \"\"\n",
    "            \n",
    "            print(f\"\\n{success} {source_id} ({source_data['source_type']}){error}\")\n",
    "            print(f\"  Documents: {source_data['document_count']}\")\n",
    "            print(f\"  Size: {format_bytes(source_data['bytes'])}\")\n",
    "            print(f\"  Processing time: {source_data['processing_time_ms']}ms\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Error running dashboard: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f39e8c",
   "metadata": {},
   "source": [
    "### Export to file\n",
    "\n",
    "Export to other formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "604f4380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Export to CSV\n",
    "def export_sessions_to_csv(metrics_manager, output_path):\n",
    "    sessions = metrics_manager.get_recent_sessions(limit=100)\n",
    "    \n",
    "    with open(output_path, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['session_id', 'timestamp', 'documents', 'sources', \n",
    "                      'success_rate', 'duration_seconds']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        writer.writeheader()\n",
    "        for session in sessions:\n",
    "            writer.writerow({\n",
    "                'session_id': session['session_id'],\n",
    "                'timestamp': session['timestamp_start'],\n",
    "                'documents': session['document_count'],\n",
    "                'sources': session['success_count'] + session['failure_count'],\n",
    "                'success_rate': session['success_rate'],\n",
    "                'duration_seconds': session['duration_seconds']\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a3c645",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

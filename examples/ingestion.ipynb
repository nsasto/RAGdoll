{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecbdbf6d",
   "metadata": {},
   "source": [
    "# Ingestion Service\n",
    "\n",
    "Some basic usage examples of the RagDoll2 ingestion service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5100a982",
   "metadata": {},
   "source": [
    "## Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f4eb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from ragdoll.ingestion import ContentExtractionService\n",
    "\n",
    "# Get absolute path to the test_data directory\n",
    "current_file = Path(os.path.abspath(\"\"))  # Current notebook directory\n",
    "test_data_dir = (current_file.parent / \"tests\" / \"test_data\").resolve()\n",
    "\n",
    "# Find all files using glob\n",
    "file_paths = glob.glob(str(test_data_dir / \"*\"))\n",
    "print(f\"Found {len(file_paths)} files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a50c3604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 21:59:59,477 - INFO - Cache initialized at C:\\Users\\PG518JW\\.ragdoll\\cache with TTL=3600s\n",
      "2025-05-27 21:59:59,478 - INFO - Metrics system initialized with storage at C:\\Users\\PG518JW\\.ragdoll\\metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 22:00:00,009 - INFO - Module langchain_markitdown.loaders for extension .epub could not be imported. This extension will not be supported.\n",
      "2025-05-27 22:00:00,010 - INFO - Module langchain_markitdown.loaders for extension .xlsx could not be imported. This extension will not be supported.\n",
      "2025-05-27 22:00:00,011 - INFO - Module langchain_markitdown.loaders for extension .html could not be imported. This extension will not be supported.\n",
      "2025-05-27 22:00:00,013 - INFO - Module langchain_markitdown.loaders for extension .bmp could not be imported. This extension will not be supported.\n",
      "2025-05-27 22:00:00,016 - INFO - Module langchain_markitdown.loaders for extension .jpeg could not be imported. This extension will not be supported.\n",
      "2025-05-27 22:00:00,019 - INFO - Module langchain_markitdown.loaders for extension .jpg could not be imported. This extension will not be supported.\n",
      "2025-05-27 22:00:00,024 - INFO - Module langchain_markitdown.loaders for extension .png could not be imported. This extension will not be supported.\n",
      "2025-05-27 22:00:00,027 - INFO - Module langchain_markitdown.loaders for extension .tiff could not be imported. This extension will not be supported.\n",
      "2025-05-27 22:00:00,132 - INFO - Module langchain_markitdown.loaders for extension .pptx could not be imported. This extension will not be supported.\n",
      "2025-05-27 22:00:00,132 - INFO - Module langchain_markitdown.loaders for extension .docx could not be imported. This extension will not be supported.\n",
      "2025-05-27 22:00:00,133 - INFO - Module langchain_markitdown.loaders for extension .xml could not be imported. This extension will not be supported.\n",
      "2025-05-27 22:00:00,133 - WARNING - Module langchain_community.document_loaders does not have attribute RtfLoader for extension .rtf. Skipping this loader.\n",
      "2025-05-27 22:00:00,211 - WARNING - USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "2025-05-27 22:00:00,212 - INFO - Loaded 9 file extension loaders\n",
      "2025-05-27 22:00:00,213 - INFO - Service initialized: loaders=9, max_threads=10\n",
      "2025-05-27 22:00:00,214 - INFO - Starting ingestion of 5 inputs\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to load C:\\dev\\RAGdoll\\tests\\test_data\\test_docx.docx: Unsupported source: ext=.docx",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mC:\\dev\\RAGdoll\\ragdoll\\content_extraction\\content_extraction.py:135\u001b[0m, in \u001b[0;36mContentExtractionService._load_source\u001b[1;34m(self, source, batch_id)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 135\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported source: ext=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;241m.\u001b[39mextension\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mValueError\u001b[0m: Unsupported source: ext=.docx",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m service \u001b[38;5;241m=\u001b[39m ContentExtractionService()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Process all documents\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[43mservice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mingest_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_paths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Show how many documents were extracted\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(documents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m documents\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\dev\\RAGdoll\\ragdoll\\content_extraction\\content_extraction.py:172\u001b[0m, in \u001b[0;36mContentExtractionService.ingest_documents\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    169\u001b[0m batch_id \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_threads) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m--> 172\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m docs \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m    174\u001b[0m         documents\u001b[38;5;241m.\u001b[39mextend(docs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[1;34m()\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[0;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 619\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 317\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32mC:\\dev\\RAGdoll\\ragdoll\\content_extraction\\content_extraction.py:172\u001b[0m, in \u001b[0;36mContentExtractionService.ingest_documents.<locals>.<lambda>\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    169\u001b[0m batch_id \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_threads) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m--> 172\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(executor\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m s: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_id\u001b[49m\u001b[43m)\u001b[49m, batch))\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m docs \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m    174\u001b[0m         documents\u001b[38;5;241m.\u001b[39mextend(docs)\n",
      "File \u001b[1;32mc:\\dev\\RAGdoll\\.venv\\Lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\dev\\RAGdoll\\.venv\\Lib\\site-packages\\retry\\api.py:73\u001b[0m, in \u001b[0;36mretry.<locals>.retry_decorator\u001b[1;34m(f, *fargs, **fkwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m args \u001b[38;5;241m=\u001b[39m fargs \u001b[38;5;28;01mif\u001b[39;00m fargs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m()\n\u001b[0;32m     72\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m fkwargs \u001b[38;5;28;01mif\u001b[39;00m fkwargs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m__retry_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexceptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_delay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackoff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjitter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\dev\\RAGdoll\\.venv\\Lib\\site-packages\\retry\\api.py:33\u001b[0m, in \u001b[0;36m__retry_internal\u001b[1;34m(f, exceptions, tries, delay, max_delay, backoff, jitter, logger)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m _tries:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     35\u001b[0m         _tries \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mC:\\dev\\RAGdoll\\ragdoll\\content_extraction\\content_extraction.py:139\u001b[0m, in \u001b[0;36mContentExtractionService._load_source\u001b[1;34m(self, source, batch_id)\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported source: ext=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;241m.\u001b[39mextension\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;241m.\u001b[39midentifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_record_metrics(metrics_info, batch_id, source, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, success\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e))\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to load C:\\dev\\RAGdoll\\tests\\test_data\\test_docx.docx: Unsupported source: ext=.docx"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create ingestion service with default settings\n",
    "\n",
    "service = ContentExtractionService()\n",
    "# Process all documents\n",
    "documents = service.ingest_documents(file_paths)\n",
    "\n",
    "# Show how many documents were extracted\n",
    "print(f\"Processed {len(documents)} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50a3d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First document content (preview): <!-- Slide number: 1 -->\n",
      "# Shapes\n",
      "\n",
      "Transparent\n",
      "\n",
      "<!-- Slide number: 2 -->\n",
      "a\n",
      "\n",
      "Restart\n",
      "\n",
      "<!-- Slide numb...\n",
      "Metadata:\n",
      " {\n",
      "  \"source\": \"/home/user/RAGdoll/tests/test_data/test_pptx.pptx\",\n",
      "  \"file_name\": \"test_pptx.pptx\",\n",
      "  \"file_size\": 217939,\n",
      "  \"conversion_success\": true,\n",
      "  \"slide_count\": 8,\n",
      "  \"author\": \"Lingineni, Raviteja\",\n",
      "  \"title\": \"feedback@customer.cool\",\n",
      "  \"created\": \"2018-06-23 03:43:30\",\n",
      "  \"modified\": \"2025-04-16 15:42:35\",\n",
      "  \"last_modified_by\": \"Nathan Sasto\",\n",
      "  \"revision\": \"52\",\n",
      "  \"image_count\": 12,\n",
      "  \"text_box_count\": 2,\n",
      "  \"chart_count\": 0,\n",
      "  \"table_count\": 0,\n",
      "  \"content_type\": \"presentation_full\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Access the first document\n",
    "if documents:\n",
    "    doc = documents[0]\n",
    "    print(f\"First document content (preview): {doc.page_content[:100]}...\")\n",
    "    print(f\"Metadata:\\n {json.dumps(doc.metadata, indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992a8122",
   "metadata": {},
   "source": [
    "## Working with different file types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 18:25:56,936 - INFO - Cache initialized at /home/user/.ragdoll/cache with TTL=3600s\n",
      "2025-05-27 18:25:56,938 - INFO - Metrics system initialized with storage at /home/user/.ragdoll/metrics\n",
      "2025-05-27 18:25:56,939 - WARNING - Module langchain_community.document_loaders does not have attribute RtfLoader for extension .rtf. Skipping this loader.\n",
      "2025-05-27 18:25:56,940 - INFO - Loaded 20 file extension loaders\n",
      "2025-05-27 18:25:56,942 - INFO - Service initialized: loaders=20, max_threads=10\n",
      "2025-05-27 18:25:56,943 - INFO - Starting ingestion of 1 inputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 18:26:00,551 - INFO - Finished ingestion: 773 documents\n",
      "2025-05-27 18:26:00,553 - INFO - Starting ingestion of 2 inputs\n",
      "2025-05-27 18:26:00,559 - INFO - Finished ingestion: 2 documents\n",
      "2025-05-27 18:26:00,561 - INFO - Starting ingestion of 1 inputs\n",
      "2025-05-27 18:26:00,820 - INFO - Finished ingestion: 1 documents\n",
      "2025-05-27 18:26:00,822 - INFO - Starting ingestion of 1 inputs\n",
      "2025-05-27 18:26:02,445 - INFO - Finished ingestion: 1 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 777\n",
      "Documents by type:\n",
      "  - PDF: 773\n",
      "  - Text: 2\n",
      "  - DOCX: 1\n",
      "  - Web: 1\n"
     ]
    }
   ],
   "source": [
    "from ragdoll.ingestion import ContentExtractionService\n",
    "\n",
    "# Initialize service\n",
    "service = ContentExtractionService()\n",
    "# Process files of different types\n",
    "pdf_docs = service.ingest_documents([\"../tests/test_data/test_pdf.pdf\"])\n",
    "text_docs = service.ingest_documents([\"../tests/test_data/test_txt.txt\", \"../tests/test_data/test_txt.txt\"])\n",
    "docx_docs = service.ingest_documents([\"../tests/test_data/test_docx.docx\"])\n",
    "\n",
    "# Process HTML from URLs\n",
    "web_docs = service.ingest_documents([\"https://github.com/nsasto/langchain-markitdown\"])\n",
    "\n",
    "# Combine all documents\n",
    "all_docs = pdf_docs + text_docs + docx_docs + web_docs\n",
    "\n",
    "print(f\"Total documents: {len(all_docs)}\")\n",
    "print(f\"Documents by type:\")\n",
    "print(f\"  - PDF: {len(pdf_docs)}\")\n",
    "print(f\"  - Text: {len(text_docs)}\")\n",
    "print(f\"  - DOCX: {len(docx_docs)}\")\n",
    "print(f\"  - Web: {len(web_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae935eb",
   "metadata": {},
   "source": [
    "## Customizing Ingestion Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06757cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 18:26:48,294 - INFO - Cache initialized at /home/user/.ragdoll/cache with TTL=3600s\n",
      "2025-05-27 18:26:48,296 - INFO - Metrics system initialized with storage at /home/user/.ragdoll/metrics\n",
      "2025-05-27 18:26:48,297 - WARNING - Module langchain_community.document_loaders does not have attribute RtfLoader for extension .rtf. Skipping this loader.\n",
      "2025-05-27 18:26:48,299 - INFO - Loaded 20 file extension loaders\n",
      "2025-05-27 18:26:48,300 - INFO - Service initialized: loaders=20, max_threads=4\n",
      "2025-05-27 18:26:48,301 - INFO - Starting ingestion of 5 inputs\n",
      "2025-05-27 18:26:48,302 - INFO - Started metrics session 7e58a3cf-c258-4e04-a3de-6c8c3d0cc685 with 5 inputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 18:26:54,551 - INFO - Metrics session completed and saved to /home/user/.ragdoll/metrics/session_7e58a3cf-c258-4e04-a3de-6c8c3d0cc685.json\n",
      "2025-05-27 18:26:54,552 - INFO - Processed 777 documents with 100.0% success rate\n",
      "2025-05-27 18:26:54,553 - INFO - Finished ingestion: 777 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 777 document chunks\n",
      "First document size: 1043 characters\n"
     ]
    }
   ],
   "source": [
    "# Modified initialization with supported parameters\n",
    "from ragdoll.ingestion import ContentExtractionService\n",
    "\n",
    "# Initialize with only the supported parameters\n",
    "service = ContentExtractionService(\n",
    "    max_threads=4,                # Limit concurrency\n",
    "    batch_size=10,                # Process files in batches of 10\n",
    "    use_cache=True,               # Enable caching\n",
    "    collect_metrics=True          # Enable metrics collection\n",
    ")\n",
    "\n",
    "# Process documents - pass file_paths directly, not [file_paths]\n",
    "documents = service.ingest_documents(file_paths)\n",
    "\n",
    "print(f\"Processed {len(documents)} document chunks\")\n",
    "\n",
    "# Document properties can be accessed differently depending on type\n",
    "if documents:\n",
    "    doc = documents[0]\n",
    "    if hasattr(doc, 'page_content'):\n",
    "        content_length = len(doc.page_content)\n",
    "    elif isinstance(doc, dict) and 'page_content' in doc:\n",
    "        content_length = len(doc['page_content'])\n",
    "    else:\n",
    "        content_length = 0\n",
    "    print(f\"First document size: {content_length} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db81bcb",
   "metadata": {},
   "source": [
    "## Working with Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bc5244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 18:27:16,507 - INFO - Cache initialized at /home/user/.ragdoll/cache with TTL=3600s\n",
      "2025-05-27 18:27:16,510 - INFO - Metrics system initialized with storage at /home/user/.ragdoll/metrics\n",
      "2025-05-27 18:27:16,512 - WARNING - Module langchain_community.document_loaders does not have attribute RtfLoader for extension .rtf. Skipping this loader.\n",
      "2025-05-27 18:27:16,514 - INFO - Loaded 20 file extension loaders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 18:27:16,515 - INFO - Service initialized: loaders=20, max_threads=10\n",
      "2025-05-27 18:27:16,566 - INFO - Cache initialized at /home/user/.ragdoll/cache with TTL=3600s\n",
      "2025-05-27 18:27:16,568 - INFO - Metrics system initialized with storage at /home/user/.ragdoll/metrics\n",
      "2025-05-27 18:27:16,570 - WARNING - Module langchain_community.document_loaders does not have attribute RtfLoader for extension .rtf. Skipping this loader.\n",
      "2025-05-27 18:27:16,571 - INFO - Loaded 20 file extension loaders\n",
      "2025-05-27 18:27:16,573 - INFO - Service initialized: loaders=20, max_threads=10\n",
      "2025-05-27 18:27:16,574 - INFO - Starting ingestion of 1 inputs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 18:27:20,167 - INFO - Finished ingestion: 773 documents\n",
      "2025-05-27 18:27:20,669 - INFO - Starting ingestion of 1 inputs\n",
      "2025-05-27 18:27:24,296 - INFO - Finished ingestion: 773 documents\n",
      "2025-05-27 18:27:24,799 - INFO - Starting ingestion of 1 inputs\n",
      "2025-05-27 18:27:28,355 - INFO - Finished ingestion: 773 documents\n",
      "2025-05-27 18:27:28,405 - INFO - Cache initialized at /home/user/.ragdoll/cache with TTL=3600s\n",
      "2025-05-27 18:27:28,406 - INFO - Metrics system initialized with storage at /home/user/.ragdoll/metrics\n",
      "2025-05-27 18:27:28,408 - WARNING - Module langchain_community.document_loaders does not have attribute RtfLoader for extension .rtf. Skipping this loader.\n",
      "2025-05-27 18:27:28,409 - INFO - Loaded 20 file extension loaders\n",
      "2025-05-27 18:27:28,410 - INFO - Service initialized: loaders=20, max_threads=10\n",
      "2025-05-27 18:27:28,411 - INFO - Starting ingestion of 1 inputs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Without cache:\n",
      "  Processed 773 documents\n",
      "  Average time: 3.595 seconds\n",
      "  Min time: 3.559, Max time: 3.630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 18:27:31,797 - INFO - Finished ingestion: 773 documents\n",
      "2025-05-27 18:27:32,298 - INFO - Starting ingestion of 1 inputs\n",
      "2025-05-27 18:27:35,719 - INFO - Finished ingestion: 773 documents\n",
      "2025-05-27 18:27:36,222 - INFO - Starting ingestion of 1 inputs\n",
      "2025-05-27 18:27:39,704 - INFO - Finished ingestion: 773 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "With cache:\n",
      "  Processed 773 documents\n",
      "  Average time: 3.431 seconds\n",
      "  Min time: 3.387, Max time: 3.484\n",
      "\n",
      "Cache performance improvement: 4.5%\n"
     ]
    }
   ],
   "source": [
    "# Complete caching performance test\n",
    "from ragdoll.ingestion import ContentExtractionService\n",
    "import time\n",
    "import statistics\n",
    "\n",
    "def measure_processing_time(use_cache: bool, file_path: str, runs: int = 3) -> dict:\n",
    "    \"\"\"Measure document processing time with or without cache.\"\"\"\n",
    "    times = []\n",
    "    \n",
    "    service = ContentExtractionService(use_cache=use_cache)\n",
    "    \n",
    "    # Run multiple times to get average performance\n",
    "    for i in range(runs):\n",
    "        start = time.time()\n",
    "        docs = service.ingest_documents([file_path])\n",
    "        elapsed = time.time() - start\n",
    "        times.append(elapsed)\n",
    "        \n",
    "        # Don't sleep on the last run\n",
    "        if i < runs-1:\n",
    "            time.sleep(0.5)  # Short pause between runs\n",
    "    \n",
    "    return {\n",
    "        \"avg_time\": statistics.mean(times),\n",
    "        \"min_time\": min(times),\n",
    "        \"max_time\": max(times),\n",
    "        \"doc_count\": len(docs),\n",
    "        \"runs\": runs\n",
    "    }\n",
    "\n",
    "# Clear any existing cache first\n",
    "service_clear = ContentExtractionService(use_cache=True)\n",
    "service_clear.clear_cache()\n",
    "print(\"Cache cleared\")\n",
    "\n",
    "# Test with no cache\n",
    "no_cache_results = measure_processing_time(False, \"../tests/test_data/test_pdf.pdf\", runs=3)\n",
    "print(\"\\nWithout cache:\")\n",
    "print(f\"  Processed {no_cache_results['doc_count']} documents\")\n",
    "print(f\"  Average time: {no_cache_results['avg_time']:.3f} seconds\")\n",
    "print(f\"  Min time: {no_cache_results['min_time']:.3f}, Max time: {no_cache_results['max_time']:.3f}\")\n",
    "\n",
    "# Test with cache (first run populates, subsequent runs use cache)\n",
    "cache_results = measure_processing_time(True, \"../tests/test_data/test_pdf.pdf\", runs=3)\n",
    "print(\"\\nWith cache:\")\n",
    "print(f\"  Processed {cache_results['doc_count']} documents\")\n",
    "print(f\"  Average time: {cache_results['avg_time']:.3f} seconds\")\n",
    "print(f\"  Min time: {cache_results['min_time']:.3f}, Max time: {cache_results['max_time']:.3f}\")\n",
    "\n",
    "# Speed improvement calculation\n",
    "if no_cache_results['avg_time'] > 0:\n",
    "    improvement = (no_cache_results['avg_time'] - cache_results['avg_time']) / no_cache_results['avg_time'] * 100\n",
    "    print(f\"\\nCache performance improvement: {improvement:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530e15e5",
   "metadata": {},
   "source": [
    "## Handling Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c071a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 18:29:55,884 - INFO - Cache initialized at /home/user/.ragdoll/cache with TTL=3600s\n",
      "2025-05-27 18:29:55,886 - INFO - Metrics system initialized with storage at /home/user/.ragdoll/metrics\n",
      "2025-05-27 18:29:55,887 - WARNING - Module langchain_community.document_loaders does not have attribute RtfLoader for extension .rtf. Skipping this loader.\n",
      "2025-05-27 18:29:55,889 - INFO - Loaded 20 file extension loaders\n",
      "2025-05-27 18:29:55,891 - INFO - Service initialized: loaders=20, max_threads=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 18:29:55,893 - INFO - Starting ingestion of 4 inputs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during ingestion: No valid sources found\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from ragdoll.ingestion import ContentExtractionService\n",
    "import logging\n",
    "\n",
    "# Configure logging to see warnings and errors\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Create service\n",
    "\n",
    "service = ContentExtractionService()\n",
    "# Mix of valid and invalid files\n",
    "files = [\n",
    "    \"documents/valid.pdf\",\n",
    "    \"documents/corrupted.pdf\",\n",
    "    \"documents/nonexistent.txt\",\n",
    "    \"documents/valid.txt\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Service will skip files it can't process\n",
    "    documents = service.ingest_documents(files)\n",
    "    print(f\"Successfully processed {len(documents)} documents\")\n",
    "    \n",
    "    # Check how many files were actually processed\n",
    "    sources = set([doc['metadata'].get('source') for doc in documents if 'source' in doc['metadata']])\n",
    "    print(f\"Documents came from {len(sources)} source files\")\n",
    "    print(f\"Source files: {sources}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during ingestion: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deea98b7",
   "metadata": {},
   "source": [
    "## Logging metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09343461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 files\n"
     ]
    }
   ],
   "source": [
    "# Replace your current loading code with this\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from ragdoll.ingestion import ContentExtractionService\n",
    "\n",
    "# Get absolute path to the test_data directory\n",
    "current_file = Path(os.path.abspath(\"\"))  # Current notebook directory\n",
    "test_data_dir = (current_file.parent / \"tests\" / \"test_data\").resolve()\n",
    "\n",
    "# Instead of using Path.glob(), use the glob module which handles absolute paths\n",
    "file_paths = glob.glob(str(test_data_dir / \"*\"))\n",
    "print(f\"Found {len(file_paths)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f60ef0",
   "metadata": {},
   "source": [
    "### Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c2f235e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 18:30:23,834 - INFO - Cache initialized at /home/user/.ragdoll/cache with TTL=3600s\n",
      "2025-05-27 18:30:23,835 - INFO - Metrics system initialized with storage at /home/user/.ragdoll/metrics\n",
      "2025-05-27 18:30:23,837 - WARNING - Module langchain_community.document_loaders does not have attribute RtfLoader for extension .rtf. Skipping this loader.\n",
      "2025-05-27 18:30:23,838 - INFO - Loaded 20 file extension loaders\n",
      "2025-05-27 18:30:23,840 - INFO - Service initialized: loaders=20, max_threads=10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'enabled': True,\n",
       " 'recent_sessions': [{'session_id': '7e58a3cf-c258-4e04-a3de-6c8c3d0cc685',\n",
       "   'timestamp_start': '2025-05-27T18:26:48.302684',\n",
       "   'timestamp_end': '2025-05-27T18:26:54.550597',\n",
       "   'input_count': 5,\n",
       "   'success_count': 5,\n",
       "   'failure_count': 0,\n",
       "   'document_count': 777,\n",
       "   'total_bytes': 3434368,\n",
       "   'total_processing_time_ms': 19229,\n",
       "   'sources': {'/home/user/RAGdoll/tests/test_data/test_txt.txt': {'batch_id': 1,\n",
       "     'source_id': '/home/user/RAGdoll/tests/test_data/test_txt.txt',\n",
       "     'source_type': '.txt',\n",
       "     'timestamp_start': '2025-05-27T18:26:48.309265',\n",
       "     'timestamp_end': '2025-05-27T18:26:48.323054',\n",
       "     'processing_time_ms': 13,\n",
       "     'success': True,\n",
       "     'document_count': 1,\n",
       "     'bytes': 48,\n",
       "     'error': None},\n",
       "    '/home/user/RAGdoll/tests/test_data/test_docx.docx': {'batch_id': 1,\n",
       "     'source_id': '/home/user/RAGdoll/tests/test_data/test_docx.docx',\n",
       "     'source_type': '.docx',\n",
       "     'timestamp_start': '2025-05-27T18:26:48.306300',\n",
       "     'timestamp_end': '2025-05-27T18:26:50.686459',\n",
       "     'processing_time_ms': 2380,\n",
       "     'success': True,\n",
       "     'document_count': 1,\n",
       "     'bytes': 327119,\n",
       "     'error': None},\n",
       "    '/home/user/RAGdoll/tests/test_data/test_xlsx.xlsx': {'batch_id': 1,\n",
       "     'source_id': '/home/user/RAGdoll/tests/test_data/test_xlsx.xlsx',\n",
       "     'source_type': '.xlsx',\n",
       "     'timestamp_start': '2025-05-27T18:26:48.323341',\n",
       "     'timestamp_end': '2025-05-27T18:26:52.737787',\n",
       "     'processing_time_ms': 4414,\n",
       "     'success': True,\n",
       "     'document_count': 1,\n",
       "     'bytes': 29215,\n",
       "     'error': None},\n",
       "    '/home/user/RAGdoll/tests/test_data/test_pdf.pdf': {'batch_id': 1,\n",
       "     'source_id': '/home/user/RAGdoll/tests/test_data/test_pdf.pdf',\n",
       "     'source_type': '.pdf',\n",
       "     'timestamp_start': '2025-05-27T18:26:48.309969',\n",
       "     'timestamp_end': '2025-05-27T18:26:54.488334',\n",
       "     'processing_time_ms': 6178,\n",
       "     'success': True,\n",
       "     'document_count': 773,\n",
       "     'bytes': 2860047,\n",
       "     'error': None},\n",
       "    '/home/user/RAGdoll/tests/test_data/test_pptx.pptx': {'batch_id': 1,\n",
       "     'source_id': '/home/user/RAGdoll/tests/test_data/test_pptx.pptx',\n",
       "     'source_type': '.pptx',\n",
       "     'timestamp_start': '2025-05-27T18:26:48.305403',\n",
       "     'timestamp_end': '2025-05-27T18:26:54.549860',\n",
       "     'processing_time_ms': 6244,\n",
       "     'success': True,\n",
       "     'document_count': 1,\n",
       "     'bytes': 217939,\n",
       "     'error': None}},\n",
       "   'duration_seconds': 6.247913,\n",
       "   'success_rate': 1.0},\n",
       "  {'session_id': 'e0b2d099-c37d-4295-9fe3-6b6f1d098e44',\n",
       "   'timestamp_start': '2025-04-23T11:14:05.688869',\n",
       "   'timestamp_end': '2025-04-23T11:14:05.689662',\n",
       "   'input_count': 1,\n",
       "   'success_count': 0,\n",
       "   'failure_count': 0,\n",
       "   'document_count': 1,\n",
       "   'total_bytes': 0,\n",
       "   'total_processing_time_ms': 0,\n",
       "   'sources': {},\n",
       "   'duration_seconds': 0.000793,\n",
       "   'success_rate': 0},\n",
       "  {'session_id': '13d9f59e-3ac2-44b6-8bf5-0de062ff85a4',\n",
       "   'timestamp_start': '2025-04-23T11:13:23.627064',\n",
       "   'timestamp_end': '2025-04-23T11:13:23.627861',\n",
       "   'input_count': 1,\n",
       "   'success_count': 0,\n",
       "   'failure_count': 0,\n",
       "   'document_count': 1,\n",
       "   'total_bytes': 0,\n",
       "   'total_processing_time_ms': 0,\n",
       "   'sources': {},\n",
       "   'duration_seconds': 0.000797,\n",
       "   'success_rate': 0},\n",
       "  {'session_id': '827b12ee-0762-4b8c-97d3-754e817c55a7',\n",
       "   'timestamp_start': '2025-04-23T11:13:14.672480',\n",
       "   'timestamp_end': '2025-04-23T11:13:14.673464',\n",
       "   'input_count': 1,\n",
       "   'success_count': 0,\n",
       "   'failure_count': 0,\n",
       "   'document_count': 1,\n",
       "   'total_bytes': 0,\n",
       "   'total_processing_time_ms': 0,\n",
       "   'sources': {},\n",
       "   'duration_seconds': 0.000984,\n",
       "   'success_rate': 0},\n",
       "  {'session_id': '7d02e4a3-1b03-4773-9bdc-32eb88c843db',\n",
       "   'timestamp_start': '2025-04-23T11:12:30.210593',\n",
       "   'timestamp_end': '2025-04-23T11:12:30.211386',\n",
       "   'input_count': 1,\n",
       "   'success_count': 0,\n",
       "   'failure_count': 0,\n",
       "   'document_count': 1,\n",
       "   'total_bytes': 0,\n",
       "   'total_processing_time_ms': 0,\n",
       "   'sources': {},\n",
       "   'duration_seconds': 0.000793,\n",
       "   'success_rate': 0}],\n",
       " 'aggregate': {'total_sessions': 1,\n",
       "  'total_documents': 777,\n",
       "  'total_sources': 5,\n",
       "  'successful_sources': 5,\n",
       "  'failed_sources': 0,\n",
       "  'avg_success_rate': 1.0,\n",
       "  'avg_documents_per_source': 155.4,\n",
       "  'avg_processing_time_ms': 3845.8,\n",
       "  'by_source_type': {'.txt': {'count': 1,\n",
       "    'success_count': 1,\n",
       "    'document_count': 1,\n",
       "    'total_processing_time_ms': 13,\n",
       "    'avg_processing_time_ms': 13.0,\n",
       "    'avg_documents': 1.0,\n",
       "    'success_rate': 1.0},\n",
       "   '.docx': {'count': 1,\n",
       "    'success_count': 1,\n",
       "    'document_count': 1,\n",
       "    'total_processing_time_ms': 2380,\n",
       "    'avg_processing_time_ms': 2380.0,\n",
       "    'avg_documents': 1.0,\n",
       "    'success_rate': 1.0},\n",
       "   '.xlsx': {'count': 1,\n",
       "    'success_count': 1,\n",
       "    'document_count': 1,\n",
       "    'total_processing_time_ms': 4414,\n",
       "    'avg_processing_time_ms': 4414.0,\n",
       "    'avg_documents': 1.0,\n",
       "    'success_rate': 1.0},\n",
       "   '.pdf': {'count': 1,\n",
       "    'success_count': 1,\n",
       "    'document_count': 773,\n",
       "    'total_processing_time_ms': 6178,\n",
       "    'avg_processing_time_ms': 6178.0,\n",
       "    'avg_documents': 773.0,\n",
       "    'success_rate': 1.0},\n",
       "   '.pptx': {'count': 1,\n",
       "    'success_count': 1,\n",
       "    'document_count': 1,\n",
       "    'total_processing_time_ms': 6244,\n",
       "    'avg_processing_time_ms': 6244.0,\n",
       "    'avg_documents': 1.0,\n",
       "    'success_rate': 1.0}}}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create service\n",
    "service = ContentExtractionService(collect_metrics=True)\n",
    "metrics = service.get_metrics(days=30) \n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f03f74be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 18:30:25,389 - INFO - Cache initialized at /home/user/.ragdoll/cache with TTL=3600s\n",
      "2025-05-27 18:30:25,390 - INFO - Metrics system initialized with storage at /home/user/.ragdoll/metrics\n",
      "2025-05-27 18:30:25,392 - WARNING - Module langchain_community.document_loaders does not have attribute RtfLoader for extension .rtf. Skipping this loader.\n",
      "2025-05-27 18:30:25,395 - INFO - Loaded 20 file extension loaders\n",
      "2025-05-27 18:30:25,397 - INFO - Service initialized: loaders=20, max_threads=10\n",
      "2025-05-27 18:30:25,401 - INFO - Starting ingestion of 5 inputs\n",
      "2025-05-27 18:30:25,403 - INFO - Started metrics session e6f816fb-5e9a-4e69-bfc0-8f995d353b57 with 5 inputs\n",
      "2025-05-27 18:30:31,816 - INFO - Metrics session completed and saved to /home/user/.ragdoll/metrics/session_e6f816fb-5e9a-4e69-bfc0-8f995d353b57.json\n",
      "2025-05-27 18:30:31,817 - INFO - Processed 777 documents with 100.0% success rate\n",
      "2025-05-27 18:30:31,819 - INFO - Finished ingestion: 777 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents processed: 1554\n",
      "Average success rate: 100.00%\n",
      "\n",
      "Metrics for .txt sources:\n",
      "  Count: 2\n",
      "  Success rate: 100.00%\n",
      "  Average documents: 1.0\n",
      "  Average processing time: 6.5ms\n",
      "\n",
      "Metrics for .docx sources:\n",
      "  Count: 2\n",
      "  Success rate: 100.00%\n",
      "  Average documents: 1.0\n",
      "  Average processing time: 2369.5ms\n",
      "\n",
      "Metrics for .xlsx sources:\n",
      "  Count: 2\n",
      "  Success rate: 100.00%\n",
      "  Average documents: 1.0\n",
      "  Average processing time: 4834.0ms\n",
      "\n",
      "Metrics for .pdf sources:\n",
      "  Count: 2\n",
      "  Success rate: 100.00%\n",
      "  Average documents: 773.0\n",
      "  Average processing time: 6207.5ms\n",
      "\n",
      "Metrics for .pptx sources:\n",
      "  Count: 2\n",
      "  Success rate: 100.00%\n",
      "  Average documents: 1.0\n",
      "  Average processing time: 6326.5ms\n",
      "\n",
      "Latest session (e6f816fb-5e9a-4e69-bfc0-8f995d353b57):\n",
      "  Time: 2025-05-27T18:30:25.403459\n",
      "  Documents: 777\n",
      "  Duration: 6.41 seconds\n"
     ]
    }
   ],
   "source": [
    "# Create service\n",
    "service = ContentExtractionService(collect_metrics=True)\n",
    "\n",
    "# Pass the actual file paths, not the glob pattern\n",
    "service.ingest_documents(file_paths)\n",
    "\n",
    "\n",
    "# Get metrics after running\n",
    "metrics = service.get_metrics(days=30)  # Get metrics from the last 30 days\n",
    "\n",
    "# Use the metrics data\n",
    "print(f\"Total documents processed: {metrics['aggregate']['total_documents']}\")\n",
    "print(f\"Average success rate: {metrics['aggregate']['avg_success_rate']:.2%}\")\n",
    "\n",
    "# Print metrics for each source type\n",
    "for source_type, type_metrics in metrics['aggregate']['by_source_type'].items():\n",
    "    print(f\"\\nMetrics for {source_type} sources:\")\n",
    "    print(f\"  Count: {type_metrics['count']}\")\n",
    "    print(f\"  Success rate: {type_metrics['success_rate']:.2%}\")\n",
    "    print(f\"  Average documents: {type_metrics['avg_documents']:.1f}\")\n",
    "    print(f\"  Average processing time: {type_metrics['avg_processing_time_ms']:.1f}ms\")\n",
    "\n",
    "# Get the most recent session details\n",
    "if metrics['recent_sessions']:\n",
    "    latest = metrics['recent_sessions'][0]\n",
    "    print(f\"\\nLatest session ({latest['session_id']}):\")\n",
    "    print(f\"  Time: {latest['timestamp_start']}\")\n",
    "    print(f\"  Documents: {latest['document_count']}\")\n",
    "    print(f\"  Duration: {latest['duration_seconds']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b5cb4d",
   "metadata": {},
   "source": [
    "### Direct Access\n",
    "\n",
    "You can also access metrics directly from the metrics directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d096cbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session ID: e6f816fb-5e9a-4e69-bfc0-8f995d353b57\n",
      "Date: 2025-05-27T18:30:25.403459\n",
      "Documents processed: 777\n",
      "Success rate: 100.00%\n",
      "\n",
      "Source: /home/user/RAGdoll/tests/test_data/test_txt.txt\n",
      "  Type: .txt\n",
      "  Success: True\n",
      "  Documents: 1\n",
      "  Processing time: 0ms\n",
      "\n",
      "Source: /home/user/RAGdoll/tests/test_data/test_docx.docx\n",
      "  Type: .docx\n",
      "  Success: True\n",
      "  Documents: 1\n",
      "  Processing time: 2359ms\n",
      "\n",
      "Source: /home/user/RAGdoll/tests/test_data/test_xlsx.xlsx\n",
      "  Type: .xlsx\n",
      "  Success: True\n",
      "  Documents: 1\n",
      "  Processing time: 5254ms\n",
      "\n",
      "Source: /home/user/RAGdoll/tests/test_data/test_pdf.pdf\n",
      "  Type: .pdf\n",
      "  Success: True\n",
      "  Documents: 773\n",
      "  Processing time: 6237ms\n",
      "\n",
      "Source: /home/user/RAGdoll/tests/test_data/test_pptx.pptx\n",
      "  Type: .pptx\n",
      "  Success: True\n",
      "  Documents: 1\n",
      "  Processing time: 6409ms\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Default metrics location\n",
    "metrics_dir = Path.home() / \".ragdoll\" / \"metrics\"\n",
    "# Or custom location if you specified one\n",
    "# metrics_dir = Path(\"/path/to/your/metrics\")\n",
    "\n",
    "# List all session files\n",
    "session_files = list(metrics_dir.glob(\"session_*.json\"))\n",
    "# Sort by modification time (most recent first)\n",
    "session_files.sort(key=os.path.getmtime, reverse=True)\n",
    "\n",
    "# Read the most recent session\n",
    "if session_files:\n",
    "    with open(session_files[0], \"r\", encoding=\"utf-8\") as f:\n",
    "        latest_session = json.load(f)\n",
    "        \n",
    "    print(f\"Session ID: {latest_session['session_id']}\")\n",
    "    print(f\"Date: {latest_session['timestamp_start']}\")\n",
    "    print(f\"Documents processed: {latest_session['document_count']}\")\n",
    "    print(f\"Success rate: {latest_session['success_rate']:.2%}\")\n",
    "    \n",
    "    # Print details about each source\n",
    "    for source_id, source_data in latest_session[\"sources\"].items():\n",
    "        print(f\"\\nSource: {source_id}\")\n",
    "        print(f\"  Type: {source_data['source_type']}\")\n",
    "        print(f\"  Success: {source_data['success']}\")\n",
    "        print(f\"  Documents: {source_data['document_count']}\")\n",
    "        print(f\"  Processing time: {source_data['processing_time_ms']}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab4fa95",
   "metadata": {},
   "source": [
    "### Displaying Outputs\n",
    "\n",
    "Simple Metrics dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b7ce8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 18:30:34,942 - INFO - Metrics system initialized with storage at /home/user/.ragdoll/metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  RAGdoll Metrics Dashboard\n",
      "================================================================================\n",
      "Showing metrics for the past 30 days:\n",
      "  Total sessions: 2\n",
      "  Total documents: 1554\n",
      "  Total sources: 10\n",
      "  Success rate: 100.00%\n",
      "  Avg processing time: 3948.8ms per source\n",
      "\n",
      "================================================================================\n",
      "  Metrics by Source Type\n",
      "================================================================================\n",
      "\n",
      ".TXT Sources:\n",
      "  Count: 2\n",
      "  Success rate: 100.00%\n",
      "  Avg documents: 1.0 per source\n",
      "  Avg processing time: 6.5ms\n",
      "\n",
      ".DOCX Sources:\n",
      "  Count: 2\n",
      "  Success rate: 100.00%\n",
      "  Avg documents: 1.0 per source\n",
      "  Avg processing time: 2369.5ms\n",
      "\n",
      ".XLSX Sources:\n",
      "  Count: 2\n",
      "  Success rate: 100.00%\n",
      "  Avg documents: 1.0 per source\n",
      "  Avg processing time: 4834.0ms\n",
      "\n",
      ".PDF Sources:\n",
      "  Count: 2\n",
      "  Success rate: 100.00%\n",
      "  Avg documents: 773.0 per source\n",
      "  Avg processing time: 6207.5ms\n",
      "\n",
      ".PPTX Sources:\n",
      "  Count: 2\n",
      "  Success rate: 100.00%\n",
      "  Avg documents: 1.0 per source\n",
      "  Avg processing time: 6326.5ms\n",
      "\n",
      "================================================================================\n",
      "  Recent Sessions\n",
      "================================================================================\n",
      "\n",
      "Session: e6f816fb-5e9a-4e69-bfc0-8f995d353b57\n",
      "  Date: 2025-05-27 18:30:25\n",
      "  Duration: 6.41 seconds\n",
      "  Documents: 777\n",
      "  Sources: 5 (5 successful, 0 failed)\n",
      "  Success rate: 100.00%\n",
      "\n",
      "Session: 7e58a3cf-c258-4e04-a3de-6c8c3d0cc685\n",
      "  Date: 2025-05-27 18:26:48\n",
      "  Duration: 6.25 seconds\n",
      "  Documents: 777\n",
      "  Sources: 5 (5 successful, 0 failed)\n",
      "  Success rate: 100.00%\n",
      "\n",
      "Session: e0b2d099-c37d-4295-9fe3-6b6f1d098e44\n",
      "  Date: 2025-04-23 11:14:05\n",
      "  Duration: 0.00 seconds\n",
      "  Documents: 1\n",
      "  Sources: 0 (0 successful, 0 failed)\n",
      "  Success rate: 0.00%\n",
      "\n",
      "Session: 13d9f59e-3ac2-44b6-8bf5-0de062ff85a4\n",
      "  Date: 2025-04-23 11:13:23\n",
      "  Duration: 0.00 seconds\n",
      "  Documents: 1\n",
      "  Sources: 0 (0 successful, 0 failed)\n",
      "  Success rate: 0.00%\n",
      "\n",
      "Session: 827b12ee-0762-4b8c-97d3-754e817c55a7\n",
      "  Date: 2025-04-23 11:13:14\n",
      "  Duration: 0.00 seconds\n",
      "  Documents: 1\n",
      "  Sources: 0 (0 successful, 0 failed)\n",
      "  Success rate: 0.00%\n",
      "\n",
      "================================================================================\n",
      "  Detailed Session Report: e6f816fb-5e9a-4e69-bfc0-8f995d353b57\n",
      "================================================================================\n",
      "Session: e6f816fb-5e9a-4e69-bfc0-8f995d353b57\n",
      "  Date: 2025-05-27 18:30:25\n",
      "  Duration: 6.41 seconds\n",
      "  Documents: 777\n",
      "  Sources: 5 (5 successful, 0 failed)\n",
      "  Success rate: 100.00%\n",
      "\n",
      "Source Details:\n",
      "\n",
      "✅ /home/user/RAGdoll/tests/test_data/test_txt.txt (.txt)\n",
      "  Documents: 1\n",
      "  Size: 48 B\n",
      "  Processing time: 0ms\n",
      "\n",
      "✅ /home/user/RAGdoll/tests/test_data/test_docx.docx (.docx)\n",
      "  Documents: 1\n",
      "  Size: 319.5 KB\n",
      "  Processing time: 2359ms\n",
      "\n",
      "✅ /home/user/RAGdoll/tests/test_data/test_xlsx.xlsx (.xlsx)\n",
      "  Documents: 1\n",
      "  Size: 28.5 KB\n",
      "  Processing time: 5254ms\n",
      "\n",
      "✅ /home/user/RAGdoll/tests/test_data/test_pdf.pdf (.pdf)\n",
      "  Documents: 773\n",
      "  Size: 2.7 MB\n",
      "  Processing time: 6237ms\n",
      "\n",
      "✅ /home/user/RAGdoll/tests/test_data/test_pptx.pptx (.pptx)\n",
      "  Documents: 1\n",
      "  Size: 212.8 KB\n",
      "  Processing time: 6409ms\n"
     ]
    }
   ],
   "source": [
    "# Notebook-friendly version of the dashboard script\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from ragdoll.metrics.metrics_manager import MetricsManager\n",
    "\n",
    "def print_section(title: str):\n",
    "    \"\"\"Print a section title.\"\"\"\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"  {title}\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "def print_session_summary(session: Dict[str, Any]):\n",
    "    \"\"\"Print a summary of a session.\"\"\"\n",
    "    start_time = datetime.fromisoformat(session[\"timestamp_start\"])\n",
    "    \n",
    "    print(f\"Session: {session['session_id']}\")\n",
    "    print(f\"  Date: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"  Duration: {session.get('duration_seconds', 0):.2f} seconds\")\n",
    "    print(f\"  Documents: {session['document_count']}\")\n",
    "    print(f\"  Sources: {session['success_count'] + session['failure_count']} \"\n",
    "          f\"({session['success_count']} successful, {session['failure_count']} failed)\")\n",
    "    print(f\"  Success rate: {session.get('success_rate', 0):.2%}\")\n",
    "\n",
    "def format_bytes(bytes_count: int) -> str:\n",
    "    \"\"\"Format bytes as human-readable size.\"\"\"\n",
    "    if bytes_count < 1024:\n",
    "        return f\"{bytes_count} B\"\n",
    "    elif bytes_count < 1024**2:\n",
    "        return f\"{bytes_count / 1024:.1f} KB\"\n",
    "    elif bytes_count < 1024**3:\n",
    "        return f\"{bytes_count / (1024**2):.1f} MB\"\n",
    "    else:\n",
    "        return f\"{bytes_count / (1024**3):.2f} GB\"\n",
    "\n",
    "# Initialize metrics manager with the path to your metrics directory\n",
    "metrics_dir = Path.home() / \".ragdoll\" / \"metrics\"\n",
    "metrics_manager = MetricsManager(metrics_dir=metrics_dir)\n",
    "\n",
    "# Show aggregate metrics and recent sessions\n",
    "print_section(\"RAGdoll Metrics Dashboard\")\n",
    "\n",
    "# Get aggregate metrics for the last 30 days\n",
    "days = 30\n",
    "try:\n",
    "    # Fix the date handling issue by using timedelta\n",
    "    from datetime import datetime, timedelta\n",
    "    \n",
    "    # Monkey patch the get_aggregate_metrics method to avoid date issues\n",
    "    def fixed_get_aggregate_metrics(self, days=30):\n",
    "        cutoff_date = datetime.now() - timedelta(days=days)\n",
    "        \n",
    "        aggregate = {\n",
    "            \"total_sessions\": 0,\n",
    "            \"total_documents\": 0,\n",
    "            \"total_sources\": 0,\n",
    "            \"successful_sources\": 0,\n",
    "            \"failed_sources\": 0,\n",
    "            \"avg_success_rate\": 0,\n",
    "            \"avg_documents_per_source\": 0,\n",
    "            \"avg_processing_time_ms\": 0,\n",
    "            \"by_source_type\": {}\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            json_files = list(self.metrics_dir.glob(\"session_*.json\"))\n",
    "            \n",
    "            # Process each session file\n",
    "            for file_path in json_files:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    session = json.load(f)\n",
    "                \n",
    "                # Skip if older than cutoff\n",
    "                session_date = datetime.fromisoformat(session.get(\"timestamp_start\", \"\"))\n",
    "                if session_date < cutoff_date:\n",
    "                    continue\n",
    "                \n",
    "                # Update aggregate metrics\n",
    "                aggregate[\"total_sessions\"] += 1\n",
    "                aggregate[\"total_documents\"] += session.get(\"document_count\", 0)\n",
    "                aggregate[\"total_sources\"] += session.get(\"success_count\", 0) + session.get(\"failure_count\", 0)\n",
    "                aggregate[\"successful_sources\"] += session.get(\"success_count\", 0)\n",
    "                aggregate[\"failed_sources\"] += session.get(\"failure_count\", 0)\n",
    "                \n",
    "                # Process by source type\n",
    "                for source_key, source_metrics in session.get(\"sources\", {}).items():\n",
    "                    source_type = source_metrics.get(\"source_type\", \"unknown\")\n",
    "                    \n",
    "                    if source_type not in aggregate[\"by_source_type\"]:\n",
    "                        aggregate[\"by_source_type\"][source_type] = {\n",
    "                            \"count\": 0,\n",
    "                            \"success_count\": 0,\n",
    "                            \"document_count\": 0,\n",
    "                            \"total_processing_time_ms\": 0\n",
    "                        }\n",
    "                    \n",
    "                    type_metrics = aggregate[\"by_source_type\"][source_type]\n",
    "                    type_metrics[\"count\"] += 1\n",
    "                    \n",
    "                    if source_metrics.get(\"success\", False):\n",
    "                        type_metrics[\"success_count\"] += 1\n",
    "                    \n",
    "                    type_metrics[\"document_count\"] += source_metrics.get(\"document_count\", 0)\n",
    "                    type_metrics[\"total_processing_time_ms\"] += source_metrics.get(\"processing_time_ms\", 0)\n",
    "            \n",
    "            # Calculate averages\n",
    "            if aggregate[\"total_sources\"] > 0:\n",
    "                aggregate[\"avg_success_rate\"] = aggregate[\"successful_sources\"] / aggregate[\"total_sources\"]\n",
    "                aggregate[\"avg_documents_per_source\"] = aggregate[\"total_documents\"] / aggregate[\"total_sources\"]\n",
    "                \n",
    "                total_time = 0\n",
    "                total_items = 0\n",
    "                for source_type, metrics in aggregate[\"by_source_type\"].items():\n",
    "                    total_time += metrics[\"total_processing_time_ms\"]\n",
    "                    total_items += metrics[\"count\"]\n",
    "                    \n",
    "                    # Calculate source type specific metrics\n",
    "                    if metrics[\"count\"] > 0:\n",
    "                        metrics[\"avg_processing_time_ms\"] = metrics[\"total_processing_time_ms\"] / metrics[\"count\"]\n",
    "                        metrics[\"avg_documents\"] = metrics[\"document_count\"] / metrics[\"count\"]\n",
    "                        metrics[\"success_rate\"] = metrics[\"success_count\"] / metrics[\"count\"]\n",
    "                \n",
    "                if total_items > 0:\n",
    "                    aggregate[\"avg_processing_time_ms\"] = total_time / total_items\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating aggregate metrics: {e}\")\n",
    "            \n",
    "        return aggregate\n",
    "    \n",
    "    # Apply the monkey patch\n",
    "    from types import MethodType\n",
    "    metrics_manager.get_aggregate_metrics = MethodType(fixed_get_aggregate_metrics, metrics_manager)\n",
    "    \n",
    "    aggregate = metrics_manager.get_aggregate_metrics(days=days)\n",
    "    \n",
    "    print(f\"Showing metrics for the past {days} days:\")\n",
    "    print(f\"  Total sessions: {aggregate['total_sessions']}\")\n",
    "    print(f\"  Total documents: {aggregate['total_documents']}\")\n",
    "    print(f\"  Total sources: {aggregate['total_sources']}\")\n",
    "    print(f\"  Success rate: {aggregate['avg_success_rate']:.2%}\")\n",
    "    print(f\"  Avg processing time: {aggregate['avg_processing_time_ms']:.1f}ms per source\")\n",
    "    \n",
    "    # Show metrics by source type\n",
    "    print_section(\"Metrics by Source Type\")\n",
    "    for source_type, metrics in aggregate[\"by_source_type\"].items():\n",
    "        print(f\"\\n{source_type.upper()} Sources:\")\n",
    "        print(f\"  Count: {metrics['count']}\")\n",
    "        print(f\"  Success rate: {metrics.get('success_rate', 0):.2%}\")\n",
    "        print(f\"  Avg documents: {metrics.get('avg_documents', 0):.1f} per source\")\n",
    "        print(f\"  Avg processing time: {metrics.get('avg_processing_time_ms', 0):.1f}ms\")\n",
    "    \n",
    "    # Show recent sessions\n",
    "    recent_sessions = metrics_manager.get_recent_sessions(limit=5)\n",
    "    \n",
    "    print_section(\"Recent Sessions\")\n",
    "    for session in recent_sessions:\n",
    "        print(\"\")\n",
    "        print_session_summary(session)\n",
    "        \n",
    "    # Pick a specific session to view in detail\n",
    "    if recent_sessions:\n",
    "        session_id = recent_sessions[0][\"session_id\"]\n",
    "        print_section(f\"Detailed Session Report: {session_id}\")\n",
    "        \n",
    "        # Get the session data\n",
    "        session_path = Path(metrics_manager.metrics_dir) / f\"session_{session_id}.json\"\n",
    "        with open(session_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            session = json.load(f)\n",
    "        \n",
    "        print_session_summary(session)\n",
    "        \n",
    "        print(\"\\nSource Details:\")\n",
    "        for source_id, source_data in session[\"sources\"].items():\n",
    "            success = \"✅\" if source_data[\"success\"] else \"❌\"\n",
    "            error = f\" - Error: {source_data['error']}\" if source_data[\"error\"] else \"\"\n",
    "            \n",
    "            print(f\"\\n{success} {source_id} ({source_data['source_type']}){error}\")\n",
    "            print(f\"  Documents: {source_data['document_count']}\")\n",
    "            print(f\"  Size: {format_bytes(source_data['bytes'])}\")\n",
    "            print(f\"  Processing time: {source_data['processing_time_ms']}ms\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Error running dashboard: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f39e8c",
   "metadata": {},
   "source": [
    "### Export to file\n",
    "\n",
    "Export to other formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "604f4380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Export to CSV\n",
    "def export_sessions_to_csv(metrics_manager, output_path):\n",
    "    sessions = metrics_manager.get_recent_sessions(limit=100)\n",
    "    \n",
    "    with open(output_path, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['session_id', 'timestamp', 'documents', 'sources', \n",
    "                      'success_rate', 'duration_seconds']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        writer.writeheader()\n",
    "        for session in sessions:\n",
    "            writer.writerow({\n",
    "                'session_id': session['session_id'],\n",
    "                'timestamp': session['timestamp_start'],\n",
    "                'documents': session['document_count'],\n",
    "                'sources': session['success_count'] + session['failure_count'],\n",
    "                'success_rate': session['success_rate'],\n",
    "                'duration_seconds': session['duration_seconds']\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a3c645",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

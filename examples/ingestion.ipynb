{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecbdbf6d",
   "metadata": {},
   "source": [
    "# Ingestion Service\n",
    "\n",
    "Some basic usage examples of the RagDoll2 ingestion service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5100a982",
   "metadata": {},
   "source": [
    "## Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77f4eb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 15:37:16,863 - WARNING - Module langchain_community.document_loaders does not have attribute RtfLoader for extension .rtf. Skipping this loader.\n",
      "2025-04-17 15:37:16,864 - INFO - Loaded 18 file extension loaders\n",
      "2025-04-17 15:37:16,866 - INFO - Cache initialized at C:\\Users\\PG518JW\\.ragdoll\\cache with TTL=86400s\n",
      "2025-04-17 15:37:16,867 - INFO - Initialized with 18 loaders, max_threads=10, use_cache=True, collect_metrics=False\n",
      "2025-04-17 15:37:16,868 - INFO - Starting ingestion of 5 inputs\n",
      "2025-04-17 15:37:16,874 - INFO - Processing batch 1 with 5 sources\n",
      "2025-04-17 15:37:25,854 - INFO - Finished ingestion, loaded 777 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 777 documents\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from ragdoll.ingestion.ingestion_service import IngestionService\n",
    "\n",
    "# Get absolute path to the test_data directory\n",
    "current_file = Path(os.path.abspath(\"\"))  # Current notebook directory\n",
    "test_data_dir = (current_file.parent / \"tests\" / \"test_data\").resolve()\n",
    "\n",
    "# Find all files using glob\n",
    "file_paths = glob.glob(str(test_data_dir / \"*\"))\n",
    "print(f\"Found {len(file_paths)} files\")\n",
    "\n",
    "# Create ingestion service with default settings\n",
    "service = IngestionService()\n",
    "\n",
    "# Process all documents\n",
    "documents = service.ingest_documents(file_paths)\n",
    "\n",
    "# Show how many documents were extracted\n",
    "print(f\"Processed {len(documents)} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b50a3d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First document content (preview): Content from the zip file `test_docx.docx`:\n",
      "\n",
      "## File: [Content_Types].xml\n",
      "\n",
      "<?xml version=\"1.0\" encod...\n",
      "Metadata:\n",
      " {\n",
      "  \"source\": \"C:\\\\dev\\\\RAGdoll\\\\tests\\\\test_data\\\\test_docx.docx\",\n",
      "  \"file_name\": \"test_docx.docx\",\n",
      "  \"file_size\": 327119,\n",
      "  \"conversion_success\": true,\n",
      "  \"metadata_extraction_error\": \"No module named 'exceptions'\",\n",
      "  \"content_type\": \"document_full\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Access the first document\n",
    "if documents:\n",
    "    doc = documents[0]\n",
    "    print(f\"First document content (preview): {doc.page_content[:100]}...\")\n",
    "    print(f\"Metadata:\\n {json.dumps(doc.metadata, indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992a8122",
   "metadata": {},
   "source": [
    "## Working with different file types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 15:37:25,929 - WARNING - Module langchain_community.document_loaders does not have attribute RtfLoader for extension .rtf. Skipping this loader.\n",
      "2025-04-17 15:37:25,934 - INFO - Loaded 18 file extension loaders\n",
      "2025-04-17 15:37:25,939 - INFO - Cache initialized at C:\\Users\\PG518JW\\.ragdoll\\cache with TTL=86400s\n",
      "2025-04-17 15:37:25,939 - INFO - Initialized with 18 loaders, max_threads=10, use_cache=True, collect_metrics=False\n",
      "2025-04-17 15:37:25,945 - INFO - Starting ingestion of 1 inputs\n",
      "2025-04-17 15:37:25,950 - INFO - Processing batch 1 with 1 sources\n",
      "2025-04-17 15:37:28,266 - INFO - Finished ingestion, loaded 773 documents\n",
      "2025-04-17 15:37:28,266 - INFO - Starting ingestion of 2 inputs\n",
      "2025-04-17 15:37:28,266 - INFO - Processing batch 1 with 2 sources\n",
      "2025-04-17 15:37:28,289 - INFO - Finished ingestion, loaded 2 documents\n",
      "2025-04-17 15:37:28,296 - INFO - Starting ingestion of 1 inputs\n",
      "2025-04-17 15:37:28,299 - INFO - Processing batch 1 with 1 sources\n",
      "2025-04-17 15:37:31,758 - INFO - Finished ingestion, loaded 1 documents\n",
      "2025-04-17 15:37:31,758 - INFO - Starting ingestion of 1 inputs\n",
      "2025-04-17 15:37:31,758 - INFO - Processing batch 1 with 1 sources\n",
      "2025-04-17 15:37:31,789 - ERROR - Error retrieving from cache website:https://github.com/nsasto/langchain-markitdown: Expecting value: line 6 column 5 (char 152)\n",
      "2025-04-17 15:37:34,057 - ERROR - Error caching website:https://github.com/nsasto/langchain-markitdown: 'CacheManager' object has no attribute '_get_iso_timestamp'\n",
      "2025-04-17 15:37:34,059 - INFO - Finished ingestion, loaded 1 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 777\n",
      "Documents by type:\n",
      "  - PDF: 773\n",
      "  - Text: 2\n",
      "  - DOCX: 1\n",
      "  - Web: 1\n"
     ]
    }
   ],
   "source": [
    "from ragdoll.ingestion.ingestion_service import IngestionService\n",
    "\n",
    "# Initialize service\n",
    "service = IngestionService()\n",
    "\n",
    "# Process files of different types\n",
    "pdf_docs = service.ingest_documents([\"../tests/test_data/test_pdf.pdf\"])\n",
    "text_docs = service.ingest_documents([\"../tests/test_data/test_txt.txt\", \"../tests/test_data/test_txt.txt\"])\n",
    "docx_docs = service.ingest_documents([\"../tests/test_data/test_docx.docx\"])\n",
    "\n",
    "# Process HTML from URLs\n",
    "web_docs = service.ingest_documents([\"https://github.com/nsasto/langchain-markitdown\"])\n",
    "\n",
    "# Combine all documents\n",
    "all_docs = pdf_docs + text_docs + docx_docs + web_docs\n",
    "\n",
    "print(f\"Total documents: {len(all_docs)}\")\n",
    "print(f\"Documents by type:\")\n",
    "print(f\"  - PDF: {len(pdf_docs)}\")\n",
    "print(f\"  - Text: {len(text_docs)}\")\n",
    "print(f\"  - DOCX: {len(docx_docs)}\")\n",
    "print(f\"  - Web: {len(web_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11740b7e",
   "metadata": {},
   "source": [
    "## Customizing Ingestion Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06757cd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "IngestionService.__init__() got an unexpected keyword argument 'chunk_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragdoll\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mingestion\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mingestion_service\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IngestionService\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize with custom settings\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m service \u001b[38;5;241m=\u001b[39m \u001b[43mIngestionService\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# Limit concurrency\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# Process files in batches of 10\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# Enable caching\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollect_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# Enable metrics collection\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# Set document chunk size\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_overlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# Set overlap between chunks\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Process documents\u001b[39;00m\n\u001b[0;32m     14\u001b[0m documents \u001b[38;5;241m=\u001b[39m service\u001b[38;5;241m.\u001b[39mingest_documents([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/large_corpus/*\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: IngestionService.__init__() got an unexpected keyword argument 'chunk_size'"
     ]
    }
   ],
   "source": [
    "from ragdoll.ingestion.ingestion_service import IngestionService\n",
    "\n",
    "# Initialize with custom settings\n",
    "service = IngestionService(\n",
    "    max_threads=4,                # Limit concurrency\n",
    "    batch_size=10,                # Process files in batches of 10\n",
    "    use_cache=True,               # Enable caching\n",
    "    collect_metrics=True,         # Enable metrics collection\n",
    "    chunk_size=500,               # Set document chunk size\n",
    "    chunk_overlap=50              # Set overlap between chunks\n",
    ")\n",
    "\n",
    "# Process documents\n",
    "documents = service.ingest_documents([\"data/large_corpus/*\"])\n",
    "\n",
    "print(f\"Processed {len(documents)} document chunks\")\n",
    "\n",
    "# Documents are automatically chunked according to chunk_size\n",
    "if documents:\n",
    "    print(f\"First chunk size: {len(documents[0]['page_content'])} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db81bcb",
   "metadata": {},
   "source": [
    "## Working with Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bc5244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragdoll.ingestion.ingestion_service import IngestionService\n",
    "import time\n",
    "\n",
    "# Run with caching disabled first\n",
    "print(\"First run (no cache):\")\n",
    "start = time.time()\n",
    "service_no_cache = IngestionService(use_cache=False)\n",
    "docs1 = service_no_cache.ingest_documents([\"documents/large_file.pdf\"])\n",
    "print(f\"Processed {len(docs1)} documents in {time.time() - start:.2f} seconds\")\n",
    "\n",
    "# Run again with caching enabled\n",
    "print(\"\\nSecond run (with cache):\")\n",
    "start = time.time()\n",
    "service_with_cache = IngestionService(use_cache=True, cache_ttl=3600)  # 1 hour TTL\n",
    "docs2 = service_with_cache.ingest_documents([\"documents/large_file.pdf\"])\n",
    "print(f\"Processed {len(docs2)} documents in {time.time() - start:.2f} seconds\")\n",
    "\n",
    "# Clear the cache if needed\n",
    "service_with_cache.clear_cache()\n",
    "print(\"\\nCache cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530e15e5",
   "metadata": {},
   "source": [
    "## Handling Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c071a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragdoll.ingestion.ingestion_service import IngestionService\n",
    "import logging\n",
    "\n",
    "# Configure logging to see warnings and errors\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Create service\n",
    "service = IngestionService()\n",
    "\n",
    "# Mix of valid and invalid files\n",
    "files = [\n",
    "    \"documents/valid.pdf\",\n",
    "    \"documents/corrupted.pdf\",\n",
    "    \"documents/nonexistent.txt\",\n",
    "    \"documents/valid.txt\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Service will skip files it can't process\n",
    "    documents = service.ingest_documents(files)\n",
    "    print(f\"Successfully processed {len(documents)} documents\")\n",
    "    \n",
    "    # Check how many files were actually processed\n",
    "    sources = set([doc['metadata'].get('source') for doc in documents if 'source' in doc['metadata']])\n",
    "    print(f\"Documents came from {len(sources)} source files\")\n",
    "    print(f\"Source files: {sources}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during ingestion: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deea98b7",
   "metadata": {},
   "source": [
    "## Logging metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09343461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace your current loading code with this\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from ragdoll.ingestion.ingestion_service import IngestionService\n",
    "\n",
    "# Get absolute path to the test_data directory\n",
    "current_file = Path(os.path.abspath(\"\"))  # Current notebook directory\n",
    "test_data_dir = (current_file.parent / \"tests\" / \"test_data\").resolve()\n",
    "\n",
    "# Instead of using Path.glob(), use the glob module which handles absolute paths\n",
    "file_paths = glob.glob(str(test_data_dir / \"*\"))\n",
    "print(f\"Found {len(file_paths)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f60ef0",
   "metadata": {},
   "source": [
    "### Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03f74be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create service\n",
    "service = IngestionService(collect_metrics=True)\n",
    "\n",
    "# Pass the actual file paths, not the glob pattern\n",
    "service.ingest_documents(file_paths)\n",
    "\n",
    "\n",
    "# Get metrics after running\n",
    "metrics = service.get_metrics(days=30)  # Get metrics from the last 30 days\n",
    "\n",
    "# Use the metrics data\n",
    "print(f\"Total documents processed: {metrics['aggregate']['total_documents']}\")\n",
    "print(f\"Average success rate: {metrics['aggregate']['avg_success_rate']:.2%}\")\n",
    "\n",
    "# Print metrics for each source type\n",
    "for source_type, type_metrics in metrics['aggregate']['by_source_type'].items():\n",
    "    print(f\"\\nMetrics for {source_type} sources:\")\n",
    "    print(f\"  Count: {type_metrics['count']}\")\n",
    "    print(f\"  Success rate: {type_metrics['success_rate']:.2%}\")\n",
    "    print(f\"  Average documents: {type_metrics['avg_documents']:.1f}\")\n",
    "    print(f\"  Average processing time: {type_metrics['avg_processing_time_ms']:.1f}ms\")\n",
    "\n",
    "# Get the most recent session details\n",
    "if metrics['recent_sessions']:\n",
    "    latest = metrics['recent_sessions'][0]\n",
    "    print(f\"\\nLatest session ({latest['session_id']}):\")\n",
    "    print(f\"  Time: {latest['timestamp_start']}\")\n",
    "    print(f\"  Documents: {latest['document_count']}\")\n",
    "    print(f\"  Duration: {latest['duration_seconds']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b5cb4d",
   "metadata": {},
   "source": [
    "### Direct Access\n",
    "\n",
    "You can also access metrics directly from the metrics directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d096cbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Default metrics location\n",
    "metrics_dir = Path.home() / \".ragdoll\" / \"metrics\"\n",
    "# Or custom location if you specified one\n",
    "# metrics_dir = Path(\"/path/to/your/metrics\")\n",
    "\n",
    "# List all session files\n",
    "session_files = list(metrics_dir.glob(\"session_*.json\"))\n",
    "# Sort by modification time (most recent first)\n",
    "session_files.sort(key=os.path.getmtime, reverse=True)\n",
    "\n",
    "# Read the most recent session\n",
    "if session_files:\n",
    "    with open(session_files[0], \"r\", encoding=\"utf-8\") as f:\n",
    "        latest_session = json.load(f)\n",
    "        \n",
    "    print(f\"Session ID: {latest_session['session_id']}\")\n",
    "    print(f\"Date: {latest_session['timestamp_start']}\")\n",
    "    print(f\"Documents processed: {latest_session['document_count']}\")\n",
    "    print(f\"Success rate: {latest_session['success_rate']:.2%}\")\n",
    "    \n",
    "    # Print details about each source\n",
    "    for source_id, source_data in latest_session[\"sources\"].items():\n",
    "        print(f\"\\nSource: {source_id}\")\n",
    "        print(f\"  Type: {source_data['source_type']}\")\n",
    "        print(f\"  Success: {source_data['success']}\")\n",
    "        print(f\"  Documents: {source_data['document_count']}\")\n",
    "        print(f\"  Processing time: {source_data['processing_time_ms']}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab4fa95",
   "metadata": {},
   "source": [
    "### Displaying Outputs\n",
    "\n",
    "Simple Metrics dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7ce8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook-friendly version of the dashboard script\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from ragdoll.metrics.metrics_manager import MetricsManager\n",
    "\n",
    "def print_section(title: str):\n",
    "    \"\"\"Print a section title.\"\"\"\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"  {title}\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "def print_session_summary(session: Dict[str, Any]):\n",
    "    \"\"\"Print a summary of a session.\"\"\"\n",
    "    start_time = datetime.fromisoformat(session[\"timestamp_start\"])\n",
    "    \n",
    "    print(f\"Session: {session['session_id']}\")\n",
    "    print(f\"  Date: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"  Duration: {session.get('duration_seconds', 0):.2f} seconds\")\n",
    "    print(f\"  Documents: {session['document_count']}\")\n",
    "    print(f\"  Sources: {session['success_count'] + session['failure_count']} \"\n",
    "          f\"({session['success_count']} successful, {session['failure_count']} failed)\")\n",
    "    print(f\"  Success rate: {session.get('success_rate', 0):.2%}\")\n",
    "\n",
    "def format_bytes(bytes_count: int) -> str:\n",
    "    \"\"\"Format bytes as human-readable size.\"\"\"\n",
    "    if bytes_count < 1024:\n",
    "        return f\"{bytes_count} B\"\n",
    "    elif bytes_count < 1024**2:\n",
    "        return f\"{bytes_count / 1024:.1f} KB\"\n",
    "    elif bytes_count < 1024**3:\n",
    "        return f\"{bytes_count / (1024**2):.1f} MB\"\n",
    "    else:\n",
    "        return f\"{bytes_count / (1024**3):.2f} GB\"\n",
    "\n",
    "# Initialize metrics manager with the path to your metrics directory\n",
    "metrics_dir = Path.home() / \".ragdoll\" / \"metrics\"\n",
    "metrics_manager = MetricsManager(metrics_dir=metrics_dir)\n",
    "\n",
    "# Show aggregate metrics and recent sessions\n",
    "print_section(\"RAGdoll Metrics Dashboard\")\n",
    "\n",
    "# Get aggregate metrics for the last 30 days\n",
    "days = 30\n",
    "try:\n",
    "    # Fix the date handling issue by using timedelta\n",
    "    from datetime import datetime, timedelta\n",
    "    \n",
    "    # Monkey patch the get_aggregate_metrics method to avoid date issues\n",
    "    def fixed_get_aggregate_metrics(self, days=30):\n",
    "        cutoff_date = datetime.now() - timedelta(days=days)\n",
    "        \n",
    "        aggregate = {\n",
    "            \"total_sessions\": 0,\n",
    "            \"total_documents\": 0,\n",
    "            \"total_sources\": 0,\n",
    "            \"successful_sources\": 0,\n",
    "            \"failed_sources\": 0,\n",
    "            \"avg_success_rate\": 0,\n",
    "            \"avg_documents_per_source\": 0,\n",
    "            \"avg_processing_time_ms\": 0,\n",
    "            \"by_source_type\": {}\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            json_files = list(self.metrics_dir.glob(\"session_*.json\"))\n",
    "            \n",
    "            # Process each session file\n",
    "            for file_path in json_files:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    session = json.load(f)\n",
    "                \n",
    "                # Skip if older than cutoff\n",
    "                session_date = datetime.fromisoformat(session.get(\"timestamp_start\", \"\"))\n",
    "                if session_date < cutoff_date:\n",
    "                    continue\n",
    "                \n",
    "                # Update aggregate metrics\n",
    "                aggregate[\"total_sessions\"] += 1\n",
    "                aggregate[\"total_documents\"] += session.get(\"document_count\", 0)\n",
    "                aggregate[\"total_sources\"] += session.get(\"success_count\", 0) + session.get(\"failure_count\", 0)\n",
    "                aggregate[\"successful_sources\"] += session.get(\"success_count\", 0)\n",
    "                aggregate[\"failed_sources\"] += session.get(\"failure_count\", 0)\n",
    "                \n",
    "                # Process by source type\n",
    "                for source_key, source_metrics in session.get(\"sources\", {}).items():\n",
    "                    source_type = source_metrics.get(\"source_type\", \"unknown\")\n",
    "                    \n",
    "                    if source_type not in aggregate[\"by_source_type\"]:\n",
    "                        aggregate[\"by_source_type\"][source_type] = {\n",
    "                            \"count\": 0,\n",
    "                            \"success_count\": 0,\n",
    "                            \"document_count\": 0,\n",
    "                            \"total_processing_time_ms\": 0\n",
    "                        }\n",
    "                    \n",
    "                    type_metrics = aggregate[\"by_source_type\"][source_type]\n",
    "                    type_metrics[\"count\"] += 1\n",
    "                    \n",
    "                    if source_metrics.get(\"success\", False):\n",
    "                        type_metrics[\"success_count\"] += 1\n",
    "                    \n",
    "                    type_metrics[\"document_count\"] += source_metrics.get(\"document_count\", 0)\n",
    "                    type_metrics[\"total_processing_time_ms\"] += source_metrics.get(\"processing_time_ms\", 0)\n",
    "            \n",
    "            # Calculate averages\n",
    "            if aggregate[\"total_sources\"] > 0:\n",
    "                aggregate[\"avg_success_rate\"] = aggregate[\"successful_sources\"] / aggregate[\"total_sources\"]\n",
    "                aggregate[\"avg_documents_per_source\"] = aggregate[\"total_documents\"] / aggregate[\"total_sources\"]\n",
    "                \n",
    "                total_time = 0\n",
    "                total_items = 0\n",
    "                for source_type, metrics in aggregate[\"by_source_type\"].items():\n",
    "                    total_time += metrics[\"total_processing_time_ms\"]\n",
    "                    total_items += metrics[\"count\"]\n",
    "                    \n",
    "                    # Calculate source type specific metrics\n",
    "                    if metrics[\"count\"] > 0:\n",
    "                        metrics[\"avg_processing_time_ms\"] = metrics[\"total_processing_time_ms\"] / metrics[\"count\"]\n",
    "                        metrics[\"avg_documents\"] = metrics[\"document_count\"] / metrics[\"count\"]\n",
    "                        metrics[\"success_rate\"] = metrics[\"success_count\"] / metrics[\"count\"]\n",
    "                \n",
    "                if total_items > 0:\n",
    "                    aggregate[\"avg_processing_time_ms\"] = total_time / total_items\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating aggregate metrics: {e}\")\n",
    "            \n",
    "        return aggregate\n",
    "    \n",
    "    # Apply the monkey patch\n",
    "    from types import MethodType\n",
    "    metrics_manager.get_aggregate_metrics = MethodType(fixed_get_aggregate_metrics, metrics_manager)\n",
    "    \n",
    "    aggregate = metrics_manager.get_aggregate_metrics(days=days)\n",
    "    \n",
    "    print(f\"Showing metrics for the past {days} days:\")\n",
    "    print(f\"  Total sessions: {aggregate['total_sessions']}\")\n",
    "    print(f\"  Total documents: {aggregate['total_documents']}\")\n",
    "    print(f\"  Total sources: {aggregate['total_sources']}\")\n",
    "    print(f\"  Success rate: {aggregate['avg_success_rate']:.2%}\")\n",
    "    print(f\"  Avg processing time: {aggregate['avg_processing_time_ms']:.1f}ms per source\")\n",
    "    \n",
    "    # Show metrics by source type\n",
    "    print_section(\"Metrics by Source Type\")\n",
    "    for source_type, metrics in aggregate[\"by_source_type\"].items():\n",
    "        print(f\"\\n{source_type.upper()} Sources:\")\n",
    "        print(f\"  Count: {metrics['count']}\")\n",
    "        print(f\"  Success rate: {metrics.get('success_rate', 0):.2%}\")\n",
    "        print(f\"  Avg documents: {metrics.get('avg_documents', 0):.1f} per source\")\n",
    "        print(f\"  Avg processing time: {metrics.get('avg_processing_time_ms', 0):.1f}ms\")\n",
    "    \n",
    "    # Show recent sessions\n",
    "    recent_sessions = metrics_manager.get_recent_sessions(limit=5)\n",
    "    \n",
    "    print_section(\"Recent Sessions\")\n",
    "    for session in recent_sessions:\n",
    "        print(\"\")\n",
    "        print_session_summary(session)\n",
    "        \n",
    "    # Pick a specific session to view in detail\n",
    "    if recent_sessions:\n",
    "        session_id = recent_sessions[0][\"session_id\"]\n",
    "        print_section(f\"Detailed Session Report: {session_id}\")\n",
    "        \n",
    "        # Get the session data\n",
    "        session_path = Path(metrics_manager.metrics_dir) / f\"session_{session_id}.json\"\n",
    "        with open(session_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            session = json.load(f)\n",
    "        \n",
    "        print_session_summary(session)\n",
    "        \n",
    "        print(\"\\nSource Details:\")\n",
    "        for source_id, source_data in session[\"sources\"].items():\n",
    "            success = \"✅\" if source_data[\"success\"] else \"❌\"\n",
    "            error = f\" - Error: {source_data['error']}\" if source_data[\"error\"] else \"\"\n",
    "            \n",
    "            print(f\"\\n{success} {source_id} ({source_data['source_type']}){error}\")\n",
    "            print(f\"  Documents: {source_data['document_count']}\")\n",
    "            print(f\"  Size: {format_bytes(source_data['bytes'])}\")\n",
    "            print(f\"  Processing time: {source_data['processing_time_ms']}ms\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Error running dashboard: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f39e8c",
   "metadata": {},
   "source": [
    "### Export to file\n",
    "\n",
    "Export to other formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604f4380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Export to CSV\n",
    "def export_sessions_to_csv(metrics_manager, output_path):\n",
    "    sessions = metrics_manager.get_recent_sessions(limit=100)\n",
    "    \n",
    "    with open(output_path, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['session_id', 'timestamp', 'documents', 'sources', \n",
    "                      'success_rate', 'duration_seconds']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        writer.writeheader()\n",
    "        for session in sessions:\n",
    "            writer.writerow({\n",
    "                'session_id': session['session_id'],\n",
    "                'timestamp': session['timestamp_start'],\n",
    "                'documents': session['document_count'],\n",
    "                'sources': session['success_count'] + session['failure_count'],\n",
    "                'success_rate': session['success_rate'],\n",
    "                'duration_seconds': session['duration_seconds']\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a3c645",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

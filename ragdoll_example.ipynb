{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGdoll example\n",
    "\n",
    "@untrueaxioms\n",
    "\n",
    "<img src='img/github-header-image.png' />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragdoll.helpers import set_logger\n",
    "loginfo = set_logger(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    'log_level':logging.INFO\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragdoll.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in a Jupyter Notebook or JupyterLab environment.\n"
     ]
    }
   ],
   "source": [
    "from ragdoll.helpers import is_notebook\n",
    "from ragdoll.index import RagdollIndex\n",
    "\n",
    "index= RagdollIndex(config)\n",
    "check_notebook = is_notebook(print_output=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RagdollIndex class handles all the tasks outlined in the diagram below (see more at langchain's documentation)\n",
    "\n",
    "<img src='img/load_split_embed_store.png' height='500'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set question for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"tell me more about langchain\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[index] Fetching suggested search terms for the query\u001b[0m\n",
      "\u001b[32m[index] üß† Generating potential search queries with prompt:\n",
      " tell me more about langchain\u001b[0m\n",
      "\u001b[32m[models] ü§ñ retrieving gpt-3.5-turbo-16k model \u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[index] üß† Generated potential search queries: [\"langchain features and benefits\", \"langchain reviews and testimonials\"]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['langchain features and benefits', 'langchain reviews and testimonials']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_queries = index.get_suggested_search_terms(question)\n",
    "search_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[index]   üåê Searching with query langchain features and benefits...\u001b[0m\n",
      "\u001b[32m[index]   üåê Searching with query langchain reviews and testimonials...\u001b[0m\n",
      "\u001b[32m[__init__] file_cache is only supported with oauth2client<4.0.0\u001b[0m\n",
      "\u001b[32m[__init__] file_cache is only supported with oauth2client<4.0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results=index.get_search_results(search_queries)\n",
    "#can also access this via index.search_results or get the urls only with index.url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  * https://lakefs.io/blog/what-is-langchain-ml-architecture/\n",
      "  * https://aws.amazon.com/what-is/langchain/\n",
      "  * https://www.marktechpost.com/2023/12/14/what-is-langchain-use-cases-and-benefits/\n",
      "  * https://www.reddit.com/r/LangChain/comments/18eukhc/i_just_had_the_displeasure_of_implementing/\n",
      "  * https://news.ycombinator.com/item?id=36645575\n",
      "  * https://www.reddit.com/r/LangChain/comments/16bfaeo/codedog_a_pull_request_review_tool/\n"
     ]
    }
   ],
   "source": [
    "urllist = f\"\".join(f\"\\n  * {d['href']}\" for i, d in enumerate(results))\n",
    "print(urllist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[index] üåê Fetching raw source content\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "extracted 6 sites\n",
      "----------------------------------------------------------------------------------------------------\n",
      "https://lakefs.io/blog/what-is-langchain-ml-architecture/ \n",
      "\n",
      " Skip to content\n",
      "Docs\n",
      "Quickstart Use Cases\n",
      "Product\n",
      "lakeFS Cloud lakeFS Enterprise Pricing Contact Sales\n",
      "Developers\n",
      "Community Slack\n",
      "Resources\n",
      "Blog Guides Webinars Data Version Control Data Quality\n",
      "GitHub Book a demo Try without installing\n",
      "Menu\n",
      "Docs\n",
      "Quickstart Use Cases\n",
      "Product\n",
      "lakeFS Cloud lakeFS Enterprise Pricing Contact Sales\n",
      "Developers\n",
      "Community Slack\n",
      "Resources\n",
      "Blog Guides Webinars Data Version Control Data Quality\n",
      "GitHub Book a demo Try without installing\n",
      "Add Your Heading Text Here\n",
      "Add Your H\n"
     ]
    }
   ],
   "source": [
    "documents = index.get_scraped_content()\n",
    "print(\"-\" * 100)\n",
    "print(f\"extracted {len(documents)} sites\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "print(documents[0].metadata['source'],'\\n\\n',documents[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split\n",
    "\n",
    "Document Splitting is required to split documents into smaller chunks. Document splitting happens after we load data into standardised document format but before it goes into the vector store.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default RecursiveSplitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n",
    "\n",
    "How the text is split: by list of characters.\n",
    "How the chunk size is measured: by number of characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[index] üì∞ Chunking document\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "extracted 780 documents from 6 documents\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "split_docs = index.get_split_documents(documents)\n",
    "print(\"-\" * 100)\n",
    "print(f\"extracted {len(split_docs)} documents from {len(documents)} documents\")\n",
    "print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline \n",
    "\n",
    "we can also run all in one like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[index] Running index pipeline\u001b[0m\n",
      "\u001b[32m[index] Fetching suggested search terms for the query\u001b[0m\n",
      "\u001b[32m[index] üß† Generating potential search queries with prompt:\n",
      " tell me more about langchain\u001b[0m\n",
      "\u001b[32m[models] ü§ñ retrieving gpt-3.5-turbo-16k model \u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[index] üß† Generated potential search queries: [\"langchain features and benefits\", \"langchain reviews and testimonials\"]\u001b[0m\n",
      "\u001b[32m[index]   üåê Searching with query langchain features and benefits...\u001b[0m\n",
      "\u001b[32m[index]   üåê Searching with query langchain reviews and testimonials...\u001b[0m\n",
      "\u001b[32m[__init__] file_cache is only supported with oauth2client<4.0.0\u001b[0m\n",
      "\u001b[32m[__init__] file_cache is only supported with oauth2client<4.0.0\u001b[0m\n",
      "\u001b[32m[index] üåê Fetching raw source content\u001b[0m\n",
      "\u001b[32m[index] üì∞ Chunking document\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "extracted 780 documents from 6 documents\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "split_docs = index.run_index_pipeline(question)\n",
    "print(\"-\" * 100)\n",
    "print(f\"extracted {len(split_docs)} documents from {len(documents)} documents\")\n",
    "print(\"-\" * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval\n",
    "\n",
    "The retrieval class handles the following activities:\n",
    "\n",
    "\n",
    "<img src='img/retrieve_augment_prompt.png' height='500'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed and Store\n",
    "\n",
    "Let‚Äôs start by initializing a simple vector store retriever and storing our docs (in chunks).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragdoll.retriever import RagdollRetriever\n",
    "\n",
    "ragdoll = RagdollRetriever(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic retrieval\n",
    "\n",
    "let's create a vector store from the split_docs and then query it using similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #uncomment this code if you want to test with a local doc.\n",
    "\n",
    "# from langchain.docstore.document import Document\n",
    "\n",
    "# split_docs = [\n",
    "#     Document(page_content=\"LangChain is a framework designed to simplify the creation of applications using large language models. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\", metadata={'source': 'wikipedia'})\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[retriever] üóÉÔ∏è  creating vector database (FAISS)...\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[loader] Loading faiss with AVX2 support.\u001b[0m\n",
      "\u001b[32m[loader] Could not load library with AVX2 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\")\u001b[0m\n",
      "\u001b[32m[loader] Loading faiss.\u001b[0m\n",
      "\u001b[32m[loader] Successfully loaded faiss.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "db = ragdoll.get_db(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "The similarity store returned 4 relevant documents. below is a snippet:\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "\n",
      "Source: https://aws.amazon.com/what-is/langchain/\n",
      "Title: What is LangChain? - LangChain Explained - AWS\n",
      "Content: What is LangChain?\n",
      "LangChain is an open source framework for building applications based on large language models (LLMs). LLMs are large deep-learning models pre-trained on large amounts of data that can generate responses to user queries‚Äîfor example, answering questions or creating images from text-based prompts. LangChain provides tools and abstractions to improve the customization,\n"
     ]
    }
   ],
   "source": [
    "from ragdoll.helpers import pretty_print_docs\n",
    "\n",
    "simdocs = db.similarity_search(question)\n",
    "\n",
    "print(\"-\" * 100)\n",
    "print(f\"The similarity store returned {len(simdocs)} relevant documents. below is a snippet:\")\n",
    "print(\"-\" * 100, \"\\n\\n\")\n",
    "print(pretty_print_docs(simdocs, for_llm=False)[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now utilise a langchain retriever based on our selected vector db. A langchain retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[retriever] üìã getting retriever\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "retriever = ragdoll.get_retriever() \n",
    "# we can do this because the vector db has already been created\n",
    "#if we havent run get_db yet, we can simply create the retriever with ragdoll.get_retriever(documents=split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\dev\\RAGdoll\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "The retriever returned 4 relevant documents. below is a snippet:\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "\n",
      "Source: https://aws.amazon.com/what-is/langchain/\n",
      "Title: What is LangChain? - LangChain Explained - AWS\n",
      "Content: What is LangChain?\n",
      "LangChain is an open source framework for building applications based on large language models (LLMs). LLMs are large deep-learning models pre-trained on large amounts of data that can generate responses to user queries‚Äîfor example, answering questions or creating images from text-based prompts. LangChain provides tools and abstractions to improve the customization,\n"
     ]
    }
   ],
   "source": [
    "simdocs = retriever.get_relevant_documents(question)\n",
    "print(\"-\" * 100)\n",
    "print(f\"The retriever returned {len(simdocs)} relevant documents. below is a snippet:\")\n",
    "print(\"-\" * 100, \"\\n\\n\")\n",
    "print(pretty_print_docs(simdocs, for_llm=False)[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiquery retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on ‚Äúdistance‚Äù. But, retrieval may produce different results with subtle changes in query wording or if the embeddings do not capture the semantics of the data well. With multiple queries, we are more likely get more results back from the database. The aim of multi-query is to have an expanded results sets which might be able to answer questions better than docs from a single query. These results will be deduplicated (in case the same document comes back multiple times) and then used as context in your final prompt. The MultiQueryRetriever class takes care of this, and can be selected by setting the `base_retriever` key in the config dictionary to `MULTI_QUERY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[retriever] üìã getting multi query retriever\u001b[0m\n",
      "\u001b[32m[retriever] üí≠ Remember that the multi query retriever will incur additional calls to your LLM\u001b[0m\n",
      "\u001b[32m[models] ü§ñ retrieving gpt-3.5-turbo-16k model for multi query retriever\u001b[0m\n",
      "c:\\dev\\RAGdoll\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "mq_retriever = ragdoll.get_mq_retriever() \n",
    "# we can do this because the vector db has already been created\n",
    "#if we havent run get_db yet, we can simply create the retriever with ragdoll.get_retriever(documents=split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "The retriever returned 8 relevant documents. below is a snippet:\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "\n",
      "Source: https://lakefs.io/blog/what-is-langchain-ml-architecture/\n",
      "Title: What Is LangChain: Components, Benefits & How to Get Started\n",
      "Content: works LangChain was developed in Python and JavaScript, and it supports a wide range of language models such as GPT3, Hugging Face, Jurassic-1 Jumbo, and others. To start using LangChain, you must first create a language model. This means either taking advantage of a publicly available language model, such as GPT3, or training your own model. Once complet\n"
     ]
    }
   ],
   "source": [
    "simdocs = mq_retriever.get_relevant_documents(\"what is langchain\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"The retriever returned {len(simdocs)} relevant documents. below is a snippet:\")\n",
    "print(\"-\" * 100, \"\\n\\n\")\n",
    "print(pretty_print_docs(simdocs, for_llm=False)[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Compression Retriever\n",
    "\n",
    "One challenge with retrieval is that usually you don‚Äôt know the specific queries your document storage system will face when you ingest data into the system. This means that the information most relevant to a query may be buried in a document with a lot of irrelevant text. Passing that full document through your application can lead to more expensive LLM calls and poorer responses.\n",
    "\n",
    "Contextual compression is meant to fix this. The idea is simple: instead of immediately returning retrieved documents as-is, you can compress them using the context of the given query, so that only the relevant information is returned. ‚ÄúCompressing‚Äù here refers to both compressing the contents of an individual document and filtering out documents wholesale.\n",
    "\n",
    "To use the Contextual Compression Retriever, you‚Äôll need: - a base retriever (either the standard or multi query) - and a Document Compressor\n",
    "\n",
    "The Contextual Compression Retriever passes queries to the base retriever, takes the initial documents and passes them through the Document Compressor. The Document Compressor takes a list of documents and shortens it by reducing the contents of documents or dropping documents altogether\n",
    "\n",
    "We could do this with recursive calls to an LLM but this is expensive and slow. The EmbeddingsFilter provides a cheaper and faster option by embedding the documents and query and only returning those documents which have sufficiently similar embeddings to the query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccfg={\n",
    "        \"use_embeddings_filter\":True, \n",
    "        \"use_splitter\":True, \n",
    "        \"use_redundant_filter\":True, \n",
    "        \"use_relevant_filter\":True,\n",
    "        \"similarity_threshold\":0.5, #embeddings filter settings\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[retriever] üóúÔ∏è Compression object pipeline: embeddings_filter ‚û§ splitter ‚û§ redundant_filter ‚û§ relevant_filter\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "cc_retriever = ragdoll.get_compression_retriever(retriever, ccfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "The retriever returned 5 relevant documents. below is a snippet:\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "\n",
      "Source: https://aws.amazon.com/what-is/langchain/\n",
      "Title: What is LangChain? - LangChain Explained - AWS\n",
      "Content: What is LangChain?\n",
      "LangChain is an open source framework for building applications based on large language models (LLMs). LLMs are large deep-learning models pre-trained on large amounts of data that can generate responses to user queries‚Äîfor example, answering questions or creating images from text-based prompts. LangChain provides tools and abstractions to improve the customization,\n"
     ]
    }
   ],
   "source": [
    "simdocs = cc_retriever.get_relevant_documents(question)\n",
    "print(\"-\" * 100)\n",
    "print(f\"The retriever returned {len(simdocs)} relevant documents. below is a snippet:\")\n",
    "print(\"-\" * 100, \"\\n\\n\")\n",
    "print(pretty_print_docs(simdocs, for_llm=False)[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[retriever] üîó Running RAG chain\u001b[0m\n",
      "\u001b[32m[models] ü§ñ retrieving gpt-3.5-turbo-16k model for RAG chain\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is an open-source framework that provides developers with the tools they need to build applications using large language models (LLMs). LLMs are deep-learning models that have been pre-trained on vast amounts of data and can generate responses to user queries, such as answering questions or creating images from text-based prompts.\n",
      "\n",
      "One of the key features of LangChain is its ability to improve the customization, accuracy, and relevancy of the information generated by LLMs. It achieves this by providing tools and abstractions that allow developers to create new prompt chains or customize existing templates. This flexibility enables teams to connect various prompts and orchestrate them effectively.\n",
      "\n",
      "LangChain also simplifies the process of arranging large volumes of data so that LLMs can quickly access it. This is particularly useful for developers who want to create dynamic, data-responsive applications. By leveraging LangChain, developers can ensure that LLMs have access to the most up-to-date data available online, enabling them to generate accurate and relevant responses.\n",
      "\n",
      "The framework has already been used to create advanced AI chatbots, generative question-answering (GQA) systems, and language summarization tools. These applications demonstrate the versatility and potential of LangChain in various domains.\n",
      "\n",
      "In summary, LangChain is an open-source framework that empowers developers to build applications using large language models. It enhances customization, accuracy, and relevancy by providing tools and abstractions for prompt orchestration. Additionally, it simplifies data arrangement, allowing LLMs to access the most up-to-date information. The framework has already been utilized to create advanced AI applications, showcasing its potential in the field of natural language processing.\n"
     ]
    }
   ],
   "source": [
    "response = ragdoll.answer_me_this(question, cc_retriever)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

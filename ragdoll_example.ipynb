{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGdoll example\n",
    "\n",
    "@untrueaxioms\n",
    "\n",
    "<img src='img/github-header-image.png' />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragdoll.helpers import set_logger\n",
    "loginfo = set_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in a Jupyter Notebook or JupyterLab environment.\n"
     ]
    }
   ],
   "source": [
    "from ragdoll.helpers import is_notebook\n",
    "from ragdoll.index import RagdollIndex\n",
    "\n",
    "index= RagdollIndex({'log_level':logging.INFO})\n",
    "check_notebook = is_notebook(print_output=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RagdollIndex class handles all the tasks outlined in the diagram below (see more at langchain's documentation)\n",
    "\n",
    "<img src='img/load_split_embed_store.png' height='500'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload():\n",
    "    import importlib\n",
    "\n",
    "    ragdoll_index_module = importlib.import_module(\"ragdoll.index\")  # Assuming the module exists\n",
    "    importlib.reload(ragdoll_index_module)\n",
    "    index= RagdollIndex({'log_level':logging.WARN})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set question for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"tell me more about langchain\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[index] Fetching suggested search terms for the query\u001b[0m\n",
      "\u001b[32m[index] üë®‚Äçüíª Generating potential search queries with prompt:\n",
      " tell me more about langchain\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[index] üë®‚Äçüíª Generated potential search queries: [\"langchain features and benefits\", \"langchain reviews and testimonials\"]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['langchain features and benefits', 'langchain reviews and testimonials']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_queries = index.get_suggested_search_terms(question)\n",
    "search_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[index]   üåê Searching with query langchain features and benefits...\u001b[0m\n",
      "\u001b[32m[index]   üåê Searching with query langchain reviews and testimonials...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results=index.get_search_results(search_queries)\n",
    "#can also access this via index.search_results or get the urls only with index.url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  * https://www.marktechpost.com/2023/12/14/what-is-langchain-use-cases-and-benefits/\n",
      "  * https://lakefs.io/blog/what-is-langchain-ml-architecture/\n",
      "  * https://logankilpatrick.medium.com/what-is-langchain-and-why-should-i-care-as-a-developer-b2d952c42b28\n",
      "  * https://news.ycombinator.com/item?id=36645575\n",
      "  * https://www.reddit.com/r/aipromptprogramming/comments/13f8gjr/review_langchain_vs_huggingfaces_new_agent_system/\n",
      "  * https://github.com/hwchase17/langchain/issues/4772\n"
     ]
    }
   ],
   "source": [
    "urllist = f\"\".join(f\"\\n  * {d['href']}\" for i, d in enumerate(results))\n",
    "print(urllist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[index] Fetching content URLs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "extracted 6 sites\n",
      "----------------------------------------------------------------------------------------------------\n",
      "https://www.marktechpost.com/2023/12/14/what-is-langchain-use-cases-and-benefits/ \n",
      "\n",
      " What is LangChain? Use Cases and Benefits\n",
      "LangChain is an artificial intelligence framework designed for programmers to develop applications using large language models. It allows you to facilitate the creation of applications that consist of two key features:\n",
      "1. Context-Awareness: LangChain enables applications to be context-aware by establishing connections between a language model and various context sources. These sources may include prompt instructions, few-shot examples, or other content t\n"
     ]
    }
   ],
   "source": [
    "documents = index.get_scraped_content()\n",
    "print(\"-\" * 100)\n",
    "print(f\"extracted {len(documents)} sites\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "print(documents[0].metadata['source'],'\\n\\n',documents[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split\n",
    "\n",
    "Document Splitting is required to split documents into smaller chunks. Document splitting happens after we load data into standardised document format but before it goes into the vector store.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default RecursiveSplitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n",
    "\n",
    "How the text is split: by list of characters.\n",
    "How the chunk size is measured: by number of characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[index] Chunking document\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "extracted 97 documents from 6 documents\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "split_docs = index.get_split_documents(documents)\n",
    "print(\"-\" * 100)\n",
    "print(f\"extracted {len(split_docs)} documents from {len(documents)} documents\")\n",
    "print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline \n",
    "\n",
    "we can also run all in one like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[index] Running index pipeline\u001b[0m\n",
      "\u001b[32m[index] Fetching suggested search terms for the query\u001b[0m\n",
      "\u001b[32m[index] üë®‚Äçüíª Generating potential search queries with prompt:\n",
      " tell me more about langchain\u001b[0m\n",
      "\u001b[32m[index] üë®‚Äçüíª Generated potential search queries: [\"langchain features and benefits\", \"langchain reviews and testimonials\"]\u001b[0m\n",
      "\u001b[32m[index]   üåê Searching with query langchain features and benefits...\u001b[0m\n",
      "\u001b[32m[index]   üåê Searching with query langchain reviews and testimonials...\u001b[0m\n",
      "\u001b[32m[index] Fetching content URLs\u001b[0m\n",
      "\u001b[32m[index] Chunking document\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "extracted 97 documents from 6 documents\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "split_docs = index.run_index_pipeline(question)\n",
    "print(\"-\" * 100)\n",
    "print(f\"extracted {len(split_docs)} documents from {len(documents)} documents\")\n",
    "print(\"-\" * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval\n",
    "\n",
    "The retrieval class handles the following activities:\n",
    "\n",
    "\n",
    "<img src='img/retrieve_augment_prompt.png' height='500'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed and Store\n",
    "\n",
    "Let‚Äôs start by initializing a simple vector store retriever and storing our docs (in chunks).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragdoll.retriever import RagdollRetriever\n",
    "\n",
    "ragdoll = RagdollRetriever(config={'log_level':logging.WARN})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic retrieval\n",
    "\n",
    "let's create a vector store from the split_docs and then query it using similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #uncomment this code if you want to test with a local doc.\n",
    "\n",
    "# from langchain.docstore.document import Document\n",
    "\n",
    "# split_docs = [\n",
    "#     Document(page_content=\"LangChain is a framework designed to simplify the creation of applications using large language models. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\", metadata={'source': 'wikipedia'})\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = ragdoll.get_db(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "The similarity store returned 4 relevant documents. below is a snippet:\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "\n",
      "Source: https://www.marktechpost.com/2023/12/14/what-is-langchain-use-cases-and-benefits/\n",
      "Title: What is LangChain? Use Cases and Benefits - MarkTechPost\n",
      "Content: What is LangChain? Use Cases and Benefits\n",
      "LangChain is an artificial intelligence framework designed for programmers to develop applications using large language models. It allows you to facilitate the creation of applications that consist of two key features:\n",
      "1. Context-Awareness: LangChain enables applications to be context-aware by \n"
     ]
    }
   ],
   "source": [
    "from ragdoll.helpers import pretty_print_docs\n",
    "\n",
    "simdocs = db.similarity_search('tell me about langchain')\n",
    "\n",
    "print(\"-\" * 100)\n",
    "print(f\"The similarity store returned {len(simdocs)} relevant documents. below is a snippet:\")\n",
    "print(\"-\" * 100, \"\\n\\n\")\n",
    "print(pretty_print_docs(simdocs, for_llm=False)[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now utilise a langchain retriever based on our selected vector db. A langchain retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = ragdoll.get_retriever() \n",
    "# we can do this because the vector db has already been created\n",
    "#if we havent run get_db yet, we can simply create the retriever with ragdoll.get_retriever(documents=split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "The retriever returned 4 relevant documents. below is a snippet:\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "\n",
      "Source: https://www.marktechpost.com/2023/12/14/what-is-langchain-use-cases-and-benefits/\n",
      "Title: What is LangChain? Use Cases and Benefits - MarkTechPost\n",
      "Content: What is LangChain? Use Cases and Benefits\n",
      "LangChain is an artificial intelligence framework designed for programmers to develop applications using large language models. It allows you to facilitate the creation of applications that consist of two key features:\n",
      "1. Context-Awareness: LangChain enables applications to be context-aware by \n"
     ]
    }
   ],
   "source": [
    "simdocs = retriever.get_relevant_documents(\"what is langchain\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"The retriever returned {len(simdocs)} relevant documents. below is a snippet:\")\n",
    "print(\"-\" * 100, \"\\n\\n\")\n",
    "print(pretty_print_docs(simdocs, for_llm=False)[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiquery retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on ‚Äúdistance‚Äù. But, retrieval may produce different results with subtle changes in query wording or if the embeddings do not capture the semantics of the data well. With multiple queries, we are more likely get more results back from the database. The aim of multi-query is to have an expanded results sets which might be able to answer questions better than docs from a single query. These results will be deduplicated (in case the same document comes back multiple times) and then used as context in your final prompt. The MultiQueryRetriever class takes care of this, and can be selected by setting the `base_retriever` key in the config dictionary to `MULTI_QUERY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mq_retriever = ragdoll.get_mq_retriever() \n",
    "# we can do this because the vector db has already been created\n",
    "#if we havent run get_db yet, we can simply create the retriever with ragdoll.get_retriever(documents=split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "The retriever returned 6 relevant documents. below is a snippet:\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "\n",
      "Source: https://www.marktechpost.com/2023/12/14/what-is-langchain-use-cases-and-benefits/\n",
      "Title: What is LangChain? Use Cases and Benefits - MarkTechPost\n",
      "Content: LangChain is super handy because it allows us to use language models to work with databases without manually writing complex SQL queries. It‚Äôs like having a conversation with the database, making it easier to get the information we need. This feature opens up possibilities for creating chatbots that can answer questions based on databa\n"
     ]
    }
   ],
   "source": [
    "simdocs = mq_retriever.get_relevant_documents(\"what is langchain\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"The retriever returned {len(simdocs)} relevant documents. below is a snippet:\")\n",
    "print(\"-\" * 100, \"\\n\\n\")\n",
    "print(pretty_print_docs(simdocs, for_llm=False)[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Compression Retriever\n",
    "\n",
    "One challenge with retrieval is that usually you don‚Äôt know the specific queries your document storage system will face when you ingest data into the system. This means that the information most relevant to a query may be buried in a document with a lot of irrelevant text. Passing that full document through your application can lead to more expensive LLM calls and poorer responses.\n",
    "\n",
    "Contextual compression is meant to fix this. The idea is simple: instead of immediately returning retrieved documents as-is, you can compress them using the context of the given query, so that only the relevant information is returned. ‚ÄúCompressing‚Äù here refers to both compressing the contents of an individual document and filtering out documents wholesale.\n",
    "\n",
    "To use the Contextual Compression Retriever, you‚Äôll need: - a base retriever (either the standard or multi query) - and a Document Compressor\n",
    "\n",
    "The Contextual Compression Retriever passes queries to the base retriever, takes the initial documents and passes them through the Document Compressor. The Document Compressor takes a list of documents and shortens it by reducing the contents of documents or dropping documents altogether\n",
    "\n",
    "We could do this with recursive calls to an LLM but this is expensive and slow. The EmbeddingsFilter provides a cheaper and faster option by embedding the documents and query and only returning those documents which have sufficiently similar embeddings to the query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_retriever = ragdoll.get_compression_retriever(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "The retriever returned 6 relevant documents. below is a snippet:\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "\n",
      "Source: https://www.marktechpost.com/2023/12/14/what-is-langchain-use-cases-and-benefits/\n",
      "Title: What is LangChain? Use Cases and Benefits - MarkTechPost\n",
      "Content: LangChain is super handy because it allows us to use language models to work with databases without manually writing complex SQL queries. It‚Äôs like having a conversation with the database, making it easier to get the information we need. This feature opens up possibilities for creating chatbots that can answer questions based on databa\n"
     ]
    }
   ],
   "source": [
    "simdocs = mq_retriever.get_relevant_documents(\"what is langchain\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"The retriever returned {len(simdocs)} relevant documents. below is a snippet:\")\n",
    "print(\"-\" * 100, \"\\n\\n\")\n",
    "print(pretty_print_docs(simdocs, for_llm=False)[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

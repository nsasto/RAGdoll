{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGdoll example\n",
    "\n",
    "@untrueaxioms\n",
    "\n",
    "<img src='img/github-header-image.png' />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragdoll.helpers import set_logger\n",
    "loginfo = set_logger(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    'log_level':logging.INFO\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragdoll.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in a Jupyter Notebook or JupyterLab environment.\n"
     ]
    }
   ],
   "source": [
    "from ragdoll.helpers import is_notebook\n",
    "from ragdoll.index import RagdollIndex\n",
    "\n",
    "index= RagdollIndex(config)\n",
    "check_notebook = is_notebook(print_output=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RagdollIndex class handles all the tasks outlined in the diagram below (see more at langchain's documentation)\n",
    "\n",
    "<img src='img/load_split_embed_store.png' height='500'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set question for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"tell me more about langchain\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[index] Fetching suggested search terms for the query\u001b[0m\n",
      "\u001b[32m[index] üß† Generating potential search queries with prompt:\n",
      " tell me more about langchain\u001b[0m\n",
      "\u001b[32m[models] ü§ñ retrieving gpt-3.5-turbo-16k model \u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[index] üß† Generated potential search queries: [\"What is the purpose of Langchain?\", \"Langchain reviews and user experiences\"]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['What is the purpose of Langchain?', 'Langchain reviews and user experiences']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_queries = index.get_suggested_search_terms(question)\n",
    "search_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[index]   üåê Searching with query What is the purpose of Langchain?...\u001b[0m\n",
      "\u001b[32m[index]   üåê Searching with query Langchain reviews and user experiences...\u001b[0m\n",
      "\u001b[32m[__init__] file_cache is only supported with oauth2client<4.0.0\u001b[0m\n",
      "\u001b[32m[__init__] file_cache is only supported with oauth2client<4.0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results=index.get_search_results(search_queries)\n",
    "#can also access this via index.search_results or get the urls only with index.url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  * https://aws.amazon.com/what-is/langchain/\n",
      "  * https://www.techtarget.com/searchenterpriseai/definition/LangChain\n",
      "  * https://www.reddit.com/r/LangChain/comments/12r5y1g/what_are_the_benefits_of_using_langchain_compared/\n",
      "  * https://www.reddit.com/r/LangChain/comments/18eukhc/i_just_had_the_displeasure_of_implementing/\n",
      "  * https://medium.com/llm-study-diary-a-beginners-path-through-ai/comprehensive-review-of-langchain-part-1-4734d61a49e1\n",
      "  * https://www.reddit.com/r/LangChain/comments/1bszde0/would_you_use_langchain_more_if_it_had_better/\n"
     ]
    }
   ],
   "source": [
    "urllist = f\"\".join(f\"\\n  * {d['href']}\" for i, d in enumerate(results))\n",
    "print(urllist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[index] üåê Fetching raw source content\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "extracted 6 sites\n",
      "----------------------------------------------------------------------------------------------------\n",
      "https://aws.amazon.com/what-is/langchain/ \n",
      "\n",
      " Click here to return to Amazon Web Services homepage\n",
      "About AWS\n",
      "Contact Us\n",
      "Support\n",
      "English\n",
      "My Account\n"
     ]
    }
   ],
   "source": [
    "documents = index.get_scraped_content()\n",
    "print(\"-\" * 100)\n",
    "print(f\"extracted {len(documents)} sites\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "print(documents[0].metadata['source'],'\\n\\n',documents[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split\n",
    "\n",
    "Document Splitting is required to split documents into smaller chunks. Document splitting happens after we load data into standardised document format but before it goes into the vector store.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default RecursiveSplitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n",
    "\n",
    "How the text is split: by list of characters.\n",
    "How the chunk size is measured: by number of characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[index] üì∞ Chunking document\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "extracted 481 documents from 6 documents\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "split_docs = index.get_split_documents(documents)\n",
    "print(\"-\" * 100)\n",
    "print(f\"extracted {len(split_docs)} documents from {len(documents)} documents\")\n",
    "print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline \n",
    "\n",
    "we can also run all in one like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[index] Running index pipeline\u001b[0m\n",
      "\u001b[32m[index] Fetching suggested search terms for the query\u001b[0m\n",
      "\u001b[32m[index] üß† Generating potential search queries with prompt:\n",
      " tell me more about langchain\u001b[0m\n",
      "\u001b[32m[models] ü§ñ retrieving gpt-3.5-turbo-16k model \u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[index] üß† Generated potential search queries: [\"What is the purpose of Langchain?\", \"Langchain reviews and user experiences\"]\u001b[0m\n",
      "\u001b[32m[index]   üåê Searching with query What is the purpose of Langchain?...\u001b[0m\n",
      "\u001b[32m[index]   üåê Searching with query Langchain reviews and user experiences...\u001b[0m\n",
      "\u001b[32m[__init__] file_cache is only supported with oauth2client<4.0.0\u001b[0m\n",
      "\u001b[32m[__init__] file_cache is only supported with oauth2client<4.0.0\u001b[0m\n",
      "\u001b[32m[index] üåê Fetching raw source content\u001b[0m\n",
      "\u001b[32m[index] üì∞ Chunking document\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "extracted 481 documents from 6 documents\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "split_docs = index.run_index_pipeline(question)\n",
    "print(\"-\" * 100)\n",
    "print(f\"extracted {len(split_docs)} documents from {len(documents)} documents\")\n",
    "print(\"-\" * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval\n",
    "\n",
    "The retrieval class handles the following activities:\n",
    "\n",
    "\n",
    "<img src='img/retrieve_augment_prompt.png' height='500'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed and Store\n",
    "\n",
    "Let‚Äôs start by initializing a simple vector store retriever and storing our docs (in chunks).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragdoll.retriever import RagdollRetriever\n",
    "\n",
    "ragdoll = RagdollRetriever(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic retrieval\n",
    "\n",
    "let's create a vector store from the split_docs and then query it using similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #uncomment this code if you want to test with a local doc.\n",
    "\n",
    "# from langchain.docstore.document import Document\n",
    "\n",
    "# split_docs = [\n",
    "#     Document(page_content=\"LangChain is a framework designed to simplify the creation of applications using large language models. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\", metadata={'source': 'wikipedia'})\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[retriever] üóÉÔ∏è  creating vector database (FAISS)...\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[loader] Loading faiss with AVX2 support.\u001b[0m\n",
      "\u001b[32m[loader] Could not load library with AVX2 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\")\u001b[0m\n",
      "\u001b[32m[loader] Loading faiss.\u001b[0m\n",
      "\u001b[32m[loader] Successfully loaded faiss.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "db = ragdoll.get_db(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "The similarity store returned 4 relevant documents. below is a snippet:\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "\n",
      "Source: https://aws.amazon.com/what-is/langchain/\n",
      "Title: What is LangChain? - LangChain Explained - AWS\n",
      "Content: What is LangChain?\n",
      "LangChain is an open source framework for building applications based on large language models (LLMs). LLMs are large deep-learning models pre-trained on large amounts of data that can generate responses to user queries‚Äîfor example, answering questions or creating images from text-based prompts. LangChain provides tools and abstractions to improve the customization,\n"
     ]
    }
   ],
   "source": [
    "from ragdoll.helpers import pretty_print_docs\n",
    "\n",
    "simdocs = db.similarity_search(question)\n",
    "\n",
    "print(\"-\" * 100)\n",
    "print(f\"The similarity store returned {len(simdocs)} relevant documents. below is a snippet:\")\n",
    "print(\"-\" * 100, \"\\n\\n\")\n",
    "print(pretty_print_docs(simdocs, for_llm=False)[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now utilise a langchain retriever based on our selected vector db. A langchain retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[retriever] üìã getting retriever\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "retriever = ragdoll.get_retriever() \n",
    "# we can do this because the vector db has already been created\n",
    "#if we havent run get_db yet, we can simply create the retriever with ragdoll.get_retriever(documents=split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "The retriever returned 4 relevant documents. below is a snippet:\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "\n",
      "Source: https://aws.amazon.com/what-is/langchain/\n",
      "Title: What is LangChain? - LangChain Explained - AWS\n",
      "Content: What is LangChain?\n",
      "LangChain is an open source framework for building applications based on large language models (LLMs). LLMs are large deep-learning models pre-trained on large amounts of data that can generate responses to user queries‚Äîfor example, answering questions or creating images from text-based prompts. LangChain provides tools and abstractions to improve the customization,\n"
     ]
    }
   ],
   "source": [
    "simdocs = retriever.invoke(question)\n",
    "print(\"-\" * 100)\n",
    "print(f\"The retriever returned {len(simdocs)} relevant documents. below is a snippet:\")\n",
    "print(\"-\" * 100, \"\\n\\n\")\n",
    "print(pretty_print_docs(simdocs, for_llm=False)[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiquery retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on ‚Äúdistance‚Äù. But, retrieval may produce different results with subtle changes in query wording or if the embeddings do not capture the semantics of the data well. With multiple queries, we are more likely get more results back from the database. The aim of multi-query is to have an expanded results sets which might be able to answer questions better than docs from a single query. These results will be deduplicated (in case the same document comes back multiple times) and then used as context in your final prompt. The MultiQueryRetriever class takes care of this, and can be selected by setting the `base_retriever` key in the config dictionary to `MULTI_QUERY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[retriever] üìã getting multi query retriever\u001b[0m\n",
      "\u001b[32m[retriever] üí≠ Remember that the multi query retriever will incur additional calls to your LLM\u001b[0m\n",
      "\u001b[32m[models] ü§ñ retrieving gpt-3.5-turbo-16k model for multi query retriever\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "mq_retriever = ragdoll.get_mq_retriever() \n",
    "# we can do this because the vector db has already been created\n",
    "#if we havent run get_db yet, we can simply create the retriever with ragdoll.get_retriever(documents=split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[multi_query] Generated queries: ['1. Can you provide information about the LangChain?', \"2. I'm curious to know more about LangChain. Can you explain it to me?\", '3. Could you please give me an overview of LangChain and its purpose?']\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "The retriever returned 4 relevant documents. below is a snippet:\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "\n",
      "Source: https://aws.amazon.com/what-is/langchain/\n",
      "Title: What is LangChain? - LangChain Explained - AWS\n",
      "Content: What is LangChain?\n",
      "LangChain is an open source framework for building applications based on large language models (LLMs). LLMs are large deep-learning models pre-trained on large amounts of data that can generate responses to user queries‚Äîfor example, answering questions or creating images from text-based prompts. LangChain provides tools and abstractions to improve the customization,\n"
     ]
    }
   ],
   "source": [
    "simdocs = mq_retriever.invoke(\"what is langchain\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"The retriever returned {len(simdocs)} relevant documents. below is a snippet:\")\n",
    "print(\"-\" * 100, \"\\n\\n\")\n",
    "print(pretty_print_docs(simdocs, for_llm=False)[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Compression Retriever\n",
    "\n",
    "One challenge with retrieval is that usually you don‚Äôt know the specific queries your document storage system will face when you ingest data into the system. This means that the information most relevant to a query may be buried in a document with a lot of irrelevant text. Passing that full document through your application can lead to more expensive LLM calls and poorer responses.\n",
    "\n",
    "Contextual compression is meant to fix this. The idea is simple: instead of immediately returning retrieved documents as-is, you can compress them using the context of the given query, so that only the relevant information is returned. ‚ÄúCompressing‚Äù here refers to both compressing the contents of an individual document and filtering out documents wholesale.\n",
    "\n",
    "To use the Contextual Compression Retriever, you‚Äôll need: - a base retriever (either the standard or multi query) - and a Document Compressor\n",
    "\n",
    "The Contextual Compression Retriever passes queries to the base retriever, takes the initial documents and passes them through the Document Compressor. The Document Compressor takes a list of documents and shortens it by reducing the contents of documents or dropping documents altogether\n",
    "\n",
    "We could do this with recursive calls to an LLM but this is expensive and slow. The EmbeddingsFilter provides a cheaper and faster option by embedding the documents and query and only returning those documents which have sufficiently similar embeddings to the query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccfg={\n",
    "        \"use_embeddings_filter\":True, \n",
    "        \"use_splitter\":True, \n",
    "        \"use_redundant_filter\":True, \n",
    "        \"use_relevant_filter\":True,\n",
    "        \"similarity_threshold\":0.5, #embeddings filter settings\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[retriever] üóúÔ∏è Compression object pipeline: embeddings_filter ‚û§ splitter ‚û§ redundant_filter ‚û§ relevant_filter\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "cc_retriever = ragdoll.get_compression_retriever(retriever, ccfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "The retriever returned 5 relevant documents. below is a snippet:\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "\n",
      "Source: https://aws.amazon.com/what-is/langchain/\n",
      "Title: What is LangChain? - LangChain Explained - AWS\n",
      "Content: What is LangChain?\n",
      "LangChain is an open source framework for building applications based on large language models (LLMs). LLMs are large deep-learning models pre-trained on large amounts of data that can generate responses to user queries‚Äîfor example, answering questions or creating images from text-based prompts. LangChain provides tools and abstractions to improve the customization,\n"
     ]
    }
   ],
   "source": [
    "simdocs = cc_retriever.get_relevant_documents(question)\n",
    "print(\"-\" * 100)\n",
    "print(f\"The retriever returned {len(simdocs)} relevant documents. below is a snippet:\")\n",
    "print(\"-\" * 100, \"\\n\\n\")\n",
    "print(pretty_print_docs(simdocs, for_llm=False)[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[retriever] üîó Running RAG chain\u001b[0m\n",
      "\u001b[32m[models] ü§ñ retrieving gpt-3.5-turbo-16k model for RAG chain\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is an open source framework that was launched in 2022 by co-founders Harrison Chase and Ankush Gola. It is designed to enable software developers working with artificial intelligence (AI) and machine learning to combine large language models (LLMs) with other external components to develop LLM-powered applications. LLMs are deep-learning models that have been pre-trained on large amounts of data and can generate responses to user queries, such as answering questions or creating images from text-based prompts.\n",
      "\n",
      "The main goal of LangChain is to link powerful LLMs, such as OpenAI's GPT-3.5 and GPT-4, to an array of external data sources in order to create and leverage the benefits of natural language processing (NLP) applications. This means that developers, software engineers, and data scientists with experience in Python, JavaScript, or TypeScript programming languages can utilize LangChain's packages offered in those languages.\n",
      "\n",
      "LangChain provides tools and abstractions that improve the customization, accuracy, and relevancy of the information generated by the LLMs. Developers can use LangChain components to build new prompt chains or customize existing templates. Additionally, LangChain includes components that allow LLMs to access new data sets without the need for retraining.\n",
      "\n",
      "By leveraging LangChain, developers can enhance the capabilities of their AI applications by incorporating LLMs and connecting them to external data sources. This enables the development of more advanced and accurate NLP applications that can understand and respond to user queries in a more human-like manner.\n",
      "\n",
      "It is important to note that LangChain is an open source framework, which means that it is freely available for developers to use and contribute to. This fosters collaboration and innovation within the AI and machine learning community, as developers can share their knowledge and expertise to improve the framework and create new applications.\n",
      "\n",
      "In summary, LangChain is an open source framework that allows developers to combine large language models with other external components to build powerful AI applications. It aims to link LLMs to external data sources in order to create and leverage the benefits of natural language processing. By using LangChain, developers can enhance the customization, accuracy, and relevancy of the information generated by LLMs, leading to more advanced and accurate NLP applications.\n"
     ]
    }
   ],
   "source": [
    "response = ragdoll.answer_me_this(question, cc_retriever)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

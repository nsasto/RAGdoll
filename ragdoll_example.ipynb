{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGdoll example\n",
    "\n",
    "@untrueaxioms\n",
    "\n",
    "<img src='img/github-header-image.png' />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragdoll.helpers import set_logger\n",
    "loginfo = set_logger(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    'log_level':logging.INFO\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragdoll.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in a Jupyter Notebook or JupyterLab environment.\n"
     ]
    }
   ],
   "source": [
    "from ragdoll.helpers import is_notebook\n",
    "from ragdoll.index import RagdollIndex\n",
    "\n",
    "index= RagdollIndex(config)\n",
    "check_notebook = is_notebook(print_output=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RagdollIndex class handles all the tasks outlined in the diagram below (see more at langchain's documentation)\n",
    "\n",
    "<img src='img/load_split_embed_store.png' height='500'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload():\n",
    "    import importlib\n",
    "\n",
    "    ragdoll_index_module = importlib.import_module(\"ragdoll.index\")  # Assuming the module exists\n",
    "    ragdoll_retriever_module = importlib.import_module(\"ragdoll.retriever\")  # Assuming the module exists\n",
    "    \n",
    "    importlib.reload(ragdoll_index_module)\n",
    "    importlib.reload(ragdoll_retriever_module)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set question for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"tell me more about langchain\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[index] Fetching suggested search terms for the query\u001b[0m\n",
      "\u001b[32m[index] üß† Generating potential search queries with prompt:\n",
      " tell me more about langchain\u001b[0m\n",
      "\u001b[32m[models] ü§ñ retrieving OpenAI model \u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[index] üß† Generated potential search queries: [\"langchain features and benefits\", \"langchain reviews and testimonials\"]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['langchain features and benefits', 'langchain reviews and testimonials']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_queries = index.get_suggested_search_terms(question)\n",
    "search_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[index]   üåê Searching with query langchain features and benefits...\u001b[0m\n",
      "\u001b[32m[index]   üåê Searching with query langchain reviews and testimonials...\u001b[0m\n",
      "\u001b[32m[__init__] file_cache is only supported with oauth2client<4.0.0\u001b[0m\n",
      "\u001b[32m[__init__] file_cache is only supported with oauth2client<4.0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results=index.get_search_results(search_queries)\n",
    "#can also access this via index.search_results or get the urls only with index.url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  * https://www.marktechpost.com/2023/12/14/what-is-langchain-use-cases-and-benefits/\n",
      "  * https://aws.amazon.com/what-is/langchain/\n",
      "  * https://lakefs.io/blog/what-is-langchain-ml-architecture/\n",
      "  * https://www.reddit.com/r/LangChain/comments/16bfaeo/codedog_a_pull_request_review_tool/\n",
      "  * https://news.ycombinator.com/item?id=36645575\n",
      "  * https://www.reddit.com/r/LangChain/comments/16hnasv/advice_for_langchain_architecture_location/\n"
     ]
    }
   ],
   "source": [
    "urllist = f\"\".join(f\"\\n  * {d['href']}\" for i, d in enumerate(results))\n",
    "print(urllist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[index] üåê Fetching raw source content\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "extracted 6 sites\n",
      "----------------------------------------------------------------------------------------------------\n",
      "https://www.marktechpost.com/2023/12/14/what-is-langchain-use-cases-and-benefits/ \n",
      "\n",
      " What is LangChain? Use Cases and Benefits\n",
      "LangChain is an artificial intelligence framework designed for programmers to develop applications using large language models. It allows you to facilitate the creation of applications that consist of two key features:\n",
      "1. Context-Awareness: LangChain enables applications to be context-aware by establishing connections between a language model and various context sources. These sources may include prompt instructions, few-shot examples, or other content t\n"
     ]
    }
   ],
   "source": [
    "documents = index.get_scraped_content()\n",
    "print(\"-\" * 100)\n",
    "print(f\"extracted {len(documents)} sites\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "print(documents[0].metadata['source'],'\\n\\n',documents[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split\n",
    "\n",
    "Document Splitting is required to split documents into smaller chunks. Document splitting happens after we load data into standardised document format but before it goes into the vector store.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default RecursiveSplitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n",
    "\n",
    "How the text is split: by list of characters.\n",
    "How the chunk size is measured: by number of characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[index] üì∞ Chunking document\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "extracted 88 documents from 6 documents\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "split_docs = index.get_split_documents(documents)\n",
    "print(\"-\" * 100)\n",
    "print(f\"extracted {len(split_docs)} documents from {len(documents)} documents\")\n",
    "print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline \n",
    "\n",
    "we can also run all in one like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[index] Running index pipeline\u001b[0m\n",
      "\u001b[32m[index] Fetching suggested search terms for the query\u001b[0m\n",
      "\u001b[32m[index] üß† Generating potential search queries with prompt:\n",
      " tell me more about langchain\u001b[0m\n",
      "\u001b[32m[models] ü§ñ retrieving OpenAI model \u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[index] üß† Generated potential search queries: [\"langchain features and benefits\", \"langchain reviews and testimonials\"]\u001b[0m\n",
      "\u001b[32m[index]   üåê Searching with query langchain features and benefits...\u001b[0m\n",
      "\u001b[32m[index]   üåê Searching with query langchain reviews and testimonials...\u001b[0m\n",
      "\u001b[32m[__init__] file_cache is only supported with oauth2client<4.0.0\u001b[0m\n",
      "\u001b[32m[__init__] file_cache is only supported with oauth2client<4.0.0\u001b[0m\n",
      "\u001b[32m[index] üåê Fetching raw source content\u001b[0m\n",
      "\u001b[32m[index] üì∞ Chunking document\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "extracted 88 documents from 6 documents\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "split_docs = index.run_index_pipeline(question)\n",
    "print(\"-\" * 100)\n",
    "print(f\"extracted {len(split_docs)} documents from {len(documents)} documents\")\n",
    "print(\"-\" * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval\n",
    "\n",
    "The retrieval class handles the following activities:\n",
    "\n",
    "\n",
    "<img src='img/retrieve_augment_prompt.png' height='500'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed and Store\n",
    "\n",
    "Let‚Äôs start by initializing a simple vector store retriever and storing our docs (in chunks).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragdoll.retriever import RagdollRetriever\n",
    "\n",
    "ragdoll = RagdollRetriever(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic retrieval\n",
    "\n",
    "let's create a vector store from the split_docs and then query it using similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #uncomment this code if you want to test with a local doc.\n",
    "\n",
    "# from langchain.docstore.document import Document\n",
    "\n",
    "# split_docs = [\n",
    "#     Document(page_content=\"LangChain is a framework designed to simplify the creation of applications using large language models. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\", metadata={'source': 'wikipedia'})\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[retriever] üóÉÔ∏è  creating vector database (FAISS)...\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[loader] Loading faiss with AVX2 support.\u001b[0m\n",
      "\u001b[32m[loader] Could not load library with AVX2 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\")\u001b[0m\n",
      "\u001b[32m[loader] Loading faiss.\u001b[0m\n",
      "\u001b[32m[loader] Successfully loaded faiss.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "db = ragdoll.get_db(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "The similarity store returned 4 relevant documents. below is a snippet:\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "\n",
      "Source: https://www.marktechpost.com/2023/12/14/what-is-langchain-use-cases-and-benefits/\n",
      "Title: What is LangChain? Use Cases and Benefits - MarkTechPost\n",
      "Content: What is LangChain? Use Cases and Benefits\n",
      "LangChain is an artificial intelligence framework designed for programmers to develop applications using large language models. It allows you to facilitate the creation of applications that consist of two key features:\n",
      "1. Context-Awareness: LangChain enables applications to be context-aware by \n"
     ]
    }
   ],
   "source": [
    "from ragdoll.helpers import pretty_print_docs\n",
    "\n",
    "simdocs = db.similarity_search(question)\n",
    "\n",
    "print(\"-\" * 100)\n",
    "print(f\"The similarity store returned {len(simdocs)} relevant documents. below is a snippet:\")\n",
    "print(\"-\" * 100, \"\\n\\n\")\n",
    "print(pretty_print_docs(simdocs, for_llm=False)[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now utilise a langchain retriever based on our selected vector db. A langchain retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[retriever] üìã getting retriever\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "retriever = ragdoll.get_retriever() \n",
    "# we can do this because the vector db has already been created\n",
    "#if we havent run get_db yet, we can simply create the retriever with ragdoll.get_retriever(documents=split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "The retriever returned 4 relevant documents. below is a snippet:\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "\n",
      "Source: https://www.marktechpost.com/2023/12/14/what-is-langchain-use-cases-and-benefits/\n",
      "Title: What is LangChain? Use Cases and Benefits - MarkTechPost\n",
      "Content: What is LangChain? Use Cases and Benefits\n",
      "LangChain is an artificial intelligence framework designed for programmers to develop applications using large language models. It allows you to facilitate the creation of applications that consist of two key features:\n",
      "1. Context-Awareness: LangChain enables applications to be context-aware by \n"
     ]
    }
   ],
   "source": [
    "simdocs = retriever.get_relevant_documents(question)\n",
    "print(\"-\" * 100)\n",
    "print(f\"The retriever returned {len(simdocs)} relevant documents. below is a snippet:\")\n",
    "print(\"-\" * 100, \"\\n\\n\")\n",
    "print(pretty_print_docs(simdocs, for_llm=False)[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiquery retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on ‚Äúdistance‚Äù. But, retrieval may produce different results with subtle changes in query wording or if the embeddings do not capture the semantics of the data well. With multiple queries, we are more likely get more results back from the database. The aim of multi-query is to have an expanded results sets which might be able to answer questions better than docs from a single query. These results will be deduplicated (in case the same document comes back multiple times) and then used as context in your final prompt. The MultiQueryRetriever class takes care of this, and can be selected by setting the `base_retriever` key in the config dictionary to `MULTI_QUERY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[retriever] üìã getting multi query retriever\u001b[0m\n",
      "\u001b[32m[retriever] üí≠ Remember that the multi query retriever will incur additional calls to your LLM\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "mq_retriever = ragdoll.get_mq_retriever() \n",
    "# we can do this because the vector db has already been created\n",
    "#if we havent run get_db yet, we can simply create the retriever with ragdoll.get_retriever(documents=split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "The retriever returned 6 relevant documents. below is a snippet:\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "\n",
      "Source: https://www.marktechpost.com/2023/12/14/what-is-langchain-use-cases-and-benefits/\n",
      "Title: What is LangChain? Use Cases and Benefits - MarkTechPost\n",
      "Content: LangChain is super handy because it allows us to use language models to work with databases without manually writing complex SQL queries. It‚Äôs like having a conversation with the database, making it easier to get the information we need. This feature opens up possibilities for creating chatbots that can answer questions based on databa\n"
     ]
    }
   ],
   "source": [
    "simdocs = mq_retriever.get_relevant_documents(\"what is langchain\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"The retriever returned {len(simdocs)} relevant documents. below is a snippet:\")\n",
    "print(\"-\" * 100, \"\\n\\n\")\n",
    "print(pretty_print_docs(simdocs, for_llm=False)[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Compression Retriever\n",
    "\n",
    "One challenge with retrieval is that usually you don‚Äôt know the specific queries your document storage system will face when you ingest data into the system. This means that the information most relevant to a query may be buried in a document with a lot of irrelevant text. Passing that full document through your application can lead to more expensive LLM calls and poorer responses.\n",
    "\n",
    "Contextual compression is meant to fix this. The idea is simple: instead of immediately returning retrieved documents as-is, you can compress them using the context of the given query, so that only the relevant information is returned. ‚ÄúCompressing‚Äù here refers to both compressing the contents of an individual document and filtering out documents wholesale.\n",
    "\n",
    "To use the Contextual Compression Retriever, you‚Äôll need: - a base retriever (either the standard or multi query) - and a Document Compressor\n",
    "\n",
    "The Contextual Compression Retriever passes queries to the base retriever, takes the initial documents and passes them through the Document Compressor. The Document Compressor takes a list of documents and shortens it by reducing the contents of documents or dropping documents altogether\n",
    "\n",
    "We could do this with recursive calls to an LLM but this is expensive and slow. The EmbeddingsFilter provides a cheaper and faster option by embedding the documents and query and only returning those documents which have sufficiently similar embeddings to the query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccfg={\n",
    "        \"use_embeddings_filter\":True, \n",
    "        \"use_splitter\":True, \n",
    "        \"use_redundant_filter\":True, \n",
    "        \"use_relevant_filter\":True,\n",
    "        \"similarity_threshold\":0.5, #embeddings filter settings\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[retriever] üóúÔ∏è Compression object pipeline: embeddings_filter ‚û§ splitter ‚û§ redundant_filter ‚û§ relevant_filter\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "cc_retriever = ragdoll.get_compression_retriever(retriever, ccfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "The retriever returned 8 relevant documents. below is a snippet:\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "\n",
      "Source: https://aws.amazon.com/what-is/langchain/\n",
      "Title: What is LangChain? - LangChain Explained - AWS\n",
      "Content: What Is LangChain?\n",
      "What is LangChain?\n",
      "LangChain is an open source framework for building applications based on large language models (LLMs). LLMs are large deep-learning models pre-trained on large amounts of data that can generate responses to user queries‚Äîfor example, answering questions or creating images from text-based prompts. LangChain provides tools and abstractions to improve\n"
     ]
    }
   ],
   "source": [
    "simdocs = cc_retriever.get_relevant_documents(question)\n",
    "print(\"-\" * 100)\n",
    "print(f\"The retriever returned {len(simdocs)} relevant documents. below is a snippet:\")\n",
    "print(\"-\" * 100, \"\\n\\n\")\n",
    "print(pretty_print_docs(simdocs, for_llm=False)[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[retriever] üîó Running RAG chain\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is an open source framework designed for building applications based on large language models (LLMs). LLMs are deep-learning models that have been pre-trained on vast amounts of data and can generate responses to user queries, such as answering questions or creating images from text-based prompts. The purpose of LangChain is to provide tools and abstractions that improve the customization, accuracy, and relevancy of the information generated by these models.\n",
      "\n",
      "One of the key features of LangChain is its ability to make applications context-aware. This is achieved by establishing connections between a language model and various context sources. By incorporating context into the model's responses, LangChain enables applications to have a better understanding of the user's intent and provide more relevant and accurate information.\n",
      "\n",
      "LangChain also offers easy integration with outside data sources, making conversations with language models more interesting and full of context. This data awareness feature allows developers to connect LangChain with SQL databases using natural language. Instead of manually writing complex SQL queries, developers can ask questions or give commands in a more human-like way, and LangChain will translate that into SQL queries. For example, developers can ask LangChain to generate an SQL query to find out which stores were top-performing last week. This feature not only simplifies the interaction with databases but also opens up possibilities for creating chatbots that can answer questions based on database data and build custom dashboards for data analysis.\n",
      "\n",
      "Another important aspect of LangChain is its agentic capability. It allows language models to interact with the environment, making applications lively and dynamic. This means that the models are not just passive responders but can actively engage with the user and the application's context. By incorporating reasoning capabilities, LangChain enables the models to analyze the provided context and determine appropriate responses or actions based on that context. This enhances the overall user experience and makes the applications more intelligent and adaptive.\n",
      "\n",
      "LangChain provides standardized interfaces that are easy to use and can be extended, making it versatile for developers. The framework offers libraries in Python and TypeScript/JavaScript, allowing developers to choose the programming language that suits their needs. Additionally, LangChain provides templates that serve as reference architectures and starting points for application development. This streamlines the entire application lifecycle, from development to productionization to deployment.\n",
      "\n",
      "Furthermore, LangChain fosters a community where developers can help each other and share ideas. This collaborative environment promotes knowledge exchange and supports the growth and improvement of the framework.\n",
      "\n",
      "In summary, LangChain is an artificial intelligence framework that empowers developers to build applications using large language models. It enhances customization, accuracy, and relevancy by incorporating context-awareness and agentic capabilities. With easy integration with outside data sources and simplified interaction with SQL databases, LangChain simplifies the development of chatbots and data analysis applications. The standardized interfaces and supportive community contribute to the framework's versatility and continuous improvement.\n"
     ]
    }
   ],
   "source": [
    "response = ragdoll.answer_me_this(question, cc_retriever)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

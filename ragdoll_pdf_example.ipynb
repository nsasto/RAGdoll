{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGdoll PDF Interrogate example\n",
    "\n",
    "@untrueaxioms\n",
    "\n",
    "<img src='img/github-header-image.png' />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragdoll.helpers import set_logger\n",
    "loginfo = set_logger(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    'log_level':logging.INFO \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in a Jupyter Notebook or JupyterLab environment.\n"
     ]
    }
   ],
   "source": [
    "from ragdoll.helpers import is_notebook\n",
    "from ragdoll.index import RagdollIndex\n",
    "\n",
    "index= RagdollIndex(config)\n",
    "check_notebook = is_notebook(print_output=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† I will conduct my research based on the following pdf documents:\n",
      " \n",
      " ‚óã test_docs\\ukpga_20070003_en.pdf\n",
      " ‚óã test_docs\\ukpga_20160019_en.pdf...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os\n",
    "\n",
    "pdfs=os.listdir('./test_docs')\n",
    "#return a list of relative paths pdfs in test docs folder\n",
    "pdfs = [os.path.join('test_docs', pdf) for pdf in pdfs if pdf.endswith('.pdf')]\n",
    "\n",
    "pdflist = f\"\".join(f\"\\n ‚óã {d}\" for i, d in enumerate(pdfs))\n",
    "print(f\"üß† I will conduct my research based on the following pdf documents:\\n {pdflist}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[index] üåê Fetching raw source content\u001b[0m\n",
      "\u001b[32m[index] üì∞ Chunking document\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "extracted 2 pdf documents\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "üìÑ  Splitting Documents\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "extracted 3626 documents from 2 documents\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "documents = index.get_scraped_content(pdfs)\n",
    "print(\"-\" * 100)\n",
    "print(f\"extracted {len(documents)} pdf documents\")\n",
    "print(\"-\" * 100)\n",
    "print('\\nüìÑ  Splitting Documents\\n')\n",
    "split_docs = index.get_split_documents(documents)\n",
    "print(\"-\" * 100)\n",
    "print(f\"extracted {len(split_docs)} documents from {len(documents)} documents\")\n",
    "print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[index] Running document index pipeline\u001b[0m\n",
      "\u001b[32m[index] üåê Fetching raw source content\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Or all in one like this\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[index] üì∞ Chunking document\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "extracted 3626 documents from 2 documents\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('üîó Or all in one like this')\n",
    "split_docs = index.run_document_pipeline(pdfs)\n",
    "print(\"-\" * 100)\n",
    "print(f\"extracted {len(split_docs)} documents from {len(documents)} documents\")\n",
    "print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed and Store\n",
    "\n",
    "Let‚Äôs start by initializing a simple vector store retriever and storing our docs (in chunks).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[retriever] üóÉÔ∏è  retrieving vector database (FAISS)...\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ragdoll.retriever import RagdollRetriever\n",
    "ragdoll = RagdollRetriever(config)\n",
    "db = ragdoll.get_db(split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'll use the contextual compression retriever, with a multiquery retriever as base because, well, why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[retriever] üìã getting multi query retriever\u001b[0m\n",
      "\u001b[32m[retriever] üí≠ Remember that the multi query retriever will incur additional calls to your LLM\u001b[0m\n",
      "\u001b[32m[models] ü§ñ retrieving OpenAI model for multi query retriever\u001b[0m\n",
      "\u001b[32m[retriever] üóúÔ∏è Compression object pipeline: embeddings_filter ‚û§ splitter ‚û§ redundant_filter ‚û§ relevant_filter\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "base_retriever = ragdoll.get_mq_retriever() \n",
    "\n",
    "ccfg={\n",
    "        \"use_embeddings_filter\":True, \n",
    "        \"use_splitter\":True, \n",
    "        \"use_redundant_filter\":True, \n",
    "        \"use_relevant_filter\":True,\n",
    "        \"similarity_threshold\":0.6, #embeddings filter settings\n",
    "    }\n",
    "\n",
    "retriever = ragdoll.get_compression_retriever(base_retriever, ccfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üôã‚Äç‚ôÇÔ∏è Question Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "question='what are these documents about?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[retriever] üîó Running RAG chain\u001b[0m\n",
      "\u001b[32m[models] ü§ñ retrieving OpenAI model for RAG chain\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[33m[text_splitter] Created a chunk of size 555, which is longer than the specified 500\u001b[0m\n",
      "\u001b[33m[text_splitter] Created a chunk of size 514, which is longer than the specified 500\u001b[0m\n",
      "\u001b[33m[text_splitter] Created a chunk of size 598, which is longer than the specified 500\u001b[0m\n",
      "\u001b[33m[text_splitter] Created a chunk of size 516, which is longer than the specified 500\u001b[0m\n",
      "\u001b[33m[text_splitter] Created a chunk of size 598, which is longer than the specified 500\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, the documents are related to the UK legislation, specifically the Immigration Act 2016 and the Income Tax Act 2007. The content of the documents includes provisions, amendments, penalties, enforcement, and support related to immigration and taxation in the United Kingdom.\n"
     ]
    }
   ],
   "source": [
    "response = ragdoll.answer_me_this(question, retriever)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retriever used the following docs to support the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[33m[text_splitter] Created a chunk of size 555, which is longer than the specified 500\u001b[0m\n",
      "\u001b[33m[text_splitter] Created a chunk of size 514, which is longer than the specified 500\u001b[0m\n",
      "\u001b[33m[text_splitter] Created a chunk of size 598, which is longer than the specified 500\u001b[0m\n",
      "\u001b[33m[text_splitter] Created a chunk of size 516, which is longer than the specified 500\u001b[0m\n",
      "\u001b[33m[text_splitter] Created a chunk of size 598, which is longer than the specified 500\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n",
      "\u001b[32m[_client] HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "The retriever returned 18 relevant documents. below is a snippet:\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "\n",
      "Source: test_docs\\ukpga_20160019_en.pdf\n",
      "Title: newbook.book\n",
      "Content: 19)\\nSchedule 14 ‚Äî Maritime enforcement\\n220\\n(b)\\nseize and retain any document the officer has reasonable\\ngrounds to believe to be an item subject to legal privilege.\\n(8) In this paragraph a ‚Äúnationality document‚Äù, in relation to a person,\\nmeans any document which might‚Äî\\n(a)\\nestablish the person‚Äôs identity, nationality or citizenship,\\nor\\n(b)\\nindicate the place from which the person has travelled to\\nthe United Kingdom\n"
     ]
    }
   ],
   "source": [
    "from ragdoll.helpers import pretty_print_docs\n",
    "\n",
    "simdocs = retriever.get_relevant_documents(question)\n",
    "print(\"-\" * 100)\n",
    "print(f\"The retriever returned {len(simdocs)} relevant documents. below is a snippet:\")\n",
    "print(\"-\" * 100, \"\\n\\n\")\n",
    "print(pretty_print_docs(simdocs, for_llm=False)[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

cache:
  enabled: true
  cache_ttl: 3600

monitor:
  enabled: false
  collect_metrics: true

chunker:
  default_splitter: markdown
  chunk_size: 1000
  chunk_overlap: 200
  enabled: false

ingestion:
  max_threads: 10
  batch_size: 100
  retry_attempts: 3
  retry_delay: 1
  retry_backoff: 2
  loaders:
    file_mappings:
        .json: langchain_community.document_loaders.JSONLoader
        .jsonl: langchain_community.document_loaders.JSONLoader
        .yaml: langchain_community.document_loaders.JSONLoader
        .csv: langchain_community.document_loaders.CSVLoader
        .epub: langchain_markitdown.loaders.EpubLoader
        .xlsx: langchain_markitdown.loaders.XlsxLoader
        .html: langchain_markitdown.loaders.HtmlLoader
        .bmp: langchain_markitdown.loaders.ImageLoader
        .jpeg: langchain_markitdown.loaders.ImageLoader
        .jpg: langchain_markitdown.loaders.ImageLoader
        .png: langchain_markitdown.loaders.ImageLoader
        .tiff: langchain_markitdown.loaders.ImageLoader
        .md: langchain_community.document_loaders.TextLoader
        .pdf: langchain_community.document_loaders.PyMuPDFLoader
        .pptx: langchain_markitdown.loaders.PptxLoader
        .docx: langchain_markitdown.loaders.DocxLoader
        .xml: langchain_markitdown.loaders.RssLoader
        .txt: langchain_community.document_loaders.TextLoader
        .rtf: langchain_community.document_loaders.RtfLoader
        arxiv: langchain_community.retrievers.ArxivRetriever
        website: langchain_community.document_loaders.WebBaseLoader

embeddings:
  enabled: true  
  default_client: openai
  clients:  
    openai:  
      client: openai
      model: "text-embedding-3-large"
      dimensions: 1024
    huggingface:      
      model: "sentence-transformers/all-mpnet-base-v2"
      client: huggingface
      api_key: "your_default_api_key"

entity_extraction:
  enabled: true
  spacy_model: "en_core_web_sm"
  chunking_strategy: "fixed"
  chunk_size: 1000
  chunk_overlap: 50
  coreference_resolution_method: "llm"
  entity_extraction_methods: ["ner", "llm"]
  relationship_extraction_method: "llm"
  entity_types: ["PERSON", "ORG", "GPE", "DATE", "LOC", "PRODUCT", "EVENT", "WORK_OF_ART", "LAW", "ROLE"]
  relationship_types: ["HAS_ROLE", "WORKS_FOR", "LOCATED_IN", "BORN_IN", "FOUNDED", "PARENT_OF", "SPOUSE_OF", "AFFILIATED_WITH", "BELONGS_TO", "CREATED", "PART_OF", "REPRESENTS"]
  gleaning_enabled: true
  max_gleaning_steps: 2
  entity_linking_enabled: true
  entity_linking_method: "string_similarity"
  entity_linking_threshold: 0.8
  postprocessing_steps: ["merge_similar_entities", "normalize_relations"]
  output_format: "json"
  graph_database_config:
    output_file: "graph_output.json"

# References to prompt files (without .md extension)
llm_prompts:
  entity_extraction_llm: entity_extraction
  extract_relationships: relationship_extraction
  coreference_resolution: coreference_resolution
  entity_relationship_continue_extraction: entity_relationship_continue
  entity_relationship_gleaning_done_extraction: entity_relationship_gleaning_done

llms:
  model_list:
    - model_name: gpt-3.5-turbo
      litellm_params:
        model: openai/gpt-3.5-turbo
        api_key: os.environ/OPENAI_API_KEY
        temperature: 0.0
        max_tokens: 1024
    - model_name: claude-3-sonnet
      litellm_params:
        model: anthropic/claude-3-sonnet-20240229
        api_key: os.environ/ANTHROPIC_API_KEY
        temperature: 0.0
        max_tokens: 1024
    - model_name: gemini-pro
      litellm_params:
        model: google/gemini-pro
        api_key: os.environ/GOOGLE_API_KEY
        temperature: 0.0
        max_tokens: 1024
  default_model: gpt-3.5-turbo
